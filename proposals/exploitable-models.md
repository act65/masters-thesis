%exploitable
# Exploitable models

Why do we want to do this?

## Definition

What could help the planner do its job better?
Receiving information about he structure of the model? Being able to use the model in 'other' ways.
__Q__ How can we design a system where the model adapts itself to me maximally useful to the planner?!

Thus some nice properties of a model might be;

- it is invertible
- it can be run at smaller/larger time steps (allowing the controller to trade-off computation and accuracy)
- it can more/less local/global interactions (again allowing the controller to trade-off computation and accuracy)


Want to think about this as two agents working together to achieve their goals.

- Planner's goal is to maximise external reward.
- Model's goal is to maximise ???.

$$
\begin{align*}
C: M\times V \to a \\

\end{align*}
$$

## Reward hacking and model bias

If the model is not accurate then we can easily plan for fantastic outcomes.

Maybe the model thinks it is possible to walk through a certian wall, or ...

How can we avoid the planner exploiting this inaccuracy to plan for high imagined reward, with impossible/poor results.  

Similarity is that we want to constrain the space of policies to ones that are 'reasonable'.
We want policies that;
- match the intent of the training reward (which might be a proxy)
- work in the real world
- ?

Maybe this isnt a problem? If the model though it was possible and then the planner attempts to do it, then there will be a large surprise signal!?

## Resources
