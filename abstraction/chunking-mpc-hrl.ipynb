{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18693541 -1.2806509  -1.5593134 ]\n"
     ]
    }
   ],
   "source": [
    "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANT: to show abstractions emerging from heirarchical MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpc(initial_state, reward_fn, transition_fn, T):\n",
    "    # solve it ...\n",
    "    # is there some sort of solver I can use here?\n",
    "    # differentiable MPC would be great!\n",
    "    for t in T:\n",
    "        \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCLearner():\n",
    "    \"\"\"\n",
    "    No memoization. Nothing fancy.\n",
    "    Set subgoals, use MPC to achieve them.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.policy = None\n",
    "        self.transition_fn = None\n",
    "        self.metric = None\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: the current state\n",
    "                \n",
    "        Returns:\n",
    "            a: the action to take\n",
    "        \"\"\"\n",
    "        if self.metric(state, goal) > 0.9: \n",
    "            # if the goal has been reached\n",
    "            # pick a new one\n",
    "            self.goal = self.policy(state)\n",
    "            # no need to reward the lower level learner here as it is just a dict.\n",
    "\n",
    "        self.plan = self.mpc(state, self.transition_fn, self.plan, lambda s: self.metric(s, goal), T=10)\n",
    "        \n",
    "        a = self.plan.pop_left()\n",
    "        # measure of surprise is a sharp change in the plan!?\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now construct a heirarchy of these. each with different T and goal-action spaces!?\n",
    "\n",
    "class MPCPlanner():\n",
    "    def __init__(self, x):\n",
    "        self.policy = None\n",
    "        self.transition_fn = None\n",
    "        self.metric = None\n",
    "        \n",
    "        self.action_space = None\n",
    "        \n",
    "    def __call__(self, state, goal):\n",
    "        # needs a call back!? when goal has been approx reached!?\n",
    "        self.plan = self.mpc(state, self.transition_fn, self.plan, lambda s: self.metric(s, goal), T=10)\n",
    "        a = self.plan.pop_left()\n",
    "        # measure of surprise is a sharp change in the plan!?\n",
    "        return a\n",
    "    \n",
    "    \n",
    "class Heirarchy():\n",
    "    def __init__(self):\n",
    "        self.layers = [MPCPlanner() for _ in range(3)]\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        if self.metric(state, goal) > 0.9: \n",
    "            # if the goal has been reached\n",
    "            # pick a new one\n",
    "            self.goal = self.policy(state)\n",
    "            # no need to reward the lower level learner here as it is just a dict.\n",
    "        \n",
    "        sub_goal = self.goal\n",
    "        for layer in self.layers:\n",
    "            sub_goal = layer(state, sub_goal)\n",
    "            \n",
    "        return sub_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoDict(dict):\n",
    "    # TODO should only work on the first two args; states and goals.\n",
    "    # not that doesnt make sense...\n",
    "    \n",
    "    # the key to making this useful will be using notions of similarity\n",
    "    # so we can reuse old solutions for slightly different state-goal-transition-...s\n",
    "    \n",
    "    # what about compressing the memory?\n",
    "    # using function approximation instead. this would mean ...!?\n",
    "    \n",
    "    # this is the sort of thing you would want to wrap every function with!?\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __call__(self, *args):\n",
    "        return self[args]\n",
    "    def __missing__(self, key):\n",
    "        # TODO want to add based on density / rate.\n",
    "        # if the 'same thing' keeps being solved then just remember the answer\n",
    "        ret = self[key] = self.f(*key)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyLearner():\n",
    "    def __init__(self, x):\n",
    "        # the mpc will end up storing a bunch of options / sub policies\n",
    "        self.mpc = MemoDict(mpc)  \n",
    "        \n",
    "        self.policy = None\n",
    "        self.transition_fn = None\n",
    "        self.metric = None\n",
    "        \n",
    "    def __call__(self, state, goal_fn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: the current state\n",
    "            goal_fn: a callable fn that returns the expected return. goal_fn: s -> R.\n",
    "                (could be the estimated value fn, or a subgoal / metric from a high lvl policy!?)\n",
    "                (but how can it be used!?)\n",
    "                \n",
    "        Returns:\n",
    "            a: the action to take\n",
    "        \"\"\"\n",
    "        if self.metric(state, goal) > 0.9: \n",
    "            # if the goal has been reached\n",
    "            # pick a new one\n",
    "            self.goal = self.policy(state)\n",
    "            # no need to reward the lower level learner here as it is just a dict.\n",
    "\n",
    "        self.plan = self.mpc(state, self.transition_fn, self.plan, lambda s: self.metric(s, goal), T=10)\n",
    "        \n",
    "        a = self.plan.pop_left()\n",
    "        # measure of surprise is a sharp change in the plan!?\n",
    "        \n",
    "        return a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
