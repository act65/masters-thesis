{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "n_states = 12\n",
    "episode_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(elems, T):\n",
    "    def recurse(xs):\n",
    "        if len(xs[0]) >= T:\n",
    "            return xs\n",
    "        else:\n",
    "            return recurse([x+[e] for x in xs for e in elems]) # + xs\n",
    "            \n",
    "    return recurse([[e] for e in elems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    }
   ],
   "source": [
    "elems = list(range(n_actions))\n",
    "trajectories = np.array(permutations(elems, episode_len))\n",
    "n_trajectories = trajectories.shape[0]\n",
    "print(n_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories  # think about these as sequences of actions or as sequences of states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume rewards are given for being in state s.\n",
    "\n",
    "(can I have an example of rewards as a fn of $r(s,a)$ state and action!? is it not just equivalent to $r(s_{t+1})$? not if transitions are stochastic. but surely we only reward the actionif it actually does something to the state!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = np.zeros((1, n_states))\n",
    "rewards[0,1]= 1\n",
    "rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00035946, 0.00031307, 0.00314563, ..., 0.00359513, 0.00097901,\n",
       "       0.00104118])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# via policy evaluation\n",
    "# a distribution over the possible trajectories\n",
    "\n",
    "observed_rewards = rnd.choice([0.0,1.0], size=(1, n_trajectories), p=[0.9, 0.1])\n",
    "# many trajectories might arrive in the same state and thus receive the same reward.\n",
    "\n",
    "p_traj = np.exp(rnd.standard_normal((n_trajectories)))\n",
    "p_traj /= p_traj.sum()\n",
    "p_traj\n",
    "\n",
    "# value = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two trajectories can be factorised if;\n",
    "\n",
    "$$\n",
    "t_1 = t_2 \\;\\;\\text{if}\\\\\n",
    "\\sum_{i=0}^t \\gamma^t r(s_{t_1[i]}) = \\sum_{i=0}^t \\gamma^tr(s_{t_2[i]})  \\\\\n",
    "\\text{Invariance criteria} \\\\\n",
    "V(t_1) = V(t_2) \\\\\n",
    "s_{t_1[T]} = s_{t_2[T]} \\\\\n",
    "$$\n",
    "\n",
    "But, stochasticity changes everything... Unlikely even that $V(t_1) = V(t_1)$... or $s_{t_1[T]} = s_{t_1[T]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_options = 8\n",
    "options = np.arange(n_options)\n",
    "\n",
    "# want to find a mapping from actions to options. a factorisation!?\n",
    "# s.t. options = A.T = np.dot(abstraction, trajectories). \n",
    "# abstraction = (n_options x n_trajectories)\n",
    "# s.t. the rows of A are sparse (only contain a 1 in one column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 5), (8,))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.shape, options.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Incompatible dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-fb9fbd5614d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/scratch/miniconda3/envs/venv/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0mm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Incompatible dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Incompatible dimensions"
     ]
    }
   ],
   "source": [
    "np.linalg.lstsq(trajectories, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reward is independent of / invariant to a dimension is the state space then we can factorise the action space!!!\n",
    "\n",
    "Eg. Eating candy. Still rewarded whether legs are folded or not. Or whether inside or out. Etc.\n",
    "\n",
    "This we don't have to consider actions that only change the invariant dimensions.\n",
    "\n",
    "\n",
    "Assume $r(\\cdot)$ is invariant to noise / changes in the $i$th dimension. Aka, there exits $i$ s.t. $r(s) = r(s + ne_i), n\\sim \\mathcal N$.\n",
    "\n",
    "This means that;\n",
    "- any two (sub) trajectories that vary in only the $i$th dimension can be ???\n",
    "- any action that only changes the state in the $i$th dimension can be temporally abstracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimate goal is to find a single button/action/subgoal/option that maximises instantaneous and future reward. We can just keep hitting that button.\n",
    "\n",
    "To see the future soo clearly that we can pick a policy for the next 10 years. And achieve max reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of temporally abstracted options, there are may redundancies.\n",
    "\n",
    "For example, imagine we have the action, step left (l), step right (r) and do nothing (n).\n",
    "Then we might have the case where;\n",
    "\n",
    "- llr =  rll = lrl = lnn = nln = nnl (ie the order doesnt matter).\n",
    "\n",
    "But sometimes the order may matter. We might have stop (s), drop (d) and roll (r).\n",
    "\n",
    "\n",
    "\n",
    "What other 'higher' order symmetries might there be within sequences of actions. Inter subset symmetries of actionscan be; \n",
    "\n",
    "- permuted (any permutation)\n",
    "- mirrored (abcd = dcba)\n",
    "- rotated (abcd = dabc)\n",
    "- ?\n",
    "\n",
    "And intra subset symmetries of actions can have symmetries\n",
    "\n",
    "- mirrored (AasdsfgfB = BasdsfgfA) where A=sdf, B=rge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High freq setting.\n",
    "\n",
    "Imagine a setting where you ahve access to hundreds of actions. But only a few actually move you through the maze (left, right, up, down). Each of these moves takes ~n steps to execute. Other (non movement) actions can be taken during their execution.\n",
    "\n",
    "This problem is a lot harder (how much???) than the original (left, right up down).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subgoal (that is to be reached in k steps) defines a subset of possible trajectories. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
