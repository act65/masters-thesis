---
pagetitle: Summaries
---
<div>

## World models

- Learn a large RNN that approximates the transition function.
- Add noise when imagining/simulating experience to avoid the controller overfitting to bias in the model.
- Can use the world-model to imagine training experience for training the controller.

[https://worldmodels.github.io/](https://worldmodels.github.io/)

## Temporal difference models

> conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal

> Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? ... [R]ewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying
“physics.”

$$
r(s_t, a_t, s_{t+1}, s_g) = −D(s_{t+1}, s_g) \\
$$


So $r$ is now a measure of _reachability_!?

> TDM ... tells us how close the agent will get to a given goal state $s_g$ after $τ$ time steps, when it is attempting to reach that state in $τ$ steps.

$$
Q(s_t, a_t, s_g, \tau) = E_{p(s_{t+1}\mid s_t, a_t)}(-D(s_{t+1}, s_g)) \cdot1[\tau =0] + \text{max}_a Q(s_{t+1}, a, s_g, \tau-1)\cdot1[\tau \neq0]
$$

_Wait, doesnt $\tau -1$ mean that we will have negative $\tau$s? I think it ishould be $1[\tau>0]$?_

So if $\tau=0$, and $s_{t+1} = s_g$ then then $Q$ value should be zero.

> ... vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point.

_hmm. want to think about this!! but not sure who it works, how can you do the argmax now?!?_

$$
\begin{align}
Q(s_t, a_t, s_g, \tau) &= \sum_i \psi_i(s_t, a_t, s_g, \tau) \\
&=  \sum_i - \mid s_t - s_g \mid \\
\mathcal L &= \frac{1}{2}\parallel y - \psi(s_t, a_t, s_g, \tau) \parallel_2^2 \\
\end{align}
$$

$Q$ is used to take actions, but is constructed/trained via some representation of distances. Reminds me of successor features?!?

$$
\begin{align}
Q(s_t, a_t) &= g(f(s_t, a_t)) \\
\mathcal L &= \frac{1}{2}\parallel y - f(s_t, a_t) \parallel_2^2 \\
\end{align}
$$

Ahh ok. I think I see. We are simply learning the model. And we already know how to extract $Q$ values from it. Therefore we dont need to train $g$.

***

- Define a proxy reward as the distance between a goal state and the current state.
- Learn a Q value that estimates the reachability of different states within t steps.
- It turns out when t>>1 we recover model-free learning and yet we can still plan with t~=1.

[https://bair.berkeley.edu/blog/2018/04/26/tdm/](https://bair.berkeley.edu/blog/2018/04/26/tdm/)

## Learning to reinforcement learn

- Use a RNN as an actor-critic but also provide the reward recieved as an input.
- Train weights over sampled MPDs to maximise cumlative reward over an episode.
- Freeze weights and test on new MPDs.

[https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)

## Independent causal mechanisms

- Learn a set of experts
- Hold a tournament between the experts
- Train the winner

Aka, a GAN with many generators.

> each element [in the dataset] has been generated by one of the (independent) mechanisms, but we do not know by which one.

> The motivation behind competitively updating only the winning expert is to enforce specialization

__Q:__ How does specialisation relate to independence?

[https://arxiv.org/abs/1712.00961](https://arxiv.org/abs/1712.00961)

## Pieter Abbeel talking at NIPs workshop on HRL
(https://www.youtube.com/watch?v=WpSc3D__Av8)

- Information theoretic perspective: if you set a goal now, that you tell you something about the future
- Grid world where agent must discover the passcodes for the actions (left right up down). For example left might be 0,0,1.


> there is still no consensus on what constitute good options. [A Matrix Splitting Perspective on Planning with Options](https://arxiv.org/pdf/1612.00916.pdf)
> if the option set for the task is not ideal, and cannot express the primitive optimal policy well, shorter options offer more flexibility and can yield a better solution. [Learning with Options that Terminate Off-Policy](https://arxiv.org/pdf/1711.03817.pdf)


## A Theory of State Abstraction for Reinforcement Learning
(https://david-abel.github.io/papers/aaai_dc_2019.pdf)

> I propose three desiderata that characterize what it means for an abstraction to be useful for RL:
> 1. SUPPORT EFFICIENT DECISION MAKING: The abstraction enables fast planning and efficient RL.
> 2. PRESERVE SOLUTION QUALITY: Solutions produced from the abstracted model should be useful enough for solving the desired problems.
> 3. EASY TO CONSTRUCT: Creating the abstractions should not require an unrealistic statistical or computational budget.


## On the necessity of abstraction
(https://www.sciencedirect.com/science/article/pii/S2352154618302080)

> The challenge in the single-task case is overcoming the additional cost of discovering the options; this results in a narrow opportunity for performance improvements, but a well-defined objective. In the skill transfer case, the key challenge is predicting the usefulness of a particular option to future tasks, given limited data.


## Near optimal representation learning for HRL
(https://openreview.net/forum?id=H1emus0qF7)


$$
\begin{align}
D_{KL}(P_{\pi^{* }(s, g)}(s'\mid s) \parallel K_{\theta}(s' \mid s, g)) \\
K_{\theta}(s' \mid s, g) = \rho(s') \frac{e^{-E(s', s, g)}}{Z} \\
E(s', s, g) = D(f(s'), \varphi(s, \psi(s, g))) \tag{distance is measured in abstract space}\\
\end{align}
$$
Kinda like an autoencoder?
$$
\begin{align}
D_{KL}(P(x) \parallel K(x' \mid x)) \\
K(x \mid x) = \rho(x) \frac{e^{-E(x)}}{Z} \\
E(x) = D(x, d(e(x))) \\
\end{align}
$$


## Model based RL for atari

(https://arxiv.org/abs/1903.00374)

- Achieve sota in approx 100k interactions.
- Learn the transition fn and the reward fn via (self) supervision
- Learn the policy and value via simulation (only)
- Short planning horizon, augmented with mixture of real data.
- Give reward for final (planned) time step.


## Understanding the asymptotic performance of model based RL methods

(http://willwhitney.com/assets/papers/Understanding.the.Asymptotic.Performance.of.MBRL.pdf)

- Multi time step transition models (somehow) achieve less error (constant wrt horizon!?) than single time step models (linear wrt horizon!?). (WHY?!)
- Planning with the environment simulator (MUJOCU) still only allows 40 time steps (for the half cheeta) before diverging.

## Near Optimal Behavior via Approximate State Abstraction
(https://arxiv.org/abs/1701.04113)

What are the necessary conditions of epsilon-optimality on an abstraction?
$$
\forall_{s\in S_G, a\in A_G} \mid Q_G^{\pi^* }(s, a) - Q_G^{\pi_{GA}^* }(s, a) \mid \le 2 \epsilon \eta_f \implies \phi() ???
$$

## The Value Function Polytope in Reinforcement Learning
(https://arxiv.org/abs/1901.11524)

$$
\begin{align}
V &= r_{\pi} + \gamma P_{\pi} V \\
V - \gamma P_{\pi} V &= r_{\pi}\\
(I-\gamma P_{\pi})V &= r_{\pi}\\
V &= (I-\gamma P_{\pi})^{-1}r_{\pi}\\
\end{align}
$$


## On the Complexity of Solving Markov Decision Problems
(https://arxiv.org/abs/1302.4971)

Sketch of proof of value iteration computational complexity

1. Bound the distance from the initial total-cost function to the optimal total-cost function.
1. Show that each iteration results in an improvement of a factor of at least $\gamma$. (standard contraction mapping result from Puterman)
1. Give an expression for the distance between estimated and optimal total-cost functions after $n$ iterations. Show how this gives a bound on the number of iterations required for an $\epsilon$-optimal policy.
1. Argue that there is a value for $\epsilon > 0$ for which an $\epsilon$-optimal policy is, in fact, optimal.
1. Substitute this value of $\epsilon$ into the bound to get a bound on the number of iterations needed for an exact answer.

> Let $\pi_n$ be the policy found after $n$ iterations of policy iteration. Let $E_{\pi_n}[\Sigma_{\gamma}\mid i]$ be the total-cost fuction associated with $\pi$. Let $E^n[\Sigma_{\gamma}\mid i]$ be the total-cost function found by n steps of value iteration, starting with $E_{\pi_0}[\Sigma_{\gamma}\mid i]$ as an initial total-cost function. Puterman (1994 theorem 6.4.6) shows that $E_{\pi_n}[\Sigma_{\gamma}\mid i]$ always dominates or is equal to $E^n[\Sigma_{\gamma}\mid i]$ and therefore policy iteration converges no more slowly than value iteration for discounted infinite-horizon MDPs

So this is the only proof of the computational complexity of policy iteration?!


## On the Complexity of Policy Iteration
(http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.4677&rep=rep1&type=pdf)

> We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor

Why? Also, how would the discount effect the complexity??


## Efficient computation of optimal abstractions
(https://www.pnas.org/content/106/28/11478)

Searching through policy space is expensive. Want to use the structure in the MDP to reduce the search space.

$$
v(s) = \mathop{\text{min}}_a \big[ r(s, a) + \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} v(x') \big]\\
r(s, a) = q(x) + \mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} \\
a(x' | x) = p(x' | x, \pi(x)) \\
v(x) = e^{-z} \\
$$

$$
\begin{align}
v(s) &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} +  \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} v(x') \big]\\
 &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} -  \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} \log z(x') \big]\\
  &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)z(x')} \big]\\
\end{align}
$$

Words

$$
\begin{align}
a^{* }(x' | x) &= \frac{p(x' | x)z(x')}{G[z](x)} \\
G[z](x) &= \mathop{\mathbb E}_{x'\sim p(\cdot | x)} z(x') \\
-\log z(x)  &= q(x) + \mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log \frac{a^{* }(x' | x)}{p(x' | x)z(x')} \\
&= q(x) + \mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log G[z](x) \\
z(x)  &= e^{-q(x)} e^{\mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log G[z](x)} \\
&= e^{-q(x)} e^{\log G[z](x)} \\
&= e^{-q(x)}G[z](x) \\
z &= e^{-q}Pz \\
\end{align}
$$

Or in the finite case with terminating states.

$$
\begin{align}
P_n &\in [0,1]^{n_{int} \times n_{int}}, P_t \in [0,1]^{n_{int} \times n_{ext}} \\
M_n &=  \text{diag}(e^{-q_n}) \\
(I - M_n P_n)z_n &= M_n P_tz_t \\
z_n &= (I - M_n P_n)^{-1} M_n P_t z_t\\
\end{align}
$$

__Want to embed!? How?__

Given $\{(s, a, r, s')\}$ can estimate $r(s, a), \tau(s' | s, a)$. Want to estimate $q(x), p(x' | x)$.

$$
p(x' | x) = E_{a \sim U(n)} \tau(s' | x, a) \\
q(x) =  r(x, a) - KL(\tau(\cdot|x, a) \parallel p(\cdot | x)) \\
$$


__Connection to preirarchy__


## Symmetry-adapted representation learning
[https://www.sciencedirect.com/science/article/pii/S0031320318302620](https://www.sciencedirect.com/science/article/pii/S0031320318302620)

$$
\text{argmin} \;\; L(W, S_N) + \gamma L'(W, S_n) + \beta R(W) \\
$$

- $L(W, S_N)$: The training target. Maybe defined via supervision
- $L'(W, S_n)$: The unsupervised symmetry loss.
- $R(W)$: Regularise so that the representation forms an orbit of a group



### Dynamics

Inspired by [Characterising divergence in DQL](https://arxiv.org/abs/1903.08894).

Value iteration + taylor approximation.
- How do the params change via the updates?
- How do changes in the params change the value estimates.
- Combine.

$$
\begin{align}
Q_{t+1}(s,a) &= Q_t(s,a) + \alpha_t (T^{* }Q_t(s,a) - Q_t(s,a)) \\
\theta_{t+1} &= \theta + \alpha_t(T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \\
\\
Q_{\theta'}(s,a) &= Q_{\theta}(s,a) + \nabla_{\theta}Q_{\theta}(s,a)^T(\theta' - \theta) + \mathcal O(\parallel \theta' - \theta \parallel^2) \\
\\
Q_{\theta_{t+1}}(s,a) &= Q_{\theta}(s,a) + \nabla_{\theta}Q_{\theta}(s,a)^T\Big(\alpha_t(T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \Big) + \mathcal O(\parallel \theta_{t+1} - \theta \parallel^2) \\
Q_{\theta_{t+1}} &= Q_\theta + \alpha K_{\theta}D_{\rho}(T^{* }Q_\theta-Q_\theta) +
\end{align}
$$



What if we set $D_{\rho} = \text{diag}((1-\gamma) \cdot (T-\gamma P_{\pi'}) \cdot d_0)$?
Where using $\pi'$ represents some off policy training. And $\pi = \pi'$ is on-policy?

What about momentum?

$$
\begin{align}
g_t &= (T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \\
m_{t+1} &= \beta m_t + g_t \\
\theta_{t+1} &= \theta + \alpha_t m_t \\
\end{align}
$$



What about for policy iteration / policy gradients?


$$
\begin{align}
\theta_{t+1} &= \theta_t + \alpha \mathop{\mathbb E}_{\xi \sim d^{\pi}}  [V(\xi)\sum_{t=0}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)] \\
\pi_{\theta'}(s) &= \pi_{\theta} + \nabla_{\theta} \pi_\theta(s)^T (\theta' - \theta) + \mathcal O(\parallel \theta' - \theta \parallel^2) \\
\pi_{\theta_{t+1}}(s) &= \pi_{\theta} + \nabla_{\theta} \pi_\theta(s)^T \Big(\alpha \mathop{\mathbb E}_{\xi \sim d^{\pi}}  [V(\xi)\sum_{t=0}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)]\Big) + \mathcal O(\parallel \theta_{t+1} - \theta \parallel^2) \\
\pi_{\theta_{t+1}} &=
\end{align}
$$

Hmm. Is this going to be constrained to a normalised distribution?


***


Want;

- on / off policy plots
- Plot $Q = Q + \alpha KD(TQ-Q)$ versus VI (how good / bad is this approximation?)
- Plot $\theta' = \theta + \alpha\nabla Q(TQ-Q)$ versus $\theta' = \theta + \alpha\nabla QK^{-1}(TQ-Q)$ (the corrected version)
- test with / without momentum
- test with overparameterisation


### BSuite

> We do not have a clear pictureof whether such a learning algorithm will perform well at driving a car, or managing apower plant. If we want to take the next leaps forward, we need to continue to enhance ourunderstanding.

> A notable omission from thebsuite2019 release is the lack of any targeted experiments for‘hierarchical reinforcement learning’ (HRL). We invite the community to help us curate excellentexperiments that can evaluate quality of HRL

</div>
