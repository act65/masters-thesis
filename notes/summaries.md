---
pagetitle: Summaries
---
<div>

## World models

- Learn a large RNN that approximates the transition function.
- Add noise when imagining/simulating experience to avoid the controller overfitting to bias in the model.
- Can use the world-model to imagine training experience for training the controller.

[https://worldmodels.github.io/](https://worldmodels.github.io/)

## Temporal difference models

> conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal

> Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? ... [R]ewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying
“physics.”

$$
r(s_t, a_t, s_{t+1}, s_g) = −D(s_{t+1}, s_g) \\
$$


So $r$ is now a measure of _reachability_!?

> TDM ... tells us how close the agent will get to a given goal state $s_g$ after $τ$ time steps, when it is attempting to reach that state in $τ$ steps.

$$
Q(s_t, a_t, s_g, \tau) = E_{p(s_{t+1}\mid s_t, a_t)}(-D(s_{t+1}, s_g)) \cdot1[\tau =0] + \text{max}_a Q(s_{t+1}, a, s_g, \tau-1)\cdot1[\tau \neq0]
$$

_Wait, doesnt $\tau -1$ mean that we willhave negative $\tau$s? I think it ishould be $1[\tau>0]$?_

So if $\tau=0$, and $s_{t+1} = s_g$ then then $Q$ value should be zero.

> ... vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point.

_hmm. want to think about this!! but not sure who it works, how can you do the argmax now?!?_

$$
\begin{align}
Q(s_t, a_t, s_g, \tau) &= \sum_i \psi_i(s_t, a_t, s_g, \tau) \\
&=  \sum_i - \mid s_t - s_g \mid \\
\mathcal L &= \frac{1}{2}\parallel y - \psi(s_t, a_t, s_g, \tau) \parallel_2^2 \\
\end{align}
$$

$Q$ is used to take actions, but is constructed/trained via some representation of distances. Reminds me of successor features?!?

$$
\begin{align}
Q(s_t, a_t) &= g(f(s_t, a_t)) \\
\mathcal L &= \frac{1}{2}\parallel y - f(s_t, a_t) \parallel_2^2 \\
\end{align}
$$

Ahh ok. I think I see. We are simply learning the model. And we already know how to extract $Q$ values from it. Therefore we dont need to train $g$.

***

- Define a proxy reward as the distance between a goal state and the current state.
- Learn a Q value that estimates the reachability of different states within t steps.
- It turns out when t>>1 we recover model-free learning and yet we can still plan with t~=1.

[https://bair.berkeley.edu/blog/2018/04/26/tdm/](https://bair.berkeley.edu/blog/2018/04/26/tdm/)

## Learning to reinforcement learn

- Use a RNN as an actor-critic but also provide the reward recieved as an input.
- Train weights over sampled MPDs to maximise cumlative reward over an episode.
- Freeze weights and test on new MPDs.

[https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)

## Independent causal mechanisms

- Learn a set of experts
- Hold a tournament between the experts
- Train the winner

Aka, a GAN with many generators.

> each element [in the dataset] has been generated by one of the (independent) mechanisms, but we do not know by which one.

> The motivation behind competitively updating only the winning expert is to enforce specialization

__Q:__ How does specialisation relate to independence?

[https://arxiv.org/abs/1712.00961](https://arxiv.org/abs/1712.00961)

## Pieter Abbeel talking at NIPs workshop on HRL
(https://www.youtube.com/watch?v=WpSc3D__Av8)

- Information theoretic perspective: if you set a goal now, that you tell you something about the future
- Grid world where agent must discover the passcodes for the actions (left right up down). For example left might be 0,0,1.


> there is still no consensus on what constitute good options. [A Matrix Splitting Perspective on Planning with Options](https://arxiv.org/pdf/1612.00916.pdf)
> if the option set for the task is not ideal, and cannot express the primitive optimal policy well, shorter options offer more flexibility and can yield a better solution. [Learning with Options that Terminate Off-Policy](https://arxiv.org/pdf/1711.03817.pdf)


## A Theory of State Abstraction for Reinforcement Learning
(https://david-abel.github.io/papers/aaai_dc_2019.pdf)

> I propose three desiderata that characterize what it means for an abstraction to be useful for RL:
> 1. SUPPORT EFFICIENT DECISION MAKING: The abstraction enables fast planning and efficient RL.
> 2. PRESERVE SOLUTION QUALITY: Solutions produced from the abstracted model should be useful enough for solving the desired problems.
> 3. EASY TO CONSTRUCT: Creating the abstractions should not require an unrealistic statistical or computational budget.


## On the necessity of abstraction
(https://www.sciencedirect.com/science/article/pii/S2352154618302080)

> The challenge in the single-task case is overcoming the additional cost of discovering the options; this results in a narrow opportunity for performance improvements, but a well-defined objective. In the skill transfer case, the key challenge is predicting the usefulness of a particular option to future tasks, given limited data.


## Near optimal representation learnning for HRL
(https://openreview.net/forum?id=H1emus0qF7)


$$
\begin{align}
D_{KL}(P_{\pi^{* }(s, g)}(s'\mid s) \parallel K_{\theta}(s' \mid s, g)) \\
K_{\theta}(s' \mid s, g) = \rho(s') \frac{e^{-E(s', s, g)}}{Z} \\
E(s', s, g) = D(f(s'), \varphi(s, \psi(s, g))) \tag{distance is measured in abstract space}\\
\end{align}
$$
Kinda like an autoencoder?
$$
\begin{align}
D_{KL}(P(x) \parallel K(x' \mid x)) \\
K(x \mid x) = \rho(x) \frac{e^{-E(x)}}{Z} \\
E(x) = D(x, d(e(x))) \\
\end{align}
$$


## Model based RL for atari

(https://arxiv.org/abs/1903.00374)

- Achieve sota in approx 100k interactions.
- Learn the transition fn and the reward fn via (self) supervision
- Learn the policy and value via simulation (only)
- Short planning horizon, augmented with mixture of real data.
- Give reward for final (planned) time step.


## Understanding the asymptotic performance of model based RL methods

(http://willwhitney.com/assets/papers/Understanding.the.Asymptotic.Performance.of.MBRL.pdf)

- Multi time step transition models (somehow) achieve less error (constant wrt horizon!?) than single time step models (linear wrt horizon!?). (WHY?!)
- Planning with the environment simulator (MUJOCU) still only allows 40 time steps (for the half cheeta) before diverging.

</div>
