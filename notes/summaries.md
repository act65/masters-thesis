---
pagetitle: Summaries
---
<div>

## World models

- Learn a large RNN that approximates the transition function.
- Add noise when imagining/simulating experience to avoid the controller overfitting to bias in the model.
- Can use the world-model to imagine training experience for training the controller.

[https://worldmodels.github.io/](https://worldmodels.github.io/)

## Temporal difference models

> conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal

> Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? ... [R]ewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying
“physics.”

$$
r(s_t, a_t, s_{t+1}, s_g) = −D(s_{t+1}, s_g) \\
$$


So $r$ is now a measure of _reachability_!?

> TDM ... tells us how close the agent will get to a given goal state $s_g$ after $τ$ time steps, when it is attempting to reach that state in $τ$ steps.

$$
Q(s_t, a_t, s_g, \tau) = E_{p(s_{t+1}\mid s_t, a_t)}(-D(s_{t+1}, s_g)) \cdot1[\tau =0] + \text{max}_a Q(s_{t+1}, a, s_g, \tau-1)\cdot1[\tau \neq0]
$$

_Wait, doesnt $\tau -1$ mean that we will have negative $\tau$s? I think it ishould be $1[\tau>0]$?_

So if $\tau=0$, and $s_{t+1} = s_g$ then then $Q$ value should be zero.

> ... vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point.

_hmm. want to think about this!! but not sure who it works, how can you do the argmax now?!?_

$$
\begin{align}
Q(s_t, a_t, s_g, \tau) &= \sum_i \psi_i(s_t, a_t, s_g, \tau) \\
&=  \sum_i - \mid s_t - s_g \mid \\
\mathcal L &= \frac{1}{2}\parallel y - \psi(s_t, a_t, s_g, \tau) \parallel_2^2 \\
\end{align}
$$

$Q$ is used to take actions, but is constructed/trained via some representation of distances. Reminds me of successor features?!?

$$
\begin{align}
Q(s_t, a_t) &= g(f(s_t, a_t)) \\
\mathcal L &= \frac{1}{2}\parallel y - f(s_t, a_t) \parallel_2^2 \\
\end{align}
$$

Ahh ok. I think I see. We are simply learning the model. And we already know how to extract $Q$ values from it. Therefore we dont need to train $g$.

***

- Define a proxy reward as the distance between a goal state and the current state.
- Learn a Q value that estimates the reachability of different states within t steps.
- It turns out when t>>1 we recover model-free learning and yet we can still plan with t~=1.

[https://bair.berkeley.edu/blog/2018/04/26/tdm/](https://bair.berkeley.edu/blog/2018/04/26/tdm/)

## Learning to reinforcement learn

- Use a RNN as an actor-critic but also provide the reward recieved as an input.
- Train weights over sampled MPDs to maximise cumlative reward over an episode.
- Freeze weights and test on new MPDs.

[https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)

## Independent causal mechanisms

- Learn a set of experts
- Hold a tournament between the experts
- Train the winner

Aka, a GAN with many generators.

> each element [in the dataset] has been generated by one of the (independent) mechanisms, but we do not know by which one.

> The motivation behind competitively updating only the winning expert is to enforce specialization

__Q:__ How does specialisation relate to independence?

[https://arxiv.org/abs/1712.00961](https://arxiv.org/abs/1712.00961)

## Pieter Abbeel talking at NIPs workshop on HRL
(https://www.youtube.com/watch?v=WpSc3D__Av8)

- Information theoretic perspective: if you set a goal now, that you tell you something about the future
- Grid world where agent must discover the passcodes for the actions (left right up down). For example left might be 0,0,1.


> there is still no consensus on what constitute good options. [A Matrix Splitting Perspective on Planning with Options](https://arxiv.org/pdf/1612.00916.pdf)
> if the option set for the task is not ideal, and cannot express the primitive optimal policy well, shorter options offer more flexibility and can yield a better solution. [Learning with Options that Terminate Off-Policy](https://arxiv.org/pdf/1711.03817.pdf)


## A Theory of State Abstraction for Reinforcement Learning
(https://david-abel.github.io/papers/aaai_dc_2019.pdf)

> I propose three desiderata that characterize what it means for an abstraction to be useful for RL:
> 1. SUPPORT EFFICIENT DECISION MAKING: The abstraction enables fast planning and efficient RL.
> 2. PRESERVE SOLUTION QUALITY: Solutions produced from the abstracted model should be useful enough for solving the desired problems.
> 3. EASY TO CONSTRUCT: Creating the abstractions should not require an unrealistic statistical or computational budget.


## On the necessity of abstraction
(https://www.sciencedirect.com/science/article/pii/S2352154618302080)

> The challenge in the single-task case is overcoming the additional cost of discovering the options; this results in a narrow opportunity for performance improvements, but a well-defined objective. In the skill transfer case, the key challenge is predicting the usefulness of a particular option to future tasks, given limited data.


## Near optimal representation learning for HRL
(https://openreview.net/forum?id=H1emus0qF7)


$$
\begin{align}
D_{KL}(P_{\pi^{* }(s, g)}(s'\mid s) \parallel K_{\theta}(s' \mid s, g)) \\
K_{\theta}(s' \mid s, g) = \rho(s') \frac{e^{-E(s', s, g)}}{Z} \\
E(s', s, g) = D(f(s'), \varphi(s, \psi(s, g))) \tag{distance is measured in abstract space}\\
\end{align}
$$
Kinda like an autoencoder?
$$
\begin{align}
D_{KL}(P(x) \parallel K(x' \mid x)) \\
K(x \mid x) = \rho(x) \frac{e^{-E(x)}}{Z} \\
E(x) = D(x, d(e(x))) \\
\end{align}
$$


## Model based RL for atari

(https://arxiv.org/abs/1903.00374)

- Achieve sota in approx 100k interactions.
- Learn the transition fn and the reward fn via (self) supervision
- Learn the policy and value via simulation (only)
- Short planning horizon, augmented with mixture of real data.
- Give reward for final (planned) time step.


## Understanding the asymptotic performance of model based RL methods

(http://willwhitney.com/assets/papers/Understanding.the.Asymptotic.Performance.of.MBRL.pdf)

- Multi time step transition models (somehow) achieve less error (constant wrt horizon!?) than single time step models (linear wrt horizon!?). (WHY?!)
- Planning with the environment simulator (MUJOCU) still only allows 40 time steps (for the half cheeta) before diverging.

## Near Optimal Behavior via Approximate State Abstraction
(https://arxiv.org/abs/1701.04113)

What are the necessary conditions of epsilon-optimality on an abstraction?
$$
\forall_{s\in S_G, a\in A_G} \mid Q_G^{\pi^* }(s, a) - Q_G^{\pi_{GA}^* }(s, a) \mid \le 2 \epsilon \eta_f \implies \phi() ???
$$


## On the Complexity of Solving Markov Decision Problems
(https://arxiv.org/abs/1302.4971)

Sketch of proof of value iteration computational complexity

1. Bound the distance from the initial total-cost function to the optimal total-cost function.
1. Show that each iteration results in an improvement of a factor of at least $\gamma$. (standard contraction mapping result from Puterman)
1. Give an expression for the distance between estimated and optimal total-cost functions after $n$ iterations. Show how this gives a bound on the number of iterations required for an $\epsilon$-optimal policy.
1. Argue that there is a value for $\epsilon > 0$ for which an $\epsilon$-optimal policy is, in fact, optimal.
1. Substitute this value of $\epsilon$ into the bound to get a bound on the number of iterations needed for an exact answer.

> Let $\pi_n$ be the policy found after $n$ iterations of policy iteration. Let $E_{\pi_n}[\Sigma_{\gamma}\mid i]$ be the total-cost fuction associated with $\pi$. Let $E^n[\Sigma_{\gamma}\mid i]$ be the total-cost function found by n steps of value iteration, starting with $E_{\pi_0}[\Sigma_{\gamma}\mid i]$ as an initial total-cost function. Puterman (1994 theorem 6.4.6) shows that $E_{\pi_n}[\Sigma_{\gamma}\mid i]$ always dominates or is equal to $E^n[\Sigma_{\gamma}\mid i]$ and therefore policy iteration converges no more slowly than value iteration for discounted infinite-horizon MDPs

So this is the only proof of the computational complexity of policy iteration?!


## On the Complexity of Policy Iteration
(http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.4677&rep=rep1&type=pdf)

> We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor

Why? Also, how would the discount effect the complexity??


## Efficient computation of optimal abstractions
(https://www.pnas.org/content/106/28/11478)

Searching through policy space is expensive. Want to use the structure in the MDP to reduce the search space.

$$
v(s) = \mathop{\text{min}}_a \big[ r(s, a) + \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} v(x') \big]\\
r(s, a) = q(x) + \mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} \\
a(x' | x) = p(x' | x, \pi(x)) \\
v(x) = e^{-z} \\
$$

$$
\begin{align}
v(s) &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} +  \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} v(x') \big]\\
 &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)} -  \mathop{\mathbb E}_{x' \sim p(\cdot | x, a)} \log z(x') \big]\\
  &= q(x) + \mathop{\text{min}}_a \big[\mathop{\mathbb E}_{x'\sim a(\cdot | x)} \log \frac{a(x' | x)}{p(x' | x)z(x')} \big]\\
\end{align}
$$

Words

$$
\begin{align}
a^{* }(x' | x) &= \frac{p(x' | x)z(x')}{G[z](x)} \\
G[z](x) &= \mathop{\mathbb E}_{x'\sim p(\cdot | x)} z(x') \\
-\log z(x)  &= q(x) + \mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log \frac{a^{* }(x' | x)}{p(x' | x)z(x')} \\
&= q(x) + \mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log G[z](x) \\
z(x)  &= e^{-q(x)} e^{\mathop{\mathbb E}_{x'\sim a^{* }(\cdot | x)} \log G[z](x)} \\
&= e^{-q(x)} e^{\log G[z](x)} \\
&= e^{-q(x)}G[z](x) \\
z &= e^{-q}Pz \\
\end{align}
$$

Or in the finite case with terminating states.

$$
\begin{align}
P_n &\in [0,1]^{n_{int} \times n_{int}}, P_t \in [0,1]^{n_{int} \times n_{ext}} \\
M_n &=  \text{diag}(e^{-q_n}) \\
(I - M_n P_n)z_n &= M_n P_tz_t \\
z_n &= (I - M_n P_n)^{-1} M_n P_t z_t\\
\end{align}
$$

__Want to embed!? How?__

Given $\{(s, a, r, s')\}$ can estimate $r(s, a), \tau(s' | s, a)$. Want to estimate $q(x), p(x' | x)$.

$$
p(x' | x) = E_{a \sim U(n)} \tau(s' | x, a) \\
q(x) =  r(x, a) - KL(\tau(\cdot|x, a) \parallel p(\cdot | x)) \\
$$


__Connection to preirarchy__


## Symmetry-adapted representation learning
[https://www.sciencedirect.com/science/article/pii/S0031320318302620](https://www.sciencedirect.com/science/article/pii/S0031320318302620)

$$
\text{argmin} \;\; L(W, S_N) + \gamma L'(W, S_n) + \beta R(W) \\
$$

- $L(W, S_N)$: The training target. Maybe defined via supervision
- $L'(W, S_n)$: The unsupervised symmetry loss.
- $R(W)$: Regularise so that the representation forms an orbit of a group



### Dynamics

Inspired by [Characterising divergence in DQL](https://arxiv.org/abs/1903.08894).

Value iteration + taylor approximation.
- How do the params change via the updates?
- How do changes in the params change the value estimates.
- Combine.

$$
\begin{align}
Q_{t+1}(s,a) &= Q_t(s,a) + \alpha_t (T^{* }Q_t(s,a) - Q_t(s,a)) \\
\theta_{t+1} &= \theta + \alpha_t(T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \\
\\
Q_{\theta'}(s,a) &= Q_{\theta}(s,a) + \nabla_{\theta}Q_{\theta}(s,a)^T(\theta' - \theta) + \mathcal O(\parallel \theta' - \theta \parallel^2) \\
\\
Q_{\theta_{t+1}}(s,a) &= Q_{\theta}(s,a) + \nabla_{\theta}Q_{\theta}(s,a)^T\Big(\alpha_t(T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \Big) + \mathcal O(\parallel \theta_{t+1} - \theta \parallel^2) \\
Q_{\theta_{t+1}} &= Q_\theta + \alpha K_{\theta}D_{\rho}(T^{* }Q_\theta-Q_\theta) +
\end{align}
$$



What if we set $D_{\rho} = \text{diag}((1-\gamma) \cdot (T-\gamma P_{\pi'}) \cdot d_0)$?
Where using $\pi'$ represents some off policy training. And $\pi = \pi'$ is on-policy?

What about momentum?

$$
\begin{align}
g_t &= (T^{* }Q_{\theta_t}(s,a) - Q_{\theta_t}(s,a))\nabla_{\theta}Q_{\theta_t}(s,a) \\
m_{t+1} &= \beta m_t + g_t \\
\theta_{t+1} &= \theta + \alpha_t m_t \\
\end{align}
$$



What about for policy iteration / policy gradients?


$$
\begin{align}
\theta_{t+1} &= \theta_t + \alpha \mathop{\mathbb E}_{\xi \sim d^{\pi}}  [V(\xi)\sum_{t=0}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)] \\
\pi_{\theta'}(s) &= \pi_{\theta} + \nabla_{\theta} \pi_\theta(s)^T (\theta' - \theta) + \mathcal O(\parallel \theta' - \theta \parallel^2) \\
\pi_{\theta_{t+1}}(s) &= \pi_{\theta} + \nabla_{\theta} \pi_\theta(s)^T \Big(\alpha \mathop{\mathbb E}_{\xi \sim d^{\pi}}  [V(\xi)\sum_{t=0}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)]\Big) + \mathcal O(\parallel \theta_{t+1} - \theta \parallel^2) \\
\pi_{\theta_{t+1}} &=
\end{align}
$$

Hmm. Is this going to be constrained to a normalised distribution?


***


Want;

- on / off policy plots
- Plot $Q = Q + \alpha KD(TQ-Q)$ versus VI (how good / bad is this approximation?)
- Plot $\theta' = \theta + \alpha\nabla Q(TQ-Q)$ versus $\theta' = \theta + \alpha\nabla QK^{-1}(TQ-Q)$ (the corrected version)
- test with / without momentum
- test with overparameterisation


### BSuite

> We do not have a clear pictureof whether such a learning algorithm will perform well at driving a car, or managing apower plant. If we want to take the next leaps forward, we need to continue to enhance ourunderstanding.

> A notable omission from thebsuite2019 release is the lack of any targeted experiments for‘hierarchical reinforcement learning’ (HRL). We invite the community to help us curate excellentexperiments that can evaluate quality of HRL

##### Symmetry learning for function approximation in RL
(https://arxiv.org/abs/1706.02999)

They use reward trajectories to construct a notion of similarity between two state-action pairs.

This technique allows one-shot generalisation to new state-action pairs. A new state-action pair is observed, and it has the sample reward trajectory as another state-action pair. We can automatically transfer the value estimate from the former pair to the latter. (for the current policy...).
__Actually. No?!!?__

Depending on how much symmetry is displayed, we are increasing the amount of data each state - action pair has...? How much more data per symmetry?
A form a weight sharing!
This is thought to be why conv nets work so well, their kernels recieve huge amounts of data as they are shared over many spatial locations.

Problems.
- It requires discrete state / action spaces, as to represent the ??? they use a tree.
- It doesnt scale well. Number of possible future trajectories = ???.
- Requires dense reward

$N: [n_{states}\times n_{actions} \times l_{steps} \times k_{possibilities}]$

$$
\begin{align}
N(s, a, l, k) &= ??? \\
\sum_{l, k} &= \sum_{l=l_o}^L \sum_{k_o}^K \\
\chi(\langle s,a\rangle, \langle s', a'\rangle) &= \frac{\sum_{l, k} \text{min} (N(s, a, l, k), N(s', a', l, k))}{\sum_{l, k} N(s, a, l, k) \sum_{l, k} N(s', a', l, k))} \\
\chi(\langle s,a\rangle, \langle s', a'\rangle) &= \frac{\sum_{l, k} p(\tau^k_l | s, a) p(\tau^k_l | s', a')}{\sum_{l, k} p(\tau^k_l | s, a) \sum_{l, k} p(\tau^k_l | s', a'))} \\
\chi(\langle s,a\rangle, \langle s', a'\rangle) &= JS(p(\cdot| s, a) \parallel p(\cdot | s', a'))\\
\end{align}
$$

^^^ This reminds me of testing for statistical independence!? $1 = \frac{P(A,B)}{P(A) P(B)}$

The estimates of $D(p(\cdot | s, a), p(\cdot | s', a'))$ are independent of policy. Sure, the current policy will effect the distribution of trajectories $p(\cdot | s, a)$. But if $\exists f, g: s' = f(s), a' = g_s(a)$ then $p(\cdot | s', a')$ will be effected equally.
But, if $p(\cdot | s, a)$ changes allot, then we need to explore enough to also update $p(\cdot | s', a')$. Hmm. Would rather couple the two?! $p(\cdot | s, a) = p(\cdot | s', a')$ once we have figured out that they are 'similar' (under a stationary policy). Want to generalise to other policies.

$$
 \chi(\langle s,a\rangle, \langle s', a'\rangle) >\Delta \\ \implies \mathop{\text{min}}_{\theta} D\big( \zeta(s, a), \zeta('s, a')\big) \\
\mathop{\text{min}}_{\theta} \mathop{\mathbb E}_{\chi} \big[\parallel \zeta(s, a) - \zeta(s', a') \parallel_2^2 \big]\\
$$

- Problem. We have gained data efficiency, but not computational effeciency? We need to train the network for each of these symmetries.
- Question. If we are training a NN in this way, how does the invariance get implemented within the NN?
- As training proceeds, and more symmetries have been observed. There might be very many pairs that are 'similar'. Want to visualise these clusters?!
- Oh... All we are doing is clustering based on a similarity measure... How does that relate to symmetry and quotients?

##### Is a good representation sufficient for sample efficient reinforcement learning?

Not sure what they mean by lower bounds. In my mind a lower bound says something like;
for any problem, we require at least n samples to achieve optimal performance.
Rather, their construction of the lower bound is based on a single, hard to solve problem.

Bc of the structure of their hard problem, all of the complexity comes from the reward function.
Making their insight that the transition fn doesnt contribute to complexity, no very insightful.
Their transition fn is constrained to be one of a binary tree?! No loops.

Also. A lower bound really should take into account the ability to find patterns.

***



- [Latent Space Policies for Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1804.02808.pdf)
- [Learning latent state representation for speeding up exploration](https://arxiv.org/pdf/1905.12621.pdf)


### Abstraction Selection in Model-Based Reinforcement Learning

[http://proceedings.mlr.press/v37/jiang15.pdf](http://proceedings.mlr.press/v37/jiang15.pdf)

$$
Appr(h) = \epsilon^h_R + \frac{\gamma R_{max} \epsilon_T^h}{2(1-\gamma)} \\
Appr(h) = \epsilon^h_R + \frac{\gamma \epsilon^h_R \epsilon_T^h}{2(1-\gamma)} \\
$$

Error in the rewards should accumulate into the value estimate? Not be proportional to?!


Interesting inductive bias.
Shouldnt use a fine abstraction at the start, because we do not have enough data to reliably estimate many states. Rather pick a coarse model, and estimate those states with higher n. Then, as needed to improve the approximation error, added complexity.


### Notes on state abstractions

[http://nanjiang.cs.illinois.edu/files/cs598/note4.pdf](http://nanjiang.cs.illinois.edu/files/cs598/note4.pdf)

> How should we define an (approximate) equivalence notion among states? Whether they share the same optimal action? Whether they share the same Q? values? Whether they yield the same rewards and next-state distributions? It turns out that, these criteria define a hierarchy of different state abstractions, from lenient to strict. Lenient notions of abstractions yield more generalization benefits, but may not work well with certain algorithms; strict notions of abstractions preserve many properties of the original MDP and hence work with a wider range of algorithms, but the generalization benefits are relatively limited.

So then which algorithms work with which abstractions?



In "Why does Heirarchy (sometimes) work so well in reinforcement learning?" \cite{Dadashi2018}
authors claim that the benefits of HRL can be explained by better
exploration. However, I would interpret their results as saying; "for
2D environments with walls, better exploration (aka larger steps, aka actions that are more temporally abstract), result in greater
explration". But what if the walls were replaced by cliffs? I imagine
this algorithm might do a lot worse!? Also, they don't seem to consider the main problem with HRL: discovery.
Once you have discovered a nice set of abstract actions or states, then yeah,
you get faster reward propagation, better exploration, \ldots{} etc.

</div>
