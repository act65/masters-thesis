> How is the structure of an agent’s environment reflected in the ‘grammar’ of its actions?

Action space gets abstracted into more 'meaningful' concepts.
But is influenced by the interaction with the model and the reward.

https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00328
https://www.biorxiv.org/content/10.1101/708891v1

As an agent learns a more abstract action space it invents a language!?

Imagine we have a small set of primitive actions.
A) E.g the ability to place tranistors in circuits.
We can then explore different circuits and their behaviour.
We could invent a general language for describing these circuits.

B) Also we could include the influence of the reward function and tune the exploration and capacity to what is rewarding.


Does it make sense to talk about the "grammar" of different RL envs?
Nautrally they would have some? In a maze with options. It wouldnt make sense to search-the-room if you are in a hallway...!?
And some options are not possible in different states. (but we could still try!?)

***

__Q:__ Asymmetries in sequences are equivalent to a grammar?
__Q:__ How do the symmetries of a sequence relate to its parse tree?

***

https://arxiv.org/abs/1902.01119
https://arxiv.org/pdf/1901.10723.pdf
https://ecs.victoria.ac.nz/Groups/Elvis/ThomasKuehne
https://www.victoria.ac.nz/lals/about/staff/victoria-chen
https://arxiv.org/pdf/1906.07343v1.pdf
