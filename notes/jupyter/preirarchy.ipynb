{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, player, len_episode):\n",
    "    # reset\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    r = 0\n",
    "    w = np.random.randint(8)\n",
    "    \n",
    "    # play an episode\n",
    "    for _ in range(len_episode):\n",
    "        a = player.choose_action(s, r)\n",
    "        s, r = env.step(s, a)\n",
    "        total_r += r\n",
    "        \n",
    "    return total_r/len_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_action_choice(heirarchy, x):\n",
    "    # want to keep track of which policies are getting called\n",
    "    # and how much\n",
    "    a = heirarchy[0](x)\n",
    "    if len(heirarchy) == 1:\n",
    "        return a\n",
    "    elif entropy(a) < n_actions//2:\n",
    "        return a\n",
    "    else:\n",
    "        return recursive_action_choice(heirarchy[1:], x)\n",
    "\n",
    "class Heirarchy():\n",
    "    \"\"\"\n",
    "    Residual policy learning.\n",
    "    Learn to act when uncertain.\n",
    "    \n",
    "    \n",
    "    What if each policy were trained at different discounts!?\n",
    "    The first one with low discount.\n",
    "    The next one with higher.\n",
    "    ...?\n",
    "    \n",
    "    Higher up in the heirarchy corresponds to more specific knowledge.\n",
    "    What to do in a specific situation.\n",
    "    More general knowledge. Is lower down.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.policies = [TabularSARSA(n_states, n_actions)]\n",
    "        self.record = []\n",
    "        self.step = 0\n",
    "        \n",
    "    def choose_action(self, s, r):\n",
    "        # want to keep track of which policies are getting called\n",
    "        # and how much\n",
    "        certain = False\n",
    "        counter = 0\n",
    "        while not certain and (counter < len(self.policies)):\n",
    "            pi = self.policies[counter]\n",
    "            qs = pi(s)  # pi(s) returns the Q values\n",
    "            p_a = softmax(qs)\n",
    "            # p is based on the value. \n",
    "            # but this means high entropy can come from having many good options... \n",
    "\n",
    "            if entropy(p_a) < 0.25:\n",
    "                certain = True\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "        a = pi.choose_action(s, r)\n",
    "            \n",
    "        # track which policies are being used\n",
    "        self.record.append(counter)\n",
    "        \n",
    "        # only train the latest policy.\n",
    "        if self.step % 1000 == 0 and self.step > 0:\n",
    "            self.policies.append(TabularSARSA(n_states, n_actions))\n",
    "#         self.policies[-1].choose_action(s, r)\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "n_states = 512\n",
    "env = Env(n_states, n_actions)\n",
    "player = Heirarchy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00214"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_episode(env, player, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5001., 5000., 5000., 5000., 5000., 5000., 5000., 5000., 5000.,\n",
       "        4999.]),\n",
       " array([ 1. ,  5.9, 10.8, 15.7, 20.6, 25.5, 30.4, 35.3, 40.2, 45.1, 50. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(player.policies))\n",
    "plt.hist(player.record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# damn. lower level policies are certain when they shouldnt be...\n",
    "# also the value changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show temporal abstraction with preirarchy.\n",
    "\n",
    "- Learn a prior. In this case, a policy that repeats actions n times.\n",
    "- Learn a new policy, which is regularised with the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyPrior():\n",
    "    def __init__(self):\n",
    "        self.policy = MLP()\n",
    "\n",
    "    def __call__(self, s_t):\n",
    "        return tf.ones(shape=(self.n_actions,))/self.n_actions\n",
    "    \n",
    "class NStepPrior():\n",
    "    def __init__(self, n_actions):\n",
    "        self.n = 2\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def __call__(self, s_t):\n",
    "        # needs to know the most recent action and the time step.\n",
    "        # can cheat and add these to the state vector\n",
    "        # generalising n to values higher than 2 would require storing more actions\n",
    "        a_tm1 = s_t[-1]\n",
    "        t = s_t[-2]\n",
    "        \n",
    "        if t.numpy() % self.n == 0:\n",
    "            return tf.ones(shape=(self.n_actions,))/self.n_actions\n",
    "        else:\n",
    "            return tf.one_hot(int(a_tm1), self.n_actions)\n",
    "        \n",
    "# want other priors can we write down that might help!?\n",
    "# - repeated actions for long periods of time is probably bad?! although, not always...\n",
    "# - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def sample(logits):\n",
    "    g = -tf.log(-tf.log(tf.random.uniform(logits.shape)))\n",
    "    return tf.argmax(logits + g)\n",
    "\n",
    "n_actions = 8\n",
    "prior = NStepPrior(n_actions, 2)\n",
    "a_t = np.random.randint(0, n_actions)\n",
    "for i in range(10):\n",
    "    s_t = tf.cast(tf.constant([0,0,i,int(a_t)]), tf.float32)\n",
    "    a_t = np.random.choice(range(n_actions), p=prior(s_t))\n",
    "    print(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(P, Q):\n",
    "    return -tf.reduce_sum(P * tf.log(Q/P))\n",
    "\n",
    "class Pierarchy():\n",
    "    def __init__(self, prior):\n",
    "        self.prior = prior\n",
    "        self.policy = MLP()\n",
    "        \n",
    "    def __call__(self, s_t):\n",
    "        return softmax(self.policy(s_t))\n",
    "        \n",
    "    def loss_fn(self, s_t, a_t, r_t, R_t):\n",
    "        P = self.__call__(s_t)\n",
    "        Q = self.prior(s_t)\n",
    "        prior_loss = KL(P, Q)\n",
    "        \n",
    "        pg_loss = PG(s, a, R)\n",
    "        \n",
    "        return prior_loss + pg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does better?\n",
    "# - Pierarchy + EntropyPrior\n",
    "# - Pierarchy + NStepPrior\n",
    "# - Pierarchy + Pierarchy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
