{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to show a connection between the prierarchy idea and the composability of LMDPs.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) &= \\mathop{\\text{min}}_a \\big[ r(s, a) + \\mathop{\\mathbb E}_{x' \\sim p(\\cdot | x, a)} v(x') \\big]\\\\\n",
    "p_i(x' | x) &= \\tau(x' | x, \\pi_i(x)) \\\\\n",
    "r(s, a) &= q(x) +  \\mathop{\\mathbb E}_{x'\\sim a(\\cdot | x)} \\log \\frac{a(x' | x)}{p(x' | x)} - \\sum_{i=1}^k \\mathop{\\mathbb E}_{x'\\sim a(\\cdot | x)} \\log p_i(x' | x) \\\\\n",
    "&= q(x) +  \\mathop{\\mathbb E}_{x'\\sim a(\\cdot | x)} \\log \\frac{a(x' | x)}{p(x' | x)\\prod_{i=1}^k p_i(x' | x)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These priors maybe added sequential as training progresses.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-log(x) &= q(x) + \\mathop{\\text{min}}_a \\big[\\mathop{\\mathbb E}_{x'\\sim a(\\cdot | x)} \\log \\frac{a(x' | x)}{z(x')p(x' | x)\\prod_{i=1}^k p_i(x' | x)}\\big]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "???\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d(\\cdot | x) &= \\prod_{i=1}^k p_i(x' | x) \\\\\n",
    "a^{* }(x' | x) &= \\frac{p(x' | x)z(x')d(\\cdot | x)}{G[z](x)} \\\\\n",
    "G[z](x) &= \\mathop{\\mathbb E}_{x'\\sim p(\\cdot | x)d(\\cdot | x)} z(x') \\\\\n",
    "&=  \\mathop{\\mathbb E}_{x'\\sim d(\\cdot | x)} \\mathop{\\mathbb E}_{x'\\sim p(\\cdot | x)}z(x') \\\\\n",
    "z_k(x) &:= \\mathop{\\mathbb E}_{x'\\sim p(\\cdot | x)p_k(\\cdot | x)}z(x') \\\\\n",
    "G[z](x) &= \\sum_k w_k z_k(x') \\\\\n",
    "z(x) &= e^{-q(x)}G[z](x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Want to find a relationship to this.\n",
    "http://papers.nips.cc/paper/3842-compositionality-of-optimal-control-laws.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def onehot(idx, N): # hacky. i know...\n",
    "    return np.eye(N)[idx]\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def sample(x, logits):\n",
    "    # or could use gumbel trick\n",
    "    p = softmax(logits)\n",
    "    return np.random.choice(x, p=p)\n",
    "\n",
    "def entropy(z):\n",
    "    p = softmax(z)\n",
    "    return -np.sum(p*np.log(p)/np.log(n_actions))\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_states (int): the number of states\n",
    "            n_actions (int): the number of action\n",
    "        \"\"\"\n",
    "        self.S = np.arange(n_states)\n",
    "        self.A = np.arange(n_actions)\n",
    "\n",
    "        # each action move from state(i) to state(j) with probability close to 1.\n",
    "        # BUG nope. softmax doesnt do this. will need to set to -infty\n",
    "        self.P = 20*np.stack([np.random.permutation(np.eye(n_states, dtype=np.float32)) for _ in range(n_actions-1)] + [np.eye(n_states, dtype=np.float32)],axis=0)  \n",
    "        # NOTE this graph might be disconnected. but is unlikely!?\n",
    "\n",
    "        # reward is only a fn of the current state - shape = [n_states]\n",
    "        # also. is sparse.\n",
    "        self.R = onehot(np.random.randint(0, n_states), n_states)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        A tabular, probabilistic step function. \n",
    "\n",
    "        Args:\n",
    "            state (int): An element of S. The current state\n",
    "            state (int): An element of A. The action to be taken\n",
    "\n",
    "        Returns:\n",
    "            new_state (int): An element of S.\n",
    "        \"\"\"\n",
    "        # step by selecting relevant transition matrix and applying\n",
    "        logits = np.matmul(self.P[action, ...], onehot(state, n_states))\n",
    "        # convert to a distribution and sample\n",
    "        new_s = np.random.choice(self.S, p=softmax(logits))\n",
    "        return new_s, self.R[new_s]\n",
    "    \n",
    "    def rnd_policy(self, s, *args):\n",
    "        return np.random.choice(self.A)\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.random.choice(self.S)\n",
    "\n",
    "    def new_task(self):\n",
    "        self.R = onehot(np.random.randint(0, n_states), n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularSARSA():\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.gamma = 0.999\n",
    "        self.lr = 0.1\n",
    "\n",
    "        self.qs = np.zeros([n_states, n_actions])\n",
    "        \n",
    "        self.old_s = None\n",
    "        self.old_a = None\n",
    "        self.old_r = None\n",
    "    \n",
    "    def choose_action(self, s, r):\n",
    "        a = sample(np.arange(n_actions), self.qs[s, ...])\n",
    "        # should change to epsilon greedy or entropy regularised\n",
    "        \n",
    "        if self.old_s is not None:\n",
    "            self.train_step(self.old_s, self.old_a, r, s, a)\n",
    "                \n",
    "        # loop past observations so we can use them for training\n",
    "        self.old_s = s\n",
    "        self.old_a = a\n",
    "                \n",
    "        return a\n",
    "    \n",
    "    def __call__(self, s):\n",
    "        return softmax(self.qs[s, ...])\n",
    "    \n",
    "    def train_step(self, old_s, old_a, old_r, s, a):\n",
    "        target = (old_r + self.gamma*self.qs[s, a])  # bootstrap off next step. TD!\n",
    "        delta =  target - self.qs[old_s, old_a]\n",
    "        self.qs[old_s, old_a] += self.lr * delta  # incremental update. exp avg/GD!\n",
    "        self.qs -= 1e-4 * (self.qs-np.mean(self.qs, axis=1, keepdims=True))  # a form of entropy regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, player, len_episode):\n",
    "    # reset\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    r = 0\n",
    "    w = np.random.randint(8)\n",
    "    \n",
    "    # play an episode\n",
    "    for _ in range(len_episode):\n",
    "        a = player.choose_action(s, r)\n",
    "        s, r = env.step(s, a)\n",
    "        total_r += r\n",
    "        \n",
    "    return total_r/len_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 12\n",
    "n_states = 3\n",
    "env = Env(n_states, n_actions)\n",
    "player = TabularSARSA(n_states, n_actions)\n",
    "\n",
    "play_episode(env, player, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_action_choice(heirarchy, x):\n",
    "    # want to keep track of which policies are getting called\n",
    "    # and how much\n",
    "    a = heirarchy[0](x)\n",
    "    if len(heirarchy) == 1:\n",
    "        return a\n",
    "    elif entropy(a) < n_actions//2:\n",
    "        return a\n",
    "    else:\n",
    "        return recursive_action_choice(heirarchy[1:], x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Heirarchy():\n",
    "    \"\"\"\n",
    "    Residual policy learning.\n",
    "    Learn to act when uncertain.\n",
    "    \n",
    "    \n",
    "    What if each policy were trained at different discounts!?\n",
    "    The first one with low discount.\n",
    "    The next one with higher.\n",
    "    ...?\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.policies = [TabularSARSA(n_states, n_actions)]\n",
    "        self.record = []\n",
    "        self.step = 0\n",
    "        \n",
    "    def choose_action(self, s, r):\n",
    "        # want to keep track of which policies are getting called\n",
    "        # and how much\n",
    "        certain = False\n",
    "        counter = 0\n",
    "        while not certain and (counter < len(self.policies)):\n",
    "            pi = self.policies[counter]\n",
    "            \n",
    "            p_a = pi(s)\n",
    "            pi.choose_action(s, r)\n",
    "            if entropy(p_a) < self.n_actions // 10 or (len(self.policies)-1 == counter):\n",
    "                certain = True\n",
    "            \n",
    "            # track which policies are being used\n",
    "            self.record.append(counter)\n",
    "            counter += 1\n",
    "        \n",
    "        a = sample(np.arange(self.n_actions), p_a)\n",
    "        \n",
    "        # only train the latest policy.\n",
    "        if self.step % 1000 == 0 and self.step > 0:\n",
    "            self.policies.append(TabularSARSA(n_states, n_actions))\n",
    "        self.policies[-1].choose_action(s, r)\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "n_states = 128\n",
    "env = Env(n_states, n_actions)\n",
    "player = Heirarchy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "play_episode(env, player, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9000., 7999., 6999., 5999.,    0., 4999., 3999., 2999., 1999.,\n",
       "         999.]),\n",
       " array([0. , 0.8, 1.6, 2.4, 3.2, 4. , 4.8, 5.6, 6.4, 7.2, 8. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(player.record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player.policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# damn. lower level policies are certain when they shouldnt be...\n",
    "# also the value changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
