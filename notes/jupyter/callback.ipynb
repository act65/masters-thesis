{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/act65/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/act65/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import trfl\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs\n",
    "\n",
    "- Test each independently.\n",
    "    - tabular version of action decoding\n",
    "    - unsupervised encoder for mnist\n",
    "- Extend to partial info\n",
    "- Test offline version\n",
    "- Add off policy correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \"\"\"\n",
    "    Vanilla policy trained with A2C.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        self.fn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_actions+1)\n",
    "        ])\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        z = self.fn(x)\n",
    "        a = tfd.Categorical(logits=z[:,:self.n_actions]).sample()\n",
    "        return a, lambda r, tape: self.loss(z, a, r, tape)\n",
    "    \n",
    "    def loss(self, z, a, r, tape):\n",
    "        # TODO add entropy regularisation\n",
    "        loss = trfl.discrete_policy_gradient(z[:,:self.n_actions], a, z[:,-1])\n",
    "        loss += 1e-4*tf.reduce_mean(tf.square(z[:,:self.n_actions]))  # min entropy - L1 or L2??\n",
    "        return loss, self.fn.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action decoder.\n",
    "This policy has little to do with achieving 'extrinsic value'.\n",
    "Its main task is reachability. I want to go to X.\n",
    "This policy should make it happen. \n",
    "\n",
    "Are difference enough to decode actions?\n",
    "Imagine we are doing cart-pole. We observe the angle of the pole.\n",
    "We want to achieve 0 deg, but are currently at 180 deg.\n",
    "So the policy recieves 0-180=-180. That seems to tell me plenty. Rotate! And as the pole is rotate the signal will become less, untill it is zero.\n",
    "\n",
    "#### Relationship to goal conditioned policies!?\n",
    "\n",
    "Property of goal-conditioned policies.\n",
    "Can do meta learning!? If we reward with extrinsic + intrinsic.\n",
    "Might learn that more intrinsic helps get more extrinsic in the long run?!\n",
    "\n",
    "*** \n",
    "\n",
    "In this case bc of the structure of the transition fn, the action decoder/policy receives -dC/dx. <- should have a play with this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(): # aka perception!?\n",
    "    # WANT VQ.\n",
    "    # opt for;\n",
    "    # - high entropy clustering\n",
    "    # - disentanglement\n",
    "    # - sparsity\n",
    "    # - local/multiscale\n",
    "    def __init__(self, n_inputs, n_hidden, width=64):\n",
    "        self.fn = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=[n_inputs]),\n",
    "            tf.keras.layers.Dense(width, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(width, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_hidden)\n",
    "        ])\n",
    "        \n",
    "        # TODO locally structured indexes!?\n",
    "        self.vq = snt.nets.VectorQuantizer(embedding_dim=n_hidden, \n",
    "                                           num_embeddings=32, \n",
    "                                           commitment_cost=1)\n",
    "        \n",
    "        # TODO want an encoder only architecture!?\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=[n_hidden]),\n",
    "            tf.keras.layers.Dense(width, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(width, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_inputs)\n",
    "        ])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = self.fn(x)\n",
    "        z = self.vq(h, True)\n",
    "        return z['quantize'], lambda x_tp1, tape: self.loss(z, x_tp1, tape)\n",
    "        \n",
    "    def loss(self, z_t, x_tp1, tape):\n",
    "        # z_tp1, _ = self.__call__(x_tp1)\n",
    "        x_hat_tp1 = self.decoder(z_t['quantize'])\n",
    "\n",
    "        # optimise for temporal similarity (!?)\n",
    "        loss = tf.losses.mean_squared_error(x_hat_tp1, x_tp1)\n",
    "        \n",
    "        # and high perplexity\n",
    "        # loss -= z_t['perplexity']  # PROBLEM but perpelxity is estimated over batch...\n",
    "        return loss, self.fn.variables + [self.vq.embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, would be a structured representation, but forget that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(): # aka reasoning!?\n",
    "    def __init__(self, n_hidden):\n",
    "        self.energy_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.InputLayer(input_shape=[n_hidden]),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.value_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.InputLayer(input_shape=[n_hidden]),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "                \n",
    "    def __call__(self, x_t, step_size=0.1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_t)\n",
    "            # could learn a controller to pick which aspects to model!?\n",
    "            # reality, ideally, explore, ...\n",
    "            energy = self.energy_fn(x_t) \n",
    "            value = self.value_fn(x_t)\n",
    "            # novelty = self.novelty_fn(x_t)  <- DMs topological memory!? \n",
    "            # (what else might want to use the memory?)\n",
    "            # problem? getting grads for memory look up.\n",
    "            \n",
    "            cost = value - energy\n",
    "            \n",
    "        grad = tape.gradient(cost, x_t)\n",
    "         # ascend value and descend energy\n",
    "        x_hat_tp1 = x_t + step_size*grad[0]\n",
    "        return x_hat_tp1, lambda x_tp1, r, tape: self.loss(x_t, x_tp1, x_hat_tp1, \n",
    "                                                           value, r, tape)\n",
    "        \n",
    "    def loss(self, x_t, x_tp1, x_hat_tp1, v_t, r_t, tape):\n",
    "        # observations should have low energy\n",
    "        # not sure how to optimise for that!?\n",
    "        # for now. optimise E, V for accuracy\n",
    "        loss_energy = tf.losses.mean_squared_error(x_tp1, x_hat_tp1)\n",
    "        # PROBLEM optimising energy_fn for acc doesnt really make sense.\n",
    "        # is confounded by the actions taken\n",
    "        \n",
    "        # value should predict future rewards\n",
    "        v_tp1 = self.value_fn(x_tp1)\n",
    "        loss_value = tf.losses.mean_squared_error(v_t, r_t+self.gamma*v_tp1)  \n",
    "        # TODO is there a more general way to view this? \n",
    "        # where energy, value, ... can be viewed as cumulants? GVFs?\n",
    "\n",
    "        losses = [loss_value, loss_energy]\n",
    "        variables = [self.value_fn.variables, self.energy_fn.variables]\n",
    "\n",
    "        return losses, variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could add another 'intrinsic' value fn here. to encourage exploration.\n",
    "\n",
    "also should explore energy via distribution vs not?\n",
    "\n",
    "seems weird. how I am training this. the energy fn will never be able to \n",
    "achieve high next step prediction accuracy unless it has a model of V!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self, n_inputs, n_actions, n_hidden):\n",
    "        self.encoder = Encoder(n_inputs, n_hidden)\n",
    "        self.transition = Transition(n_hidden)\n",
    "        self.policy_fn = Policy(n_actions)\n",
    "        # does this need memory?\n",
    "        # the ability to integrate the deltas?\n",
    "        # the ability to remember the past?\n",
    "        # will be a pain for training...\n",
    "        \n",
    "        # not going to work online - credits assignment! need RTRL?!\n",
    "        self.rnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(n_hidden, stateful=True)\n",
    "        ])\n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer()\n",
    "        self.step = tf.train.get_or_create_global_step()\n",
    "                \n",
    "    def __call__(self, x_t):\n",
    "        \"\"\"\n",
    "        Handles training and prediction.\n",
    "        \"\"\"\n",
    "        # and/or could use a worker to collect data and train offline...\n",
    "        # BUT. how to combine both!? will need to aggregate params somehow. or stop playing.\n",
    "        # i believe all fns are currently compatible with online or offline\n",
    "        \n",
    "        # key to good online learning is a good exploration policy that\n",
    "        # produces a uniform distribution of ...\n",
    "        \n",
    "        h_t, encoder_callback = self.encoder(x_t)  # encode new info\n",
    "        s_t = self.rnn(tf.expand_dims(h_t, 1))  # integrate with current beliefs about the state\n",
    "        # TODO local -> global representations of state.\n",
    "        s_hat_tp1, transition_callback = self.transition(s_t)\n",
    "        a, policy_callback = self.policy_fn(s_t - s_hat_tp1)  # is this enough info? or do we need abs info?\n",
    "        \n",
    "        def callback(x_tp1, r, tape):\n",
    "            # this callback is nice bc we dont have to do any recompute\n",
    "            s_tp1, _ = self.encoder(x_tp1)\n",
    "            \n",
    "            encoder_loss, encoder_vars = encoder_callback(x_tp1, tape)\n",
    "            transition_losses, transition_vars = transition_callback(s_tp1, r, tape)\n",
    "            \n",
    "            # use the transtition loss the the reward for the policy.\n",
    "            # but high loss means the policy could not achieve its target.\n",
    "            # but is also good as maybe we experienced something novel!? what is the difference?\n",
    "            policy_loss, policy_vars = policy_callback(tf.stop_gradient(-transition_losses[1]), tape)\n",
    "                \n",
    "            lnvs = [\n",
    "                (encoder_loss, encoder_vars),\n",
    "                (policy_loss, policy_vars)\n",
    "            ]\n",
    "            \n",
    "            losses, variables = zip(*lnvs)\n",
    "            losses = list(losses) + transition_losses\n",
    "            variables = list(variables) + transition_vars\n",
    "            \n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                names = ['enc', 'policy', 'value', 'acc']\n",
    "                for name, loss in zip(names, losses):\n",
    "                    tf.contrib.summary.scalar(name, loss)\n",
    "\n",
    "            grads = tape.gradient(list(losses), list(variables))\n",
    "            gnvs = zip(grads, variables)\n",
    "            gnvs = [(g, v) if g is not None else (tf.zeros_like(v), v)\n",
    "                    for G, V in gnvs for g,v in zip(G, V)]\n",
    "\n",
    "            # PROBLEM!? not sure....\n",
    "#             gnvs = [(g, v) for G, V in gnvs for g,v in zip(G, V)]    \n",
    "#             count = sum([1 if g is None else 0 for g,v in gnvs])\n",
    "#             print(count)\n",
    "#             raise SystemExit\n",
    "            \n",
    "            \n",
    "            self.opt.apply_gradients(gnvs, global_step=self.step)\n",
    "        \n",
    "        return a, callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/act65/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "obs = env.reset()\n",
    "\n",
    "writer = tf.contrib.summary.create_file_writer('/tmp/net/0')\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/act65/anaconda3/lib/python3.6/site-packages/sonnet/python/modules/nets/vqvae.py:62: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "player = Net(8, env.action_space.n, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode():\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with tf.GradientTape() as tape:\n",
    "            action, callback = player(tf.constant(obs, dtype=tf.float32, shape=[1, 8]))\n",
    "            env.render()\n",
    "            obs, reward, done, info = env.step(action.numpy()[0])\n",
    "\n",
    "            callback(tf.constant(obs, dtype=tf.float32, shape=[1, 8]), \n",
    "                     tf.constant(reward, dtype=tf.float32, shape=[1, 1]), \n",
    "                     tape)\n",
    "            R += reward\n",
    "        \n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
    "            tf.contrib.summary.scalar('R', R)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
