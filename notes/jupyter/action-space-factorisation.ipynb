{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "n_states = 12\n",
    "episode_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(elems, T):\n",
    "    def recurse(xs):\n",
    "        if len(xs[0]) >= T:\n",
    "            return xs\n",
    "        else:\n",
    "            return recurse([x+[e] for x in xs for e in elems]) # + xs\n",
    "            \n",
    "    return recurse([[e] for e in elems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    }
   ],
   "source": [
    "elems = list(range(n_actions))\n",
    "trajectories = np.array(permutations(elems, episode_len))\n",
    "n_trajectories = trajectories.shape[0]\n",
    "print(n_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories  # think about these as sequences of actions or as sequences of states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume rewards are given for being in state s.\n",
    "\n",
    "(can I have an example of rewards as a fn of $r(s,a)$ state and action!? is it not just equivalent to $r(s_{t+1})$? not if transitions are stochastic. but surely we only reward the actionif it actually does something to the state!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = np.zeros((1, n_states))\n",
    "rewards[0,1]= 1\n",
    "rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00035946, 0.00031307, 0.00314563, ..., 0.00359513, 0.00097901,\n",
       "       0.00104118])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# via policy evaluation\n",
    "# a distribution over the possible trajectories\n",
    "\n",
    "observed_rewards = rnd.choice([0.0,1.0], size=(1, n_trajectories), p=[0.9, 0.1])\n",
    "# many trajectories might arrive in the same state and thus receive the same reward.\n",
    "\n",
    "p_traj = np.exp(rnd.standard_normal((n_trajectories)))\n",
    "p_traj /= p_traj.sum()\n",
    "p_traj\n",
    "\n",
    "# value = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two trajectories can be factorised if;\n",
    "\n",
    "$$\n",
    "t_1 = t_2 \\;\\;\\text{if}\\\\\n",
    "\\sum_{i=0}^t \\gamma^t r(s_{t_1[i]}) = \\sum_{i=0}^t \\gamma^tr(s_{t_2[i]})  \\\\\n",
    "\\text{Invariance criteria} \\\\\n",
    "V(t_1) = V(t_2) \\\\\n",
    "s_{t_1[T]} = s_{t_2[T]} \\\\\n",
    "$$\n",
    "\n",
    "But, stochasticity changes everything... Unlikely even that $V(t_1) = V(t_1)$... or $s_{t_1[T]} = s_{t_1[T]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_options = 8\n",
    "options = np.arange(n_options)\n",
    "\n",
    "# want to find a mapping from actions to options. a factorisation!?\n",
    "# s.t. options = A.T = np.dot(abstraction, trajectories). \n",
    "# abstraction = (n_options x n_trajectories)\n",
    "# s.t. the rows of A are sparse (only contain a 1 in one column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 5), (8,))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.shape, options.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Incompatible dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-fb9fbd5614d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/scratch/miniconda3/envs/venv/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0mm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Incompatible dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Incompatible dimensions"
     ]
    }
   ],
   "source": [
    "np.linalg.lstsq(trajectories, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reward is independent of / invariant to a dimension is the state space then we can factorise the action space!!!\n",
    "\n",
    "Eg. Eating candy. Still rewarded whether legs are folded or not. Or whether inside or out. Etc.\n",
    "\n",
    "This we don't have to consider actions that only change the invariant dimensions.\n",
    "\n",
    "\n",
    "Assume $r(\\cdot)$ is invariant to noise / changes in the $i$th dimension. Aka, there exits $i$ s.t. $r(s) = r(s + ne_i), n\\sim \\mathcal N$.\n",
    "\n",
    "This means that;\n",
    "- any two (sub) trajectories that vary in only the $i$th dimension can be ???\n",
    "- any action that only changes the state in the $i$th dimension can be temporally abstracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimate goal is to find a single button/action/subgoal/option that maximises instantaneous and future reward. We can just keep hitting that button.\n",
    "\n",
    "To see the future soo clearly that we can pick a policy for the next 10 years. And achieve max reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of temporally abstracted options, there are may redundancies.\n",
    "\n",
    "For example, imagine we have the action, step left (l), step right (r) and do nothing (n).\n",
    "Then we might have the case where;\n",
    "\n",
    "- llr =  rll = lrl = lnn = nln = nnl (ie the order doesnt matter).\n",
    "\n",
    "But sometimes the order may matter. We might have stop (s), drop (d) and roll (r).\n",
    "\n",
    "\n",
    "\n",
    "What other 'higher' order symmetries might there be within sequences of actions. Inter subset symmetries of actionscan be; \n",
    "\n",
    "- permuted (any permutation)\n",
    "- mirrored (abcd = dcba)\n",
    "- rotated (abcd = dabc)\n",
    "- ?\n",
    "\n",
    "And intra subset symmetries of actions can have symmetries\n",
    "\n",
    "- mirrored (AasdsfgfB = BasdsfgfA) where A=sdf, B=rge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High freq setting.\n",
    "\n",
    "Imagine a setting where you ahve access to hundreds of actions. But only a few actually move you through the maze (left, right, up, down). Each of these moves takes ~n steps to execute. Other (non movement) actions can be taken during their execution.\n",
    "\n",
    "This problem is a lot harder (how much???) than the original (left, right up down).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subgoal (that is to be reached in k steps) defines a subset of possible trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(elems, T):\n",
    "    def recurse(xs):\n",
    "        if len(xs[0]) >= T:\n",
    "            return xs\n",
    "        else:\n",
    "            return recurse([x+[e] for x in xs for e in elems]) # + xs\n",
    "            \n",
    "    return recurse([[e] for e in elems])\n",
    "\n",
    "class RndOptionPolicy():\n",
    "    def __init__(self, n_actions, n_time_steps):\n",
    "        self.options = np.array(permutations(range(n_actions), n_time_steps))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        rnd_idx = rnd.choice(np.arange(self.options.shape[0]))\n",
    "        return self.options[rnd_idx]\n",
    "     \n",
    "class OptionEnvWrapper():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, s, actions):\n",
    "        R = 0\n",
    "        for a in actions:\n",
    "            s, r = self.env.step(s, a)\n",
    "            R += r\n",
    "        return s, R\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def onehot(idx, N): # hacky. i know...\n",
    "    return np.eye(N)[idx]\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_states = n_states\n",
    "        self.S = np.arange(n_states)\n",
    "        self.A = np.arange(n_actions)\n",
    "\n",
    "        # model the transitions as linear fns conditional on the action.\n",
    "        # P = np.random.standard_normal([n_actions, n_states, n_states]) **2 # make sharper\n",
    "\n",
    "        # deterministic transition fn\n",
    "        # each action move from state(i) to state(j) with probability 1.\n",
    "        # BUG nope. softmax doesnt do this. will need to set to -infty\n",
    "        self.P = 20*np.stack([np.random.permutation(np.eye(n_states, dtype=np.float32)) for _ in range(n_actions-1)] + [np.eye(n_states, dtype=np.float32)],axis=0)  \n",
    "        # TODO what if there is structure in P? Low rank? Shared over actions?\n",
    "        # QUESTION how does this parameterisation effect things?\n",
    "        # NOTE this graph might be disconnected. but is unlikely!?\n",
    "\n",
    "        # reward is only a fn of the current state - shape = [n_states]\n",
    "        # also. is sparse.\n",
    "        self.R = onehot(np.random.randint(0, n_states), n_states)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        A tabular, probabilistic step function. \n",
    "\n",
    "        Args:\n",
    "            state (int): An element of S. The current state\n",
    "            state (int): An element of A. The action to be taken\n",
    "\n",
    "        Returns:\n",
    "            new_state (int): An element of S.\n",
    "        \"\"\"\n",
    "        # step by selecting relevant transition matrix and applying\n",
    "        logits = np.matmul(self.P[action, ...], onehot(state, self.n_states))\n",
    "        # convert to a distribution and sample\n",
    "        new_s = np.random.choice(self.S, p=softmax(logits))\n",
    "        return new_s, self.R[new_s]\n",
    "    \n",
    "    def rnd_policy(self, s, *args):\n",
    "        return np.random.choice(self.A)\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.random.choice(self.S)\n",
    "\n",
    "    def new_task(self):\n",
    "        self.R = onehot(np.random.randint(0, self.n_states), self.n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a7e1dda72684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrnd_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRndOptionPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fea7346f7b4d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_states, n_actions)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "n_actions = 4\n",
    "n_states = 24\n",
    "env = Env(n_states, n_actions)\n",
    "rnd_policy = lambda obs: env.action_space.sample()\n",
    "op = RndOptionPolicy(n_actions, 6)\n",
    "env = OptionEnvWrapper(env)\n",
    "len(op.options), op(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, player, T=5):\n",
    "    # reset\n",
    "    s = env.reset()\n",
    "    R = 0\n",
    "    done = False\n",
    "    pairs = []\n",
    "    \n",
    "    # play an episode\n",
    "    for _ in range(10):\n",
    "\n",
    "        a = player(s)\n",
    "        new_s, r = env.step(s, a)\n",
    "        R += r\n",
    "        \n",
    "        pairs.append((np.concatenate([onehot(new_s, n_states), np.array([0])]), \n",
    "                      a, \n",
    "                      np.concatenate([onehot(new_s, n_states), np.array([R])])))\n",
    "        s = new_s\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs():\n",
    "    pairs = play_episode(env,op)\n",
    "    pairs = tuple(zip(*pairs))\n",
    "    return tuple([np.vstack(p) for p in pairs])\n",
    "\n",
    "def get_n(n):\n",
    "    pairs = [get_pairs() for i in range(n)]\n",
    "    pairs = tuple(zip(*pairs))\n",
    "    return tuple([np.vstack(p) for p in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
