{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import faiss                   # make faiss available\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember everything.\n",
    "- When asked to make an action, look up its value from past experiences.\n",
    "- Distill the expensive memoriser into a policy NN.\n",
    "\n",
    "Also https://arxiv.org/pdf/1702.08833.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_state = 8\n",
    "d_hidden = 4\n",
    "\n",
    "data = [(np.random.standard_normal((1, d_state)).astype(np.float32),\n",
    "        np.random.randint(0, 4, size=(1,1)),\n",
    "        np.random.random((1,1)))\n",
    "        for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH():\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        self.W = np.random.standard_normal((input_dims, output_dims)).astype(np.float32)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return np.dot(x, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07962756,  3.997297  ,  1.6272243 , -2.9108546 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "lsh = LSH(d_state, d_hidden)\n",
    "lsh(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memorizer():\n",
    "    def __init__(self, input_dims, n_actions, hidden_dims):\n",
    "        self.n_actions = n_actions\n",
    "#         self.buffer = deque\n",
    "        self.k = 5\n",
    "        \n",
    "        # f: S -> idx\n",
    "        self.lsh = LSH(input_dims+n_actions, hidden_dims)\n",
    "        \n",
    "        # build the index\n",
    "        self.index = faiss.IndexFlatL2(hidden_dims)\n",
    "        \n",
    "        # to store the values\n",
    "        self.values = dict()\n",
    "        \n",
    "    def __call__(self, s):\n",
    "        # calculate hash for all state-action combinations\n",
    "        a = np.arange(self.n_actions)\n",
    "        s = np.vstack([s for _ in range(self.n_actions)])\n",
    "        x = bundle(s, a, self.n_actions)\n",
    "        h = self.lsh(x)\n",
    "        \n",
    "        # find the nearest neighbors\n",
    "        # n_args x n_neighbors (n_args = first dim of h)\n",
    "        dist, indexes  = self.index.search(h, self.k)\n",
    "        \n",
    "        # index the value for the nearest neighbor.\n",
    "        # n_actions x n_neighbors\n",
    "        # TODO want a better way to index the values\n",
    "        vs = np.array([[self.values[idx] for idx in indexes[:, i]] \n",
    "                         for i in range(self.k)]).squeeze().T\n",
    "        # softmax over neighbors. the neighbor with the;\n",
    "        # - smallest dist exponentially dominates\n",
    "        # - largest value exponentially dominates\n",
    "        vs_adj = np.einsum('ij,kj->i', softmax(vs), softmax(-dist))\n",
    "        return np.argmax(vs_adj)\n",
    "        \n",
    "    def add(self, traj):\n",
    "        s, a, r = tuple([np.vstack(val) for val in zip(*traj)])\n",
    "        x = bundle(s, a, self.n_actions)\n",
    "        # v = discounted_rewards(r)\n",
    "        v = r\n",
    "        h = self.lsh(x)\n",
    "        init_idx = self.index.ntotal\n",
    "        self.index.add(h)\n",
    "        final_idx = self.index.ntotal\n",
    "        \n",
    "        for i, v_t in zip(range(init_idx, final_idx), r):\n",
    "            self.values[i] = v_t\n",
    "            \n",
    "    def compress(self):\n",
    "        # find dense regions and summarise them.\n",
    "        \n",
    "        # could do average. not sure it makes sense.\n",
    "        # take value to be the max so far\n",
    "        # self.values[idx, a_t] = np.maximum(self.values[idx, a_t], v_t)\n",
    "        pass\n",
    "        \n",
    "    def balance(self, gen):\n",
    "        gen_copy = copy.deepcopy(gen)\n",
    "        self.hash.train(gen)\n",
    "        \n",
    "        self.index.reset()\n",
    "        self.values.reset()\n",
    "        self.add(gen_copy)\n",
    "        \n",
    "        # reset value memory.\n",
    "        # TODO but only if we have actually changed the hash...\n",
    "        # when balancing. want to hashes with low churn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x, N):\n",
    "    return np.eye(N)[x].astype(np.float32)\n",
    "\n",
    "def bundle(s, a, N):\n",
    "    B = s.shape[0]\n",
    "    return np.concatenate([s, onehot(a, N).reshape((B, N))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Memorizer(d_state, 4, d_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.standard_normal((1, d_state)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "player(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehavourialCloner():\n",
    "    # Can use behaviourial cloning techniques here?!\n",
    "    # is there some fancy stuff?!\n",
    "    def __init__(self):\n",
    "        self.policy = make_NN()\n",
    "    \n",
    "    def __call__(self, s):\n",
    "        return sample(self.policy(s))\n",
    "    \n",
    "    def loss_fn(self, s, a_star):\n",
    "        logits = self.policy(s)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=a_star, logits=logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def train(self, generator):\n",
    "        for s, a in generator:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.loss_fn(s, a)\n",
    "            grads = tape.gradient(loss, self.policy.variables)\n",
    "            self.opt.apply_gradients(zip(grads, self.policy.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self):\n",
    "        self.buffer = Buffer()\n",
    "        self.memorizer = Memorizer()\n",
    "        self.policy = BehavourialCloner()\n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        return self.policy(s)\n",
    "        \n",
    "    def sleep(self):\n",
    "        # \n",
    "        s = self.buffer.get_states()\n",
    "        a = self.memorizer(s)\n",
    "        self.policy.train(zip(s, a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
