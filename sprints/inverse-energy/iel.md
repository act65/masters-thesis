# IEL

The setting I want to capture is: Given many different observations, we are able to _explain_$^{* }$ them using a simple energy function (or 'principle').

We are given access to a set of optimal trajectories, $\tau_k = \{x_0, \dots, x_t, \dots, x_T\}, \{k \in 1:K\}$ that are assumed to be generated by taking optimal actions under an energy function. We interpret optimal actions to be steps via gradient descent (rather than the maximisation of the bellman operator).

Let $E$ be the energy function being minimised, and let $x_t$ be our observations (and/or the state) at time $t$.

$$
\begin{align}
x_{t+1} &= f(x_t) \\
x_{t+1} &= x_t - \eta \nabla E(x_t) \\
\end{align}
$$

Our goal is to learn an energy fuction $E(x)$ and a set of initial conditions $\{z_0, \dots, z_K\}$ such that minimising $E$ initialised at $z_k$ yields the trajectory $\tau_k$.

_(I also want to capture a notion of controls that are easier/harder. Sometimes certain variables are hard to control.)_

## Related work

This setting above is related to [Efficient Encoding of Dynamical Systems
through Local Approximations](https://arxiv.org/abs/1805.09714) which seeks to encode a step function as a set of linear functions and anchors (rather than initialisations paired with an energy function).

We can always convert an energy into a __density__ via the Boltzmann distribution (although the normalisation needs to be tractable). Where the samples correspond to the likely observations under minimisation dynamics on $E$.

$$
p(x) =\frac{e^{-E(x)}}{\int_i e^{-E(x_i)}} \\
$$

Conversely it is possible to assume the data you observe $\{\{x^0_i, \dots, x^t_i\}: i\in [1:N] \}$ are the result of Langevin dynamics on a density. This allows you to solve a maximisation problem to recover samples from $p(x)$.

$$
\begin{align}
\Delta x &= \frac{\alpha}{2} \nabla_x p(x) + z \tag{$z\sim N(0, \alpha)$}\\
\end{align}
$$

- It would be interesting to explore the relationship between Langevin optimisation and sampling!? ([welling](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf))
- Which is more general? Energy or density?


Approaches to __Energy-based ML__ often use a fixed measure of energy, for example, Boltzmann machines use:

$$
E(u) =-\frac{1}{2} \sum_{i}\sum_{j\neq i} W_{ij}\rho(u_i)\rho(u_j) - \sum_i b_i\rho(u_i)\\
$$

This equation governs the dynamics, and ultimately the prediction of the learner/network. The difference to IEL is simply that we use a different parameterisation of the energy function.

[Concept Learning with Energy-Based Models](https://arxiv.org/abs/1811.02486) is a really nice paper, and is better than what I wanted to write. Fortunately, there is still remaining work: answer a few theoretical questions that would motivate the use of IEL, scale it to real world problems.

__IRL__ (see `irl.md`)

## Theory

- Is it a universal approximator? (what can we not do with it?)
- In which cases does it work (better)?
- Does it actually aid transfer? When?
- How can two energies be combined sensibly?
- What are the dynamics of learning $E$?

### Gradients

Ok, what do the gradients of IEL look like?

$$
\begin{align}
f_{\theta}(x_t) &= x_t - \eta \frac{\partial E_{\theta}}{\partial x} \tag{by defn}\\
\frac{\partial f_{\theta}}{\partial \theta} &= -\eta\frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{diff wrt params}\\
\frac{\partial L}{\partial \theta} &= \frac{\partial L}{\partial f_{\theta}} \cdot \frac{\partial f_{\theta}}{\partial \theta} \tag{chain rule}\\
&=  \frac{\partial L}{\partial f_{\theta}} \cdot -\eta\frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{substitution}\\
\frac{d \theta}{d t} &= -\alpha\frac{\partial L}{\partial \theta} \tag{GD}\\
&=  \alpha\eta \frac{\partial L}{\partial f_{\theta}} \cdot \frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{substitution}\\
\end{align}
$$

## Local energies

I am espeically curious about how global energies can be composed of local energies that are invariant to certain tranformations. (In my mind, this relates to emergent phenomena and physical laws.)

$$
\begin{align}
E(s) &= \sum_{i \dots k \subseteq I} E(s_{i \dots k}) \\
\end{align}
$$

Let $s$ be a n dimensional tensor $\in R^n$. Then the global energy could be written as the sum over many dimensions (such as space or time) of the local energies.

All possible states can be described by $s\in S \subset R^n$. The env is described by a set of all possible states, $S$. The current state is an element of that set. For example, a tensor of real values (representing, say, a map).

__But.__
- __Q:__ How do we construct $s$ in the first place to have locally structured dimensions?
- __Q:__ How do we know which dimensions to share the loss over?

What if $x$ was a graph instead? A dimension would be an ordering of the nodes (ordered by reachability or proximity in time).
How do you learn that the x and y dimensions???

- In boids the dimension we are using is for each 'bird'.
- In colloids the dimension we are using is each 'colloid'.
- In classical physics the dimension we are using is time (???). https://en.wikipedia.org/wiki/Principle_of_least_action
- https://en.wikipedia.org/wiki/Gauss%27s_principle_of_least_constraint
- Spin glasses? Phase transitions? Tensor networks and renormalisation?

Clustering the nodes in the graph by similarity (within subset of their embeddings) could find these dimensions!? (at least the boids/colloids?)

#### Other thoughts

- How do local energies add representational power?
- Is it possible to get non-linear behaviour from many locally convex energy fns? (yes?)
- If we used many IEL/IRL modules, would they learn to model the agents acting in say PACman, independently capturing each value function?

#### Experimental setting.

Learn from the Ising model. The would be state being in 2d $s_t \in \mathbb R^{n \times n}$, while observations are made in $x_t \in \mathbb R^{k \times k}$.

Actions:

- Change the temperature (1d cts),
- Apply a magnetic field (smooth vector field),
- Change the viewpoint (2d cts).

Goal is to recover a succinct description of the underlying physics.

(_similarly, we could apply to Boids, The game of life, colloids. Each depends on local rules to determine time evolution. We want to recover those local rules._)

### Refs

- [OpenAI EBM](https://arxiv.org/abs/1811.02486)
- [EBMs review](https://arxiv.org/abs/1708.06008)
- [GANs, IRL, EBMs](https://arxiv.org/abs/1611.03852)
- [Tutorial](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)
- [BM for time-series](https://arxiv.org/pdf/1708.06004.pdf)
