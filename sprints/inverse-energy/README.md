> 7. Inverse energy learning. Inspired by inverse reinforcement learning [@Ng2000AlgorithmsFI], what if we assume that the observations we make are the results of some optimal action, in this case, of an energy function being minimised.

We are given access to a set of optimal trajectories, $\tau_k = \{x_0, \dots, x_t, \dots, x_T\}$ that are assumed to be generated by taking optimal actions under an energy function. We interpret optimal actions to be steps via gradient descent (rather than the maximisation of the bellman operator).

Let $E$ be the energy function being minimised, and let $x_t$ be our observations (and/or the state) at time $t$.

$$
\begin{align}
x_{t+1} &= f(x_t) \\
x_{t+1} &= x_t - \eta \nabla E(x_t) \\
\end{align}
$$

The learner's goal is to recover $E$ from the observed optimal trajectories.

### 1) Motivate the idea as a solution to an existing problem

__Claim.__ A energy function will give a more efficient, succinct, robust and transferrable representation that a step function.

__Efficiency.__

- `energy -> step` It is easy to calculate steps from the energy fn. $s_{t+1} = f(s_t) = s_t - \eta \nabla E(s_t)$
- `step -> energy` It is not easy to calculate the energy with the step function. You would need to integrate. _(well, this might not matter. the computational cost of an expernsive operation that is never used is zero... Ok. this is interesting!! what do we need the energy function for? <- facilitates efficient planning. you are playing with another powerful agent. you know what they want. when planning you only bother searching for solutions that also satisfy what the other agent wants)_

<font color='red'>TODO</font> want to make this argument more formal! Comptational complexity.


__Generalisation.__

If the system being modelled is truly minimising an energy function, then using the right parameterisation will give a good inductive bias for learning, making it more efficient _(but is the gain in efficiency actually significant?)_.

<font color='red'>TODO</font> want to make this argument more formal! Sample complexity.

__Transfer.__ A communication problem between teacher and student.

Info required to communicate E is much more that dE!?

<font color='red'>TODO</font> want to make this argument more formal! Communication complexity.

__Composable.__

Using $f(s_t) = g_1(s_t) + g_2(s_t)$ will not make sense. But following the gradient of $E(s_t) = E_1(s_t) + E_2(s_t)$ will make sense.

_(hmm. i doubt this is true!? What assumptions does this actually require? How does it restrict the actual step fn?_)

### 2) Demonstrate that the "existing" problem really exists

Why is learning an energy function hard? Is it even hard?

Ok. How can this be done? Show that step fns do not acheive a very close to what is possible.

Under-constrained problem. Many possible energy functions could fit the observations. (but many more step functions?)

### 3) Generate alternative solutions and a suitable baseline

Compare:

- vanilla neural network,
- residual neural network,
- IEL,
- ?.


### 4) Design the minimal viable experiment to falsify the proposed solution

- Learn a simple 1d energy function using observations of its gradients.
- Learn a n-d energy function from trajectories of gradient descent.
- Grid world experiment?
- partial observations via a projection matrix!?

Want a set of simple trajectories, agent moves in a straight line towards its goal. E should learn to measure the (squared) distance!?


### Thoughts

- IEL is interesting if approached in the multi agent setting.
- There is a bunch of theoritical work missing that would motivate the exploration of IEL. In which cases are energies better!?
