> 7. Inverse energy learning. Inspired by inverse reinforcement learning [@Ng2000AlgorithmsFI], what if we assume that the observations we make are the results of some optimal action, in this case, of an energy function being minimised.

We are given access to a set of optimal trajectories, $\tau_k = \{x_0, \dots, x_t, \dots, x_T\}$ that are assumed to be generated by taking optimal actions under an energy function.

We interpret optimal actions to be steps via gradient descent (although, maybe we could do better than this).

Let $E$ be the energy function being minimised, and let $x_t$ be our observations (and/or the state) at time $t$.

$$
\begin{align}
x_{t+1} &= f(x_t) \\
x_{t+1} &= x_t - \eta \nabla E(x_t) \\
\end{align}
$$

OR

$$
\begin{align}
x_{t+1} &\sim \frac{e^{-E(x)}}{\sum_j e^{-E(x_i)}}
\end{align}
$$

How are these related!? Both will find the minima. (?).


### Motivate the idea as a solution to an existing problem

What is the best way to learn a model?
What if that model is structured as an energy function? (how common is this?)

##### IRL

IRL allows a you to observe the optimal policy and learn the value function. This is the inverse of the usual RL set up where, you are given the reward function and must learn the optimal policy.

In nature, we commonly observe the optimal policy under the [principle of least action](https://en.wikipedia.org/wiki/Principle_of_least_action). The lagrangian defines an energy function, say of ... which is minimised.

In economies (or social environments) there are many agents with their own agendas. We tend to take actions that maximise our own rewards.

##### Time series.

Unsupervised learning for time series. How can this be done? It is often done by modelling the time series as being generated by a step function $s_{t+1} = f(s_t)$



***


1) Why is it better to have the energy function rather than the step function?

- Easy to calculate steps from the energy fn. Only rquires differentiation (which is O(?))
- Not easy to calculate the energy with only the step function. Need to integrate over ...?!?

TODO want to make this argument more formal! comptational complexity.

2) Generalise better.

?!?!?

3) A communication problem between teacher and student.

Info required to communicate E is much more that dE!?

### Demonstrate that the "existing" problem really exists

Why is learning an energy function hard? Is it even hard?

Under-constrained problem. Many possible energy function that could fit the observations.


### Generate alternative solutions and a suitable baseline

Compare:

- vanilla neural network,
- residual neural network,
- IEL,
- ?.

Possible ways to train the learned energy?

- Max likelihood - $\mathop{\text{argmax}}_{\theta} \prod_{i=1}^T p(x_i \mid x_{i-1}: x_0)$
- Squared error - $\mathop{\text{argmin}}_{\theta} \parallel f(x_t) - x_{t+1} \parallel$

### Design the minimal viable experiment to falsify the proposed solution

- Learn a simple 1d energy function using observations of its gradients.
- Learn a n-d energy function from trajectories of gradient descent.
- Grid world experiment?
- partial observations via a projection matrix!?

Want a set of simple trajectories, agent moves in a straight line towards its goal. E should learn to measure the (squared) distance!?

### Implement the experiment if feasible
