# IEL

We are given access to a set of optimal trajectories, $\tau_k = \{x_0, \dots, x_t, \dots, x_T\}$ that are assumed to be generated by taking optimal actions under an energy function. We interpret optimal actions to be steps via gradient descent (rather than the maximisation of the bellman operator).

Let $E$ be the energy function being minimised, and let $x_t$ be our observations (and/or the state) at time $t$.

$$
\begin{align}
x_{t+1} &= f(x_t) \\
x_{t+1} &= x_t - \eta \nabla E(x_t) \\
\end{align}
$$

The learner's goal is to recover $E$ from the observed optimal trajectories.

## Related work

Unsupervised learning for time series. How can this be done? It is often done by modelling the time series as being generated by a step function $s_{t+1} = f(s_t)$

What is the difference between learning a __distribution__ and learning an energy?!? Obviously, the distribution needs to be normalised. But is that the only difference? We can always convert an energy into a distribution (although the normalisation needs to be tractable).

$$
p(x) =\frac{e^{-E(x)}}{\int_i e^{-E(x_i)}} \\
$$


__Energy-based ML__ use a fixed measure of energy, for example, RBMs:

$$
E(u) =-\frac{1}{2} \sum_{i}\sum_{j\neq i} W_{ij}\rho(u_i)\rho(u_j) - \sum_i b_i\rho(u_i)\\
$$

This equation governs the dynamics, and ultimately the prediction of the network.

[Concept Learning with Energy-Based Models](https://arxiv.org/abs/1811.02486) give a good proof of concept that IEL can work, but in my mind the remaining work is: answer a few theoretical questions, scale it to real world problems.

## Setting

Given many different observations. We are able to _explain_$^{* }$ them using a simple energy function (or principle).

We receive a set of observations $\{x_0, \dots, x_n \in X\}$ that are the result of some known dynamical system $\frac{dx}{dt} = f(x_t)$. Our goal is to learn an energy fuction $E(x)$ and a set of initial conditions $\{z_0, \dots, z_n\}$ such that $x_0 \mathop{\text{min}}_x E(x) \text{ s.t. init } x=z_0$ (the steps taken dont need to be GD steps of down E).

_(I also want to capture a notion of controls that are easier/harder. Sometimes certain variables are hard to control.)_

This setting is closely related to [Efficient Encoding of Dynamical Systems
through Local Approximations](https://arxiv.org/abs/1805.09714) which seeks to encode the transition function as a set of linear functions and anchors (rather than initialisations and an energy function).


## Gradients

Ok, what do the gradients of IEL look like?

$$
\begin{align}
f_{\theta}(x_t) &= x_t - \eta \frac{\partial E_{\theta}}{\partial x} \tag{by defn}\\
\frac{\partial f_{\theta}}{\partial \theta} &= -\eta\frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{diff wrt params}\\
\frac{\partial L}{\partial \theta} &= \frac{\partial L}{\partial f_{\theta}} \cdot \frac{\partial f_{\theta}}{\partial \theta} \tag{chain rule}\\
&=  \frac{\partial L}{\partial f_{\theta}} \cdot -\eta\frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{substitution}\\
\frac{d \theta}{d t} &= -\alpha\frac{\partial L}{\partial \theta} \tag{GD}\\
&=  \alpha\eta \frac{\partial L}{\partial f_{\theta}} \cdot \frac{\partial^2 E_{\theta}}{\partial x \partial \theta} \tag{substitution}\\
\end{align}
$$


## Densities

Assume the data you observe $\{\{x^0_i, \dots, x^t_i\}: i\in [1:N] \}$ are the result of Langevin dynamics on a density.

$$
\begin{align}
x(t+dt) &= x + \frac{\alpha}{2} \nabla_x p(x_t) + z \tag{$z\sim N(0, \alpha)$}\\
p(x_{t+1}) &= p(x_t) -\eta \nabla_x D (p(x_t), \pi(x_t)) \tag{!?} \\
\end{align}
$$

- It would be interesting to explore the relationship between Langevin optimisation and sampling!? ([welling](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf))
- Which is more general? Energy or density?

## Theory

- Is it a universal approximator? (what can we not do with it?)
- In which cases does it work (better)?
- Does it actually aid transfer? When?

### Refs

- [OpenAI EBM](https://arxiv.org/abs/1811.02486)
- [EBMs review](https://arxiv.org/abs/1708.06008)
- [GANs, IRL, EBMs](https://arxiv.org/abs/1611.03852)
- [Tutorial](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)
- [BM for time-series](https://arxiv.org/pdf/1708.06004.pdf)
