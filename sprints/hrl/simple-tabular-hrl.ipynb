{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs\n",
    "\n",
    "- Want more than a two level heirarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 16\n",
    "n_states = 64\n",
    "gamma = 0.999\n",
    "H = 100  # number of time steps in a episode\n",
    "K = 1000  # number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def onehot(idx, N):\n",
    "    return np.eye(N)[idx]\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.S = np.arange(n_states)\n",
    "        self.A = np.arange(n_actions)\n",
    "\n",
    "        # model the transitions as linear fns conditional on the action.\n",
    "        # P = np.random.standard_normal([n_actions, n_states, n_states]) **2 # make sharper\n",
    "\n",
    "        # deterministic transition fn\n",
    "        # each action move from state(i) to state(j) with probability 1.\n",
    "        # BUG nope. softmax doesnt do this. will need to set to -infty\n",
    "        self.P = 20*np.stack([np.random.permutation(np.eye(n_states, dtype=np.float32)) for _ in range(n_actions-1)] + [np.eye(n_states, dtype=np.float32)],axis=0)  \n",
    "        # TODO what if there is structure in P? Low rank? Shared over actions?\n",
    "        # QUESTION how does this parameterisation effect things?\n",
    "        # NOTE this graph might be disconnected. but is unlikely!?\n",
    "\n",
    "        # reward is only a fn of the current state - shape = [n_states]\n",
    "        # also. is sparse.\n",
    "        self.R = onehot(np.random.randint(0, n_states), n_states)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        A tabular, probabilistic step function. \n",
    "\n",
    "        Args:\n",
    "            state (int): An element of S. The current state\n",
    "            state (int): An element of A. The action to be taken\n",
    "\n",
    "        Returns:\n",
    "            new_state (int): An element of S.\n",
    "        \"\"\"\n",
    "        # step by selecting relevant transition matrix and applying\n",
    "        logits = np.matmul(self.P[action, ...], onehot(state, n_states))\n",
    "        # convert to a distribution and sample\n",
    "        new_s = np.random.choice(self.S, p=softmax(logits))\n",
    "        return new_s, self.R[new_s]\n",
    "    \n",
    "    def rnd_policy(self, s, *args):\n",
    "        return np.random.choice(self.A)\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.random.choice(self.S)\n",
    "\n",
    "    def new_task(self):\n",
    "        self.R = onehot(np.random.randint(0, n_states), n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env(n_states, n_actions)\n",
    "env.step(0, env.rnd_policy(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, logits):\n",
    "    # or could use gumbel trick\n",
    "    p = softmax(logits)\n",
    "    return np.random.choice(x, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ():\n",
    "    # this is SARSA!?\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.gamma = 0.999\n",
    "        self.lr = 0.1\n",
    "\n",
    "        self.qs = np.zeros([n_states, n_actions])\n",
    "        \n",
    "        self.old_s = None\n",
    "        self.old_a = None\n",
    "        self.old_r = None\n",
    "    \n",
    "    def __call__(self, s, r):\n",
    "        a = sample(np.arange(n_actions), self.qs[s, ...])\n",
    "        # should change to epsilon greedy or entropy regularised\n",
    "        \n",
    "        if self.old_s is not None:\n",
    "            self.train_step(self.old_s, self.old_a, r, s, a)\n",
    "                \n",
    "        # loop past observations so we can use them for training\n",
    "        self.old_s = s\n",
    "        self.old_a = a\n",
    "                \n",
    "        return a\n",
    "    \n",
    "    def train_step(self, old_s, old_a, old_r, s, a):\n",
    "        target = (old_r + self.gamma*self.qs[s, a])  # bootstrap off next step. TD!\n",
    "        delta =  target - self.qs[old_s, old_a]\n",
    "        self.qs[old_s, old_a] += self.lr * delta  # incremental update. exp avg/GD!\n",
    "        self.qs -= 1e-6 * self.qs  # entropy regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularFuN():  # or HIRO!?\n",
    "    # Tabular feudal network\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "\n",
    "        self.qs = np.zeros([n_states, n_actions])\n",
    "        \n",
    "        self.old_s = None\n",
    "        self.old_a = None\n",
    "        self.old_r = None\n",
    "    \n",
    "    def __call__(self, s, r):\n",
    "        a = sample(self.action_space, self.qs[s, ...])\n",
    "        \n",
    "        if self.old_s is not None:\n",
    "            self.train_step(self.old_s, self.old_a, self.old_r, s, a)\n",
    "                \n",
    "        # loop past observations so we can use them for training\n",
    "        self.old_s = s\n",
    "        self.old_a = a\n",
    "        self.old_r = r\n",
    "                \n",
    "        return a\n",
    "    \n",
    "    def train_step(self, old_s, old_a, old_r, s, a):\n",
    "        target = (old_r + self.gamma*self.qs[s, a])  # bootstrap off next step. TD!\n",
    "        delta =  target - self.qs[old_s, old_a]\n",
    "        self.qs[old_s, old_a] += self.lr * delta  # incremental update. exp avg/GD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularHRIO():\n",
    "    # Tabular feudal network\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "\n",
    "        self.qs = np.zeros([n_states, n_actions])\n",
    "        \n",
    "        self.old_s = None\n",
    "        self.old_a = None\n",
    "        self.old_r = None\n",
    "    \n",
    "    def __call__(self, s, r):\n",
    "        a = sample(self.action_space, self.qs[s, ...])\n",
    "        \n",
    "        if self.old_s is not None:\n",
    "            self.train_step(self.old_s, self.old_a, self.old_r, s, a)\n",
    "                \n",
    "        # loop past observations so we can use them for training\n",
    "        self.old_s = s\n",
    "        self.old_a = a\n",
    "        self.old_r = r\n",
    "                \n",
    "        return a\n",
    "    \n",
    "    def train_step(self, old_s, old_a, old_r, s, a):\n",
    "        target = (old_r + self.gamma*self.qs[s, a])  # bootstrap off next step. TD!\n",
    "        delta =  target - self.qs[old_s, old_a]\n",
    "        self.qs[old_s, old_a] += self.lr * delta  # incremental update. exp avg/GD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularOpC():\n",
    "    # Tabular option critic\n",
    "    def __init__(self, n_states, n_options, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_options = n_options\n",
    "        self.n_states = n_states\n",
    "        self.gamma = 0.999\n",
    "        self.lr = 0.1\n",
    "\n",
    "        # hmm. viewing options as another dim seems weird?\n",
    "        # so each option yields a different set of qs.\n",
    "        self.qs = np.zeros([n_states, n_options, n_actions])\n",
    "        # so adding another layer would add another dimension to this tensor!?\n",
    "        # so this algol scales exp in its depth!?\n",
    "        \n",
    "        self.term_probs = np.ones([n_states, n_options])\n",
    "        self.term_probs /= np.sum(self.term_probs, axis=0)\n",
    "        \n",
    "        self.old_s = None\n",
    "        self.old_a = None\n",
    "        self.old_r = None\n",
    "        \n",
    "        self.w = np.random.randint(n_options)\n",
    "    \n",
    "    def __call__(self, s, r):\n",
    "        # NOTE in the OpC paper they use a policy not the Q fn to take actions\n",
    "        a = sample(np.arange(self.n_actions), self.qs[s, self.w, :])\n",
    "        \n",
    "        if self.old_s is not None:\n",
    "            self.train_step(self.old_s, self.old_a, r, s, a)\n",
    "                \n",
    "        if self.term_probs[s, self.w] < np.random.random():\n",
    "            self.w = sample(np.arange(n_options), self.q_omega(s))\n",
    "                \n",
    "        # loop past observations so we can use them for training\n",
    "        self.old_s = s\n",
    "        self.old_a = a\n",
    "                \n",
    "        return a\n",
    "    \n",
    "    def train_step(self, old_s, old_a, old_r, s, a):\n",
    "        ### Qs\n",
    "        # the option critic value estimate\n",
    "        omega_s_tp1 = self.q_omega(s)\n",
    "        next_state_value = ((1-self.term_probs[s, self.w]) * omega_s_tp1[self.w]\n",
    "                           + self.term_probs[s, self.w] * np.max(omega_s_tp1))\n",
    "        # bootstrap off next step. TD!\n",
    "        target = old_r + self.gamma*next_state_value\n",
    "        # estimate grad of MSBE\n",
    "        delta =  target - self.qs[old_s, self.w, old_a]\n",
    "        # incremental update. exp avg/GD!\n",
    "        self.qs[old_s, self.w, old_a] += self.lr * delta  \n",
    "        \n",
    "        ### termination props\n",
    "        # value_actual_w - value_best_w\n",
    "        self.term_probs -= self.lr * (omega_s_tp1[self.w] - next_state_value)\n",
    "        self.term_probs /= np.sum(self.term_probs, axis=0)\n",
    "        # need to normalise these!?!?\n",
    "        \n",
    "        ### policy\n",
    "        # none...\n",
    "        pass\n",
    "    \n",
    "    def q_omega(self, s):\n",
    "        # not sure how to do this with matrix ops\n",
    "        # uhh, it slow...\n",
    "        q_omega = np.zeros([self.n_options])\n",
    "        for w in range(self.n_options):\n",
    "            p = softmax(self.qs[s, w, :])\n",
    "            q_omega[w] += np.dot(self.qs[s,w,:], p)\n",
    "        return q_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, player, len_episode):\n",
    "    # reset\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    r = 0\n",
    "    w = np.random.randint(8)\n",
    "    \n",
    "    # play an episode\n",
    "    for _ in range(len_episode):\n",
    "        a = player(s, r)\n",
    "        s, r = env.step(s, a)\n",
    "        total_r += r\n",
    "        \n",
    "    return total_r/len_episode\n",
    "\n",
    "def plot(x, label):\n",
    "    n = len(x[0])\n",
    "    idx = range(n)\n",
    "    mean = np.mean(x, axis=0)\n",
    "    stddev = np.sqrt(np.var(x,axis=0)) \n",
    "    \n",
    "    # smooth the signals\n",
    "    n_kernel = 20\n",
    "    kernel = [1]*n_kernel\n",
    "    mean = np.convolve(mean, kernel)[:n]/n_kernel\n",
    "    stddev = np.convolve(stddev, kernel)[:n]/n_kernel\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(idx, mean, label=label)\n",
    "    plt.fill_between(idx, mean-stddev, mean+stddev,  alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states=32\n",
    "n_options=16\n",
    "n_actions=8\n",
    "env = Env(n_states, n_actions)\n",
    "n_steps = 200\n",
    "n_episodes = 2000\n",
    "n_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opc_rs = []\n",
    "# for _ in range(n_trials):\n",
    "#     opc_player = TabularOpC(n_states, n_options, n_actions)\n",
    "#     rs = np.array([play_episode(env, opc_player, n_steps) for _ in range(n_episodes)])\n",
    "#     opc_rs.append(rs)\n",
    "# opc_rs = np.vstack(opc_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce Figure 2. from The option critic architecture (Bacon, Harb, Precup)\n",
    "\n",
    "env = Env(n_states, n_actions)\n",
    "\n",
    "Q_rs = []\n",
    "OpC_rs = []\n",
    "for j in range(n_trials):\n",
    "    q_player = TabularQ(n_states, n_actions)\n",
    "    q_rs = []\n",
    "    \n",
    "    opc_player = TabularOpC(n_states, n_options, n_actions)\n",
    "    opc_rs = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        if i == n_episodes//2:\n",
    "            env.new_task()\n",
    "        \n",
    "        q_r = play_episode(env, q_player, n_steps)\n",
    "        q_rs.append(q_r)\n",
    "        \n",
    "        opc_r = play_episode(env, opc_player, n_steps)\n",
    "        opc_rs.append(opc_r)\n",
    "        \n",
    "    Q_rs.append(np.array(q_rs))\n",
    "    OpC_rs.append(np.array(opc_rs))\n",
    "    print('\\r{}'.format(j), end='', flush=True)\n",
    "\n",
    "Q_rs = np.vstack(Q_rs)\n",
    "OpC_rs = np.vstack(OpC_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot(Q_rs, label='Q')\n",
    "plot(OpC_rs, label='OpC')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would expect that heirarchical approaches will generalise more easily to other rewards/tasks!?\n",
    "as they can transfer some of the options. \n",
    "BUT. the options might be totally wrong!? not convinced this should work, but it seems to...!? no. it doesnt? need better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.new_task()\n",
    "# opc_rs = np.vstack([np.array([play_episode(env, opc_player, n_steps) for _ in range(n_episodes)])\n",
    "#                  for _ in range(n_trials)])\n",
    "# q_rs = np.vstack([np.array([play_episode(env, q_player, n_steps) for _ in range(n_episodes)])\n",
    "#                  for _ in range(n_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 4))\n",
    "# plot(q_rs, label='Q')\n",
    "# plot(opc_rs, label='OpC')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
