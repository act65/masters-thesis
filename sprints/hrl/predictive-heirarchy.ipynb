{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition():\n",
    "    def __init__(self):\n",
    "        self.energy_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.value_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer()\n",
    "        self.step = tf.Variable(0, name='step')\n",
    "        \n",
    "        self.old_x = None\n",
    "        \n",
    "    def __call__(self, x, r):\n",
    "        \"\"\"\n",
    "        Handles training and prediction. (how can these be unified!?)\n",
    "        \"\"\"\n",
    "        # or use a worker to collect data and train offline\n",
    "        x_hat_tp1 = self.forward(x)\n",
    "        \n",
    "        if self.old_x is not None:\n",
    "            # recompute the x_hat_tp1. could do better.\n",
    "            loss = self.train_step(self.old_x, x, r)\n",
    "        \n",
    "        self.old_x = x\n",
    "        \n",
    "        return x_hat_tp1\n",
    "        \n",
    "    def forward(self, x, step_size=0.1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            e = self.energy_fn(x) \n",
    "            v = self.value_fn(x)\n",
    "            \n",
    "            cost = v - e\n",
    "            \n",
    "        grad = tape.gradient(cost, x)\n",
    "        return x + step_size*grad[0]  # ascend value and descend energy\n",
    "    \n",
    "    def train_step(self, x_t, x_tp1, r_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # observations should have low energy\n",
    "            # not sure how to optimise for that!?\n",
    "\n",
    "            # for now. optimise E, V for accuracy\n",
    "            x_hat_tp1 = self.forward(x_t)\n",
    "            loss_acc = tf.losses.mean_squared_error(x_tp1, x_hat_tp1)\n",
    "\n",
    "            # value should predict future rewards\n",
    "            v_t = self.value_fn(x_t)\n",
    "            v_tp1 = self.value_fn(x_tp1)\n",
    "            loss_value = tf.losses.mean_squared_error(v_t, r_t+self.gamma*v_tp1)  \n",
    "            # could split out the value fn as another class. as will need for policy as well.\n",
    "            \n",
    "            loss = loss_value+loss_acc\n",
    "            \n",
    "        variables = self.energy_fn.variables + self.value_fn.variables\n",
    "\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        self.opt.apply_gradients(zip(grads, variables), global_step=self.step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8200, shape=(1, 6), dtype=float32, numpy=\n",
       "array([[ 0.6959551 , -0.38642088,  1.6699668 , -1.3171674 , -0.15613846,\n",
       "         1.3045323 ]], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random_normal([1, 6])\n",
    "r = tf.random_normal([1,1])\n",
    "t = Transition()\n",
    "t(x, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \"\"\"\n",
    "    This policy has little to do with achieving 'extrinsic value'.\n",
    "    Its main task is reachability. I want to go to X. This policy should make it happen. \n",
    "    \"\"\"\n",
    "    \n",
    "    # does this need memory?\n",
    "    # the ability to integrate the deltas?\n",
    "    # the ability to remember the past?\n",
    "    # will be a pain for training...\n",
    "    def __init__(self, n_actions):\n",
    "        self.fn = tf.keras.Sequential([  # VQ!?\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_actions)\n",
    "        ])\n",
    "         \n",
    "    def __call__(self, x):  # possibly recieves many deltas for different layers.\n",
    "        z = self.fn(x)\n",
    "        return tfd.RelaxedOneHotCategorical(1.0, logits=z).sample()\n",
    "    \n",
    "    def train_step(self, goal, truth):\n",
    "        loss = tf.losses.mean_squared_error(goal, truth)\n",
    "        reward = tf.stop_gradient(loss)  \n",
    "        # will have zero grads anyway. \n",
    "        # unless we can use the transition fn somehow?\n",
    "        \n",
    "        # x, a, r = ?,?,reward\n",
    "        # A2C - No we can do vanilla RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, n_hidden):\n",
    "        self.fn = tf.keras.Sequential([  # VQ!? + RNN\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_hidden)\n",
    "        ])\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return self.fn(x)\n",
    "    \n",
    "    def train_step(self):\n",
    "        # trained for high entropy?\n",
    "        # independence?\n",
    "        # sparisity?\n",
    "        # fully unsupervised!?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    # __call__ and choose action will be moved to a parent class. Network, which takes many layers.\n",
    "    def __init__(self):\n",
    "        self.encode = Encoder(64)\n",
    "        self.transition = Transition()\n",
    "        \n",
    "        self.policy = Policy(4)\n",
    "        \n",
    "    def forward(self, x_t):\n",
    "        s_t = self.encode(x_t)\n",
    "        return self.transition(s_t)  # s_hat_tp1 should be stored as the state?!?\n",
    "        \n",
    "    def choose_action(self, s_hat_tp1, x_tp1):\n",
    "        s_tp1 = self.encode(x_tp1)\n",
    "        diff = s_tp1 - s_hat_tp1\n",
    "        return self.policy(diff)\n",
    "    \n",
    "    def __call__(self, x_t, x_tp1, x_tp2, r):\n",
    "        s_t = self.encode(x_t)\n",
    "        s_tp1 = self.encode(x_tp1)\n",
    "        s_tp2 = self.encode(x_tp2)\n",
    "        \n",
    "        a = self.choose_action(self.forward(x_t), x_tp1)\n",
    "        \n",
    "        # the policy is rewarded/trained on its ability to make predictions come true\n",
    "        self.policy.train_step(goal=self.forward(x_tp1), truth=s_tp2)\n",
    "        \n",
    "        # could use call backs to do training!?\n",
    "        # return lambda r: self.train_step(s, a)\n",
    "        # therefore s, a is still differentiable and we can keep them until we get the reward!?\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5782, shape=(1, 4), dtype=float32, numpy=array([[0.02318801, 0.87725997, 0.08311243, 0.01643958]], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.__call__(l.forward(x),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
