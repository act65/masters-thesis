{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import trfl\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs\n",
    "\n",
    "- Test each independently.\n",
    "    - tabular version of action decoding\n",
    "    - ?\n",
    "- Extend to partial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \"\"\"\n",
    "    Vanilla policy with A2C.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        self.fn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_actions+1)\n",
    "        ])\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        z = self.fn(x)\n",
    "        a = tfd.Categorical(logits=z[:,:self.n_actions]).sample()\n",
    "        return a, lambda r, tape: self.loss(z, a, r, tape)\n",
    "    \n",
    "    def loss(self, z, a, r, tape):\n",
    "        # TODO add entropy regularisation\n",
    "        loss = trfl.discrete_policy_gradient(z[:,:self.n_actions], a, z[:,-1])\n",
    "        return loss, self.fn.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(): # aka perception!?\n",
    "    # WANT VQ.\n",
    "    # opt for;\n",
    "    # - high entropy clustering\n",
    "    # - disentanglement\n",
    "    # - sparsity\n",
    "    # - local/multiscale\n",
    "    def __init__(self, n_hidden):\n",
    "        self.fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.InputLayer(input_shape=[8]),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(n_hidden)\n",
    "        ])\n",
    "        \n",
    "        self.vq = snt.nets.VectorQuantizer(embedding_dim=n_hidden, num_embeddings=32, commitment_cost=1)\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        h = self.fn(x)\n",
    "        z = self.vq(h, True)\n",
    "        return z['quantize'], lambda x_tp1, tape: self.loss(z, x_tp1, tape)\n",
    "        \n",
    "    def loss(self, z_t, x_tp1, tape):\n",
    "        z_tp1, _ = self.__call__(x_tp1)\n",
    "\n",
    "        # optimise for temporal similarity\n",
    "        loss_sim = tf.losses.mean_squared_error(z_t['quantize'], z_tp1)\n",
    "        \n",
    "        # and high perplexity\n",
    "        loss = loss_sim-z_t['perplexity']\n",
    "        return loss, self.fn.variables + [self.vq.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(): # aka reasoning!?\n",
    "    # could add another 'intrinsic' value fn here.\n",
    "    # to encourage exploration.\n",
    "    \n",
    "    # also should explore energy via distribution vs not.\n",
    "    \n",
    "    # seems weird. how I am training this. the energy fn will never be able to \n",
    "    # achieve high next step prediction accuracy unless it has a model of V!?\n",
    "    def __init__(self):\n",
    "        self.energy_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.InputLayer(input_shape=[16]),\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.value_fn = tf.keras.Sequential([\n",
    "            # could add some memory in here. lSTM or DNC\n",
    "            tf.keras.layers.Dense(64, activation=tf.nn.selu),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "                \n",
    "    def __call__(self, x_t, step_size=0.1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_t)\n",
    "            e = self.energy_fn(x_t) \n",
    "            v = self.value_fn(x_t)\n",
    "            \n",
    "            cost = v - e\n",
    "            \n",
    "        grad = tape.gradient(cost, x_t)\n",
    "         # ascend value and descend energy\n",
    "        x_hat_tp1 = x_t + step_size*grad[0]\n",
    "        return x_hat_tp1, lambda x_tp1, r, tape: self.loss(x_t, x_tp1, x_hat_tp1, v, r, tape)\n",
    "        \n",
    "    def loss(self, x_t, x_tp1, x_hat_tp1, v_t, r_t, tape):\n",
    "        # observations should have low energy\n",
    "        # not sure how to optimise for that!?\n",
    "\n",
    "        # for now. optimise E, V for accuracy\n",
    "        loss_acc = tf.losses.mean_squared_error(x_tp1, x_hat_tp1)\n",
    "\n",
    "        # value should predict future rewards\n",
    "        v_tp1 = self.value_fn(x_tp1)\n",
    "        loss_value = tf.losses.mean_squared_error(v_t, r_t+self.gamma*v_tp1)  \n",
    "        # could split out the value fn as another class. as will need for policy as well.\n",
    "\n",
    "        losses = [loss_value,loss_acc]\n",
    "        variables = [self.value_fn.variables, self.energy_fn.variables]\n",
    "\n",
    "        return losses, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self, n_actions):\n",
    "        self.encoder = Encoder(16)\n",
    "        self.transition = Transition()\n",
    "        self.policy_fn = Policy(n_actions)\n",
    "        \"\"\"\n",
    "        This policy has little to do with achieving 'extrinsic value'.\n",
    "        Its main task is reachability. I want to go to X. This policy should make it happen. \n",
    "        \"\"\"\n",
    "        # does this need memory?\n",
    "        # the ability to integrate the deltas?\n",
    "        # the ability to remember the past?\n",
    "        # will be a pain for training...\n",
    "        \n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer()\n",
    "        self.step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        self.writer = tf.contrib.summary.create_file_writer('/tmp/net/0')\n",
    "        self.writer.set_as_default()\n",
    "                \n",
    "    def __call__(self, x_t):\n",
    "        \"\"\"\n",
    "        Handles training and prediction.\n",
    "        \"\"\"\n",
    "        # and/or could use a worker to collect data and train offline...\n",
    "        # BUT. how to combine both!? will need to aggregate params somehow. or stop playing.\n",
    "        \n",
    "        # key to good online learning is a good exploration policy that\n",
    "        # produces a uniform distribution of ...\n",
    "        \n",
    "        s_t, encoder_callback = self.encoder(x_t)\n",
    "        s_hat_tp1, transition_callback = self.transition(s_t)\n",
    "        a, policy_callback = self.policy_fn(s_t - s_hat_tp1)  # is this enough info? or do we need abs info?\n",
    "        \n",
    "        def callback(x_tp1, r, tape):\n",
    "            # this callback is nice bc we dont have to do any recompute\n",
    "            s_tp1, _ = self.encoder(x_tp1)\n",
    "            \n",
    "            encoder_loss, encoder_vars = encoder_callback(x_tp1, tape)\n",
    "            transition_losses, transition_vars = transition_callback(s_tp1, r, tape)\n",
    "            \n",
    "            # use the transtition loss the the reward for the policy.\n",
    "            # but high loss means the policy could not achieve its target.\n",
    "            # but is also good as maybe we experienced something novel!? what is the difference?\n",
    "            policy_loss, policy_vars = policy_callback(tf.stop_gradient(-transition_losses[1]), tape)\n",
    "                \n",
    "            lnvs = [\n",
    "                (encoder_loss, encoder_vars),\n",
    "                (policy_loss, policy_vars)\n",
    "            ]\n",
    "            \n",
    "            losses, variables = zip(*lnvs)\n",
    "            losses = list(losses) + transition_losses\n",
    "            variables = list(variables) + transition_vars\n",
    "            \n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                names = ['enc', 'policy', 'value', 'acc']\n",
    "                for name, loss in zip(names, losses):\n",
    "                    tf.contrib.summary.scalar(name, loss)\n",
    "\n",
    "            \n",
    "            grads = tape.gradient(list(losses), list(variables))\n",
    "            gnvs = zip(grads, variables)\n",
    "            gnvs = [(g, v) if g is not None else (tf.zeros_like(v), v)\n",
    "                    for G, V in gnvs for g,v in zip(G, V)]\n",
    "\n",
    "            # PROBLEM!? not sure....\n",
    "#             gnvs = [(g, v) for G, V in gnvs for g,v in zip(G, V)]    \n",
    "#             count = sum([1 if g is None else 0 for g,v in gnvs])\n",
    "#             print(count)\n",
    "#             raise SystemExit\n",
    "            \n",
    "            \n",
    "            self.opt.apply_gradients(gnvs, global_step=self.step)\n",
    "        \n",
    "        return a, callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telfaralex/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/telfaralex/.local/lib/python3.6/site-packages/sonnet/python/modules/nets/vqvae.py:63: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "player = Net(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode():\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with tf.GradientTape() as tape:\n",
    "            action, callback = player(tf.constant(obs, dtype=tf.float32, shape=[1, 8]))\n",
    "            env.render()\n",
    "            obs, reward, done, info = env.step(action.numpy()[0])\n",
    "\n",
    "            callback(tf.constant(obs, dtype=tf.float32, shape=[1, 8]), \n",
    "                     tf.constant(reward, dtype=tf.float32, shape=[1, 1]), \n",
    "                     tape)\n",
    "            R += reward\n",
    "        \n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
    "            tf.contrib.summary.scalar('R', R)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    run_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
