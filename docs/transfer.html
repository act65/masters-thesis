<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>transfer</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../style.css">
  <script src="../docs/mathjax.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
    // MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      "HTML-CSS": { fonts: ["TeX"] }
    });
  </script>
</head>
<body>
<header>
<h1 class="title">transfer</h1>
</header>
<h1 id="transfer">Transfer</h1>
<p>Why do we want to do this? Transfer learning is the key to general intelligence!</p>
<h2 id="definition">Definition</h2>
<p>What do we mean by transfer in “transfer learning”? If we have two tasks/environments/action spaces/…?, <span class="math inline">\(A, B\)</span>, then the performance of one task aids the other task.</p>
<p>A MDP is defined as <span class="math display">\[M = \Big(S, \mathcal A, p(\cdot \mid s,a), R(s, a, s&#39;) \Big)\]</span></p>
<ul>
<li><span class="math inline">\(S\)</span>: It is possible to change the state space, while preserving the dynamics. (??)</li>
<li><span class="math inline">\(\mathcal A\)</span>: Change the action space, for example, instead of <span class="math inline">\(\leftarrow, \rightarrow, \uparrow, \downarrow\)</span> we use <span class="math inline">\(\uparrow, \text{rot90}\)</span></li>
<li><span class="math inline">\(p(\cdot \mid s,a)\)</span>: from subtle things like not being able to reach a state on another one, to chan</li>
<li><span class="math inline">\(R(s, a, s&#39;)\)</span>: A different reward funciton, aka a different task.</li>
</ul>
<p>But one could imagine symmetries of <span class="math inline">\(p(\cdot \mid s,a), R(s, a, s&#39;)\)</span>, such that some structure is preserved.</p>
<p><span class="math display">\[
\begin{align*}
p(\cdot \mid s,a) &amp;= T^{-1}(p(\cdot \mid T(s,a))) \\
&amp;= p(\cdot \mid T(s),a) \tag{equiv to transfer to a new state space}\\
&amp;= p(\cdot \mid s,T(a)) \\
R(s, a, s&#39;) &amp;= T(R(s, a, s&#39;)) \\
\end{align*}
\]</span></p>
<p>For example, similarities between the reward in hockey and football. Get the round thing in the oppositions goal.</p>
<p>
Huh, never thought about it this way before. The states are an unordered set. The transition fn provides all the structure on that space (much like an inner prod in Hilbert spaces?!?) The neighbors of a state are the positions reachable from a single action. No not quite. More like probabilistic vector maps? No that is only when combined with a policy.
</p>
<p>Best current solutions!?</p>
<ul>
<li>successor representation/goal embeddings. <span class="math inline">\(\to\)</span> task transfer</li>
<li>model-based RL (disentangle policy from model) allows transfer of control polices between environments and transfer of model between tasks in the same env.</li>
<li>why was it transferred? (because the domains somehow shared similarities)</li>
</ul>
<p><span class="math display">\[
\begin{align*}
L(T(B)) \le L(T(A) \to T(B)) \tag{Forward transfer}\\
L(T(A)) \le L(T(A) \to T(B)) \tag{Backward transfer} \\
\end{align*}
\]</span></p>
<p>Relationship to meta-learning. Different <i>‘levels’</i> of knowledge can be transfered.</p>
<ul>
<li>in transfer learning we tend to deal with absolute knowledge about one domain (say a robot simulation)</li>
<li>meta learning transfers <i>meta</i> knowledge about how to learn (thus it is also called learning to learn)</li>
</ul>
<p>How can we tease apart these definitions into a heirarchy?</p>
<p><span class="math display">\[
\begin{align*}
\dot L(T(B)) \le \dot L(T(A) \to T(B)) \\
\dot L(T(A)) \le \dot L(T(A) \to T(B)) \\
\end{align*}
\]</span></p>
<h2 id="analysis">Analysis</h2>
<p>What I would really like is a set of tools for analysing transfer learning. I would like to be able to answer the questions;</p>
<ul>
<li>what knowledge was transferred (high level, low level, …?)</li>
<li>how was it transferred? (if we are dealing with NNs then how does some knowledge get shared while other knowledge doesnt? because the existing knowledge allows faster learning?!)</li>
<li>why was it transferred? (because the domains somehow shared similarities)</li>
</ul>
<p>Seems quite related to representation learning. The key will be how knowledge is represented, and how easily that knowledge can be translated (/transformed)!?</p>
<p>Like a communication channel. I want to see what has been communicated! Attributing performance on a current task to a past experiences/tasks. (good at snowboarding bc i used to surf)</p>
<p>If we had a theory of transfer learning we would be able to; - predict when X will transfer to Y. - write down a pattern to generate representations for transfer between X/Y. - ?</p>
<h2 id="toy-problems">Toy problems</h2>
<p>Two environments, only difference is visual appearance, … Two environments,</p>
<p>Want to generate different MDPs that share various ‘orders’ of similarity. Navigation on graphs? Various orders of persistent homology?</p>
<h2 id="heirarhcal-rl">Heirarhcal RL</h2>
<p><strong>Conjecture</strong>: meta/transfer/continual/… learning naturally emerge from heirarchical/multi-scale RL. We naturally see a decomposition of the tasks into what they share at different levels of abstraction (not just time!?).</p>
<p><strong>TODO</strong>: Heirarchical filters and meta RL and options.</p>
<p><span class="math display">\[
\begin{align*}
z_t &amp;= f(s_t, a_t, r_t^k) \\
\pi(s_t) &amp;= g(\sum_k f_k(z_t)) \\
v_t^k &amp;= h_k(z_t) \\
\mathcal L_k &amp;= \sum \parallel v_t^k - R(\gamma_k) \parallel \\
R(\gamma) &amp;= \sum \gamma^i r_i \\
\end{align*}
\]</span></p>
<p>Ahh. But need to be heirarchically propagating forward differences <strong>!!!</strong> ? Else would need very large compute to fit in large enough batches…</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Learning to learn</a></li>
<li><a href="https://arxiv.org/abs/1612.00796">Elastic weights consolidation</a></li>
<li><a href="https://arxiv.org/abs/1606.05312">Successor features</a></li>
</ul>
</body>
</html>
