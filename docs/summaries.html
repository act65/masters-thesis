<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Summaries</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../style.css">
  <script src="../docs/mathjax.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
    // MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      "HTML-CSS": { fonts: ["TeX"] }
    });
  </script>
</head>
<body>
<div>
<h2 id="world-models">World models</h2>
<ul>
<li>Learn a large RNN that approximates the transition function.</li>
<li>Add noise when imagining/simulating experience to avoid the controller overfitting to bias in the model.</li>
<li>Can use the world-model to imagine training experience for training the controller.</li>
</ul>
<p><a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a></p>
<h2 id="temporal-difference-models">Temporal difference models</h2>
<blockquote>
<p>conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal</p>
</blockquote>
<blockquote>
<p>Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? ... [R]ewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying “physics.”</p>
</blockquote>
<p><span class="math display">\[
r(s_t, a_t, s_{t+1}, s_g) = −D(s_{t+1}, s_g) \\
\]</span></p>
<p>So <span class="math inline">\(r\)</span> is now a measure of <em>reachability</em>!?</p>
<blockquote>
<p>TDM ... tells us how close the agent will get to a given goal state <span class="math inline">\(s_g\)</span> after <span class="math inline">\(τ\)</span> time steps, when it is attempting to reach that state in <span class="math inline">\(τ\)</span> steps.</p>
</blockquote>
<p><span class="math display">\[
Q(s_t, a_t, s_g, \tau) = E_{p(s_{t+1}\mid s_t, a_t)}(-D(s_{t+1}, s_g)) \cdot1[\tau =0] + \text{max}_a Q(s_{t+1}, a, s_g, \tau-1)\cdot1[\tau \neq0]
\]</span></p>
<p><em>Wait, doesnt <span class="math inline">\(\tau -1\)</span> mean that we willhave negative <span class="math inline">\(\tau\)</span>s? I think it ishould be <span class="math inline">\(1[\tau&gt;0]\)</span>?</em></p>
<p>So if <span class="math inline">\(\tau=0\)</span>, and <span class="math inline">\(s_{t+1} = s_g\)</span> then then <span class="math inline">\(Q\)</span> value should be zero.</p>
<blockquote>
<p>... vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point.</p>
</blockquote>
<p><em>hmm. want to think about this!! but not sure who it works, how can you do the argmax now?!?</em></p>
<p><span class="math display">\[
\begin{align}
Q(s_t, a_t, s_g, \tau) &amp;= \sum_i \psi_i(s_t, a_t, s_g, \tau) \\
&amp;=  \sum_i - \mid s_t - s_g \mid \\
\mathcal L &amp;= \frac{1}{2}\parallel y - \psi(s_t, a_t, s_g, \tau) \parallel_2^2 \\
\end{align}
\]</span></p>
<p><span class="math inline">\(Q\)</span> is used to take actions, but is constructed/trained via some representation of distances. Reminds me of successor features?!?</p>
<p><span class="math display">\[
\begin{align}
Q(s_t, a_t) &amp;= g(f(s_t, a_t)) \\
\mathcal L &amp;= \frac{1}{2}\parallel y - f(s_t, a_t) \parallel_2^2 \\
\end{align}
\]</span></p>
<p>Ahh ok. I think I see. We are simply learning the model. And we already know how to extract <span class="math inline">\(Q\)</span> values from it. Therefore we dont need to train <span class="math inline">\(g\)</span>.</p>
<hr />
<ul>
<li>Define a proxy reward as the distance between a goal state and the current state.</li>
<li>Learn a Q value that estimates the reachability of different states within t steps.</li>
<li>It turns out when t&gt;&gt;1 we recover model-free learning and yet we can still plan with t~=1.</li>
</ul>
<p><a href="https://bair.berkeley.edu/blog/2018/04/26/tdm/" class="uri">https://bair.berkeley.edu/blog/2018/04/26/tdm/</a></p>
<h2 id="learning-to-reinforcement-learn">Learning to reinforcement learn</h2>
<ul>
<li>Use a RNN as an actor-critic but also provide the reward recieved as an input.</li>
<li>Train weights over sampled MPDs to maximise cumlative reward over an episode.</li>
<li>Freeze weights and test on new MPDs.</li>
</ul>
<p><a href="https://arxiv.org/abs/1611.05763" class="uri">https://arxiv.org/abs/1611.05763</a></p>
</div>
</body>
</html>
