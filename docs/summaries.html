<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Summaries</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../style.css">
  <script src="../docs/mathjax.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
    // MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      "HTML-CSS": { fonts: ["TeX"] }
    });
  </script>
</head>
<body>
<div>
<h2 id="world-models">World models</h2>
<ul>
<li>Learn a large RNN that approximates the transition function.</li>
<li>Add noise when imagining/simulating experience to avoid the controller overfitting to bias in the model.</li>
<li>Can use the world-model to imagine training experience for training the controller.</li>
</ul>
<p><a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a></p>
<h2 id="temporal-difference-models">Temporal difference models</h2>
<blockquote>
<p>conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal</p>
</blockquote>
<blockquote>
<p>Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? … [R]ewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying “physics.”</p>
</blockquote>
<p><span class="math display">\[
r(s_t, a_t, s_{t+1}, s_g) = −D(s_{t+1}, s_g) \\
\]</span></p>
<p>So <span class="math inline">\(r\)</span> is now a measure of <em>reachability</em>!?</p>
<blockquote>
<p>TDM … tells us how close the agent will get to a given goal state <span class="math inline">\(s_g\)</span> after <span class="math inline">\(τ\)</span> time steps, when it is attempting to reach that state in <span class="math inline">\(τ\)</span> steps.</p>
</blockquote>
<p><span class="math display">\[
Q(s_t, a_t, s_g, \tau) = E_{p(s_{t+1}\mid s_t, a_t)}(-D(s_{t+1}, s_g)) \cdot1[\tau =0] + \text{max}_a Q(s_{t+1}, a, s_g, \tau-1)\cdot1[\tau \neq0]
\]</span></p>
<p><em>Wait, doesnt <span class="math inline">\(\tau -1\)</span> mean that we willhave negative <span class="math inline">\(\tau\)</span>s? I think it ishould be <span class="math inline">\(1[\tau&gt;0]\)</span>?</em></p>
<p>So if <span class="math inline">\(\tau=0\)</span>, and <span class="math inline">\(s_{t+1} = s_g\)</span> then then <span class="math inline">\(Q\)</span> value should be zero.</p>
<blockquote>
<p>… vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point.</p>
</blockquote>
<p><em>hmm. want to think about this!! but not sure who it works, how can you do the argmax now?!?</em></p>
<p><span class="math display">\[
\begin{align}
Q(s_t, a_t, s_g, \tau) &amp;= \sum_i \psi_i(s_t, a_t, s_g, \tau) \\
&amp;=  \sum_i - \mid s_t - s_g \mid \\
\mathcal L &amp;= \frac{1}{2}\parallel y - \psi(s_t, a_t, s_g, \tau) \parallel_2^2 \\
\end{align}
\]</span></p>
<p><span class="math inline">\(Q\)</span> is used to take actions, but is constructed/trained via some representation of distances. Reminds me of successor features?!?</p>
<p><span class="math display">\[
\begin{align}
Q(s_t, a_t) &amp;= g(f(s_t, a_t)) \\
\mathcal L &amp;= \frac{1}{2}\parallel y - f(s_t, a_t) \parallel_2^2 \\
\end{align}
\]</span></p>
<p>Ahh ok. I think I see. We are simply learning the model. And we already know how to extract <span class="math inline">\(Q\)</span> values from it. Therefore we dont need to train <span class="math inline">\(g\)</span>.</p>
<hr />
<ul>
<li>Define a proxy reward as the distance between a goal state and the current state.</li>
<li>Learn a Q value that estimates the reachability of different states within t steps.</li>
<li>It turns out when t&gt;&gt;1 we recover model-free learning and yet we can still plan with t~=1.</li>
</ul>
<p><a href="https://bair.berkeley.edu/blog/2018/04/26/tdm/" class="uri">https://bair.berkeley.edu/blog/2018/04/26/tdm/</a></p>
<h2 id="learning-to-reinforcement-learn">Learning to reinforcement learn</h2>
<ul>
<li>Use a RNN as an actor-critic but also provide the reward recieved as an input.</li>
<li>Train weights over sampled MPDs to maximise cumlative reward over an episode.</li>
<li>Freeze weights and test on new MPDs.</li>
</ul>
<p><a href="https://arxiv.org/abs/1611.05763" class="uri">https://arxiv.org/abs/1611.05763</a></p>
<h2 id="independent-causal-mechanisms">Independent causal mechanisms</h2>
<ul>
<li>Learn a set of experts</li>
<li>Hold a tournament between the experts</li>
<li>Train the winner</li>
</ul>
<p>Aka, a GAN with many generators.</p>
<blockquote>
<p>each element [in the dataset] has been generated by one of the (independent) mechanisms, but we do not know by which one.</p>
</blockquote>
<blockquote>
<p>The motivation behind competitively updating only the winning expert is to enforce specialization</p>
</blockquote>
<p><strong>Q:</strong> How does specialisation relate to independence?</p>
<p><a href="https://arxiv.org/abs/1712.00961" class="uri">https://arxiv.org/abs/1712.00961</a></p>
<h2 id="pieter-abbeel-talking-at-nips-workshop-on-hrl">Pieter Abbeel talking at NIPs workshop on HRL</h2>
<p>(https://www.youtube.com/watch?v=WpSc3D__Av8)</p>
<ul>
<li>Information theoretic perspective: if you set a goal now, that you tell you something about the future</li>
<li>Grid world where agent must discover the passcodes for the actions (left right up down). For example left might be 0,0,1.</li>
</ul>
<blockquote>
<p>there is still no consensus on what constitute good options. <a href="https://arxiv.org/pdf/1612.00916.pdf">A Matrix Splitting Perspective on Planning with Options</a> if the option set for the task is not ideal, and cannot express the primitive optimal policy well, shorter options offer more flexibility and can yield a better solution. <a href="https://arxiv.org/pdf/1711.03817.pdf">Learning with Options that Terminate Off-Policy</a></p>
</blockquote>
<h2 id="a-theory-of-state-abstraction-for-reinforcement-learning">A Theory of State Abstraction for Reinforcement Learning</h2>
<p>(https://david-abel.github.io/papers/aaai_dc_2019.pdf)</p>
<blockquote>
<p>I propose three desiderata that characterize what it means for an abstraction to be useful for RL: 1. SUPPORT EFFICIENT DECISION MAKING: The abstraction enables fast planning and efficient RL. 2. PRESERVE SOLUTION QUALITY: Solutions produced from the abstracted model should be useful enough for solving the desired problems. 3. EASY TO CONSTRUCT: Creating the abstractions should not require an unrealistic statistical or computational budget.</p>
</blockquote>
<h2 id="on-the-necessity-of-abstraction">On the necessity of abstraction</h2>
<p>(https://www.sciencedirect.com/science/article/pii/S2352154618302080)</p>
<blockquote>
<p>The challenge in the single-task case is overcoming the additional cost of discovering the options; this results in a narrow opportunity for performance improvements, but a well-defined objective. In the skill transfer case, the key challenge is predicting the usefulness of a particular option to future tasks, given limited data.</p>
</blockquote>
<h2 id="near-optimal-representation-learnning-for-hrl">Near optimal representation learnning for HRL</h2>
<p>(https://openreview.net/forum?id=H1emus0qF7)</p>
<p><span class="math display">\[
\begin{align}
D_{KL}(P_{\pi^{* }(s, g)}(s&#39;\mid s) \parallel K_{\theta}(s&#39; \mid s, g)) \\
K_{\theta}(s&#39; \mid s, g) = \rho(s&#39;) \frac{e^{-E(s&#39;, s, g)}}{Z} \\
E(s&#39;, s, g) = D(f(s&#39;), \varphi(s, \psi(s, g))) \tag{distance is measured in abstract space}\\
\end{align}
\]</span> Kinda like an autoencoder? <span class="math display">\[
\begin{align}
D_{KL}(P(x) \parallel K(x&#39; \mid x)) \\
K(x \mid x) = \rho(x) \frac{e^{-E(x)}}{Z} \\
E(x) = D(x, d(e(x))) \\
\end{align}
\]</span></p>
<h2 id="model-based-rl-for-atari">Model based RL for atari</h2>
<p>(https://arxiv.org/abs/1903.00374)</p>
<ul>
<li>Achieve sota in approx 100k interactions.</li>
<li>Learn the transition fn and the reward fn via (self) supervision</li>
<li>Learn the policy and value via simulation (only)</li>
<li>Short planning horizon, augmented with mixture of real data.</li>
<li>Give reward for final (planned) time step.</li>
</ul>
<h2 id="understanding-the-asymptotic-performance-of-model-based-rl-methods">Understanding the asymptotic performance of model based RL methods</h2>
<p>(http://willwhitney.com/assets/papers/Understanding.the.Asymptotic.Performance.of.MBRL.pdf)</p>
<ul>
<li>Multi time step transition models (somehow) achieve less error (constant wrt horizon!?) than single time step models (linear wrt horizon!?). (WHY?!)</li>
<li>Planning with the environment simulator (MUJOCU) still only allows 40 time steps (for the half cheeta) before diverging.</li>
</ul>
<h2 id="near-optimal-behavior-via-approximate-state-abstraction">Near Optimal Behavior via Approximate State Abstraction</h2>
<p>(https://arxiv.org/abs/1701.04113)</p>
<p>What are the necessary conditions of epsilon-optimality on an abstraction? <span class="math display">\[
\forall_{s\in S_G, a\in A_G} \mid Q_G^{\pi^* }(s, a) - Q_G^{\pi_{GA}^* }(s, a) \mid \le 2 \epsilon \eta_f \implies \phi() ???
\]</span></p>
</div>
</body>
</html>
