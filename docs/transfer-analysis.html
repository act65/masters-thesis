<!DOCTYPE html>
<html>
<meta charset="utf-8">
  <head>
    <title>Transfer analysis</title>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML-full">
    </script>
    <link rel="stylesheet" href="style.css">
 </head>
 <body>
  <div>
    <h1>Transfer analysis</h1>

    Why do we want to do this? Transfer learning is the key to general intelligence!

    <h2>Definition</h2>

    What do we mean by transfer in "transfer learning"? If we have two tasks/environments/action spaces/...?, $A, B$, then the performance of one task aids the other task.


    A MDP is defined as
    $$M = \Big(S, \mathcal A, p(\cdot \mid s,a), R(s, a, s') \Big)$$


    <ul>
      <li>$S$: It is possible to change the state space, while preserving the dynamics. (??)</li>
      <li>$\mathcal A$: Change the action space, for example, instead of $\leftarrow, \rightarrow, \uparrow, \downarrow$ we use $\uparrow, \text{rot90}$</li>
      <li>$p(\cdot \mid s,a)$: from subtle things like not being able to reach a state on another one, to chan</li>
      <li>$R(s, a, s')$: A different reward funciton, aka a different task.</li>
    </ul>

    But one could imagine symmetries of $p(\cdot \mid s,a), R(s, a, s')$, such that some structure is preserved.

    $$
    \begin{align}
    p(\cdot \mid s,a) &= T(p(\cdot \mid s,a)) \\
    &= p(\cdot \mid T(s),a) \tag{equiv to transfer to a new state space}\\
    &= p(\cdot \mid s,T(a)) \\
    R(s, a, s') &= T(R(s, a, s')) \\
    \end{align}
    $$

    <p>Huh, never thought about it this way before. The states are an unordered set.
      The transition fn provides all the structure on that space (much like an inner prod in Hilbert spaces?!?)
      The neighbors of a state are the positions reachable from a single action.
    No not quite. More like probabilistic vector maps? No that is only when combined with a policy.</p>

    Best current solutions!?
    <ul>
      <li>successor representation/goal embeddings. $\to$ task transfer</li>
      <li>model-based RL (disentangle policy from model) allows transfer of control polices between environments and transfer of model between tasks in the same env.</li>
      <li>why was it transferred? (because the domains somehow shared similarities)</li>
    </ul>


    $$
    \begin{align}
    L(T(B)) \le L(T(A) \to T(B)) \tag{Forward transfer}\\
    L(T(A)) \le L(T(A) \to T(B)) \tag{Backward transfer} \\
    \end{align}
    $$

    Relationship to meta-learning. Different <i>'levels'</i> of knowledge can be transfered.
    <ol>
      <li>in transfer learning we tend to deal with absolute knowledge about one domain (say a robot simulation)</li>
      <li>meta learning transfers <i>meta</i> knowledge about how to learn (thus it is also called learning to learn)</li>
    </ol>

    How can we tease apart these definitions into a heirarchy?

    $$
    \begin{align}
    \dot L(T(B)) \le \dot L(T(A) \to T(B)) \\
    \dot L(T(A)) \le \dot L(T(A) \to T(B)) \\
    \end{align}
    $$


    <h2>Analysis</h2>

    What I would really like is a set of tools for analysing transfer learning.
    I would like to be able to answer the questions;

    <ul>
      <li>what knowledge was transferred (high level, low level, ...?)</li>
      <li>how was it transferred? (if we are dealing with NNs then how does some knowledge get shared while other knowledge doesnt?
        because the existing knowledge allows faster learning?!)</li>
      <li>why was it transferred? (because the domains somehow shared similarities)</li>
    </ul>

    Seems quite realted to representation learning. The key will be how knowledge is represented, and how easily that knowledge can be translated (/transformed)!?

    <h2>Toy problems</h2>

    Two environments, only difference is visual appearance, ...
    Two environments,

    Want to generate different MDPs that share various 'orders' of similarity.
    Navigation on graphs? Various orders of persistent homology?

    <h2>Heirarhcal RL</h2>

    <p><b><u>Conjecture</u></b>: meta/transfer/continual/... learning naturally emerge from heirarchical/multi-scale RL.</p>

    We naturally see a decomposition of the tasks into what they share at different levels of abstraction (not just time!?).

    <h2>Resources</h2>

    <ul>
      <li><a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Learning to learn</a></li>
      <li><a href="https://arxiv.org/abs/1612.00796">EWC</li>
    </ul>
  </div>
</body>
</html>
