<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>exploitable</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../style.css">
  <script src="../docs/mathjax.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/x-mathjax-config">
    // MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      "HTML-CSS": { fonts: ["TeX"] }
    });
  </script>
</head>
<body>
<header>
<h1 class="title">exploitable</h1>
</header>
<h1 id="exploitable-models">Exploitable models</h1>
<p>Why do we want to do this?</p>
<h2 id="definition">Definition</h2>
<p>What could help the planner do its job better? Receiving information about he structure of the model? Being able to use the model in ‘other’ ways. <strong>Q</strong> How can we design a system where the model adapts itself to me maximally useful to the planner?!</p>
<p>Thus some nice properties of a model might be;</p>
<ul>
<li>it is invertible</li>
<li>it can be run at smaller/larger time steps (allowing the controller to trade-off computation and accuracy)</li>
<li>it can more/less local/global interactions (again allowing the controller to trade-off computation and accuracy)</li>
</ul>
<p>Want to think about this as two agents working together to achieve their goals.</p>
<ul>
<li>Planner’s goal is to maximise external reward.</li>
<li>Model’s goal is to maximise ???.</li>
</ul>
<p>$$ <span class="math display">\[\begin{align*}
C: M\times V \to a \\

\end{align*}\]</span> $$</p>
<h2 id="reward-hacking-and-model-bias">Reward hacking and model bias</h2>
<p>If the model is not accurate then we can easily plan for fantastic outcomes.</p>
<p>Maybe the model thinks it is possible to walk through a certian wall, or …</p>
<p>How can we avoid the planner exploiting this inaccuracy to plan for high imagined reward, with impossible/poor results.</p>
<p>Similarity is that we want to constrain the space of policies to ones that are ‘reasonable’. We want policies that; - match the intent of the training reward (which might be a proxy) - work in the real world - ?</p>
<p>Maybe this isnt a problem? If the model though it was possible and then the planner attempts to do it, then there will be a large surprise signal!?</p>
<h2 id="resources">Resources</h2>
</body>
</html>
