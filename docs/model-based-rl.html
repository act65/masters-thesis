<!DOCTYPE html>
<html>
<meta charset="utf-8">
  <head>
    <title>Model-based RL</title>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML-full">
    </script>
    <link rel="stylesheet" href="style.css">
 </head>
 <body>
  <div>
    <h1>Model-based RL</h1>

    Why do we want to do this? Motivation! What problem does model-based RL solve?

    Want to find certain settings where model-free > model-based and vice versa.

    Number of samples $n$, a measure of the complexity of the environment $k$, the complexity of the model $N$, ...??? (what else?)

    $$
    d(\nabla \pi, G_{\text{model-based}}) = poly(n, k, N) \\
    d(\nabla \pi, G_{\text{model-free}}) = exp(n, k, N) \\
    $$

    problem is that this ignores the actual optimisation (it could be that some types of inaccuracy in the gradient estimation actually aid learning...)


    Is there a case where a model of the dynamics naturally emerges?


    <h2>Definition</h2>

    ??? A reinforcement learner that does not have an explicit model of the transition function.
    Struggling with this definition. Best I can do is related to how the learning is supervised.

    Model-free only gets access to the reward. Model-based also uses the state-action trajectories.
    (I want to show that this extra learning signal can change performance from exp to poly)




    <blockquote>
      conventional wisdom holds that model-free methods are less efficient but
      achieve the best asymptotic performance, while model-based methods are
      more efficient but do not produce policies that are as optimal
      (<a href=https://bair.berkeley.edu/blog/2018/04/26/tdm/>TDM</a>)
    </blockquote>

    <h2>A fundamental trade-off</h2>

    We need to sample reality... If not we can end up planning a fantasy.

    (when failing is cheap it can be easier to try, make a bunch of errors, rather than plan. esp with innaccurate models...)


    <p>In the case where the environment is more complex than can possibly be modelled, then ...?
    Planning isnt going to work... But, what advantages can be gained from an partial model?</p>

    <p>In the case where the task is very simple. If light is red, push button.
    Dont need a complex model of the environment...</p>

    what about models that capture high level/deep principles? While inaccurate (in their ability to predict a future state, they ...???)

    It depends on how expensive it is to sample the real reward!?
    If queries are cheap/unrestricted, then let's just do model-free RL!?
    How can we assign cost to queries to the oracle? Are there different types of cost?

    <ul>
      <li>memory/size of policy</li>
      <li>calls to the oracle</li>
      <li>reward received</li>
    </ul>

    <hr></hr>

    This is really just a question of gradient estimation.
    What is the variance/bias of the estimator?

    Want to see proof that Q-learning and PG have zero bias. Want to bound their variance.
    (is that all we care about!? -- Is the gradient pointing in the right direction? Does it have the right magnitude?)

    <p>How could this be studied? Construct a differentiable model and check the accuracy of various estimators.</p>

    Dont actually care if $\tau (s_t, a_t) = \hat \tau (s_t, a_t)$ we care if $\nabla \tau (s_t, a_t) = \nabla \hat \tau (s_t, a_t)$.
    Is there a way to evaluate the gradient of the transition function?

    <hr></hr>

    <p>(In the unconstrained memory case)
    <u><b>Cojecture:</b></u> Model-based learning is the optimal solution to model-free learning</p>

    I can imagine a model-free system learning to learn to use future inputs as targets to learn a model!!?!
    If we used a very large RNN to model $Q(s_t, a_t)$, it could correlate the difference between past and future state and actions, thus ...?


    <h2>Resources</h2>

    <ul>
      <li><a href=https://bair.berkeley.edu/blog/2018/04/26/tdm/>TDM</a></li>
      <li><a href=https://arxiv.org/abs/1606.05312>Successor features</a></li>
      <li><a href=https://openreview.net/forum?id=Hkxr1nCcFm>Model free planning</a></li>
    </ul>
  </div>
</body>
</html>
