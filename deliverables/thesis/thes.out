\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Reinforcement learning}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Understanding Theoretical Reinforcement learning}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Understanding Markov decision problems}{section.1.1}% 4
\BOOKMARK [2][-]{subsection.1.1.3}{Abstraction}{section.1.1}% 5
\BOOKMARK [0][-]{chapter.2}{MDPs}{}% 6
\BOOKMARK [1][-]{subsection.2.0.1}{Sequential decision problems}{chapter.2}% 7
\BOOKMARK [2][-]{subsection.2.0.2}{MDPs}{subsection.2.0.1}% 8
\BOOKMARK [2][-]{subsection.2.0.3}{MDPs in the real world}{subsection.2.0.1}% 9
\BOOKMARK [1][-]{section.2.1}{The value function polytope}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.1.1}{Distribution of policies}{section.2.1}% 11
\BOOKMARK [2][-]{subsection.2.1.2}{Discounting}{section.2.1}% 12
\BOOKMARK [1][-]{section.2.2}{Search spaces}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.2.1}{Dynamics and complexity}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.2}{Search spaces and gradient descent}{section.2.2}% 15
\BOOKMARK [0][-]{chapter.3}{Abstraction}{}% 16
\BOOKMARK [1][-]{section.3.1}{Solveable representations}{chapter.3}% 17
\BOOKMARK [2][-]{subsection.3.1.1}{Other properties}{section.3.1}% 18
\BOOKMARK [1][-]{section.3.2}{Near optimal abstractions}{chapter.3}% 19
\BOOKMARK [2][-]{subsection.3.2.1}{Discussion}{section.3.2}% 20
\BOOKMARK [0][-]{chapter.4}{Symmetry}{}% 21
\BOOKMARK [0][-]{chapter.5}{Conclusions}{}% 22
\BOOKMARK [0][-]{appendix.A}{Related work}{}% 23
\BOOKMARK [1][-]{section.A.1}{Related work}{appendix.A}% 24
\BOOKMARK [2][-]{subsection.A.1.1}{Model-based RL}{section.A.1}% 25
\BOOKMARK [2][-]{subsection.A.1.2}{Representation learning and abstraction}{section.A.1}% 26
\BOOKMARK [2][-]{subsection.A.1.3}{Heirarchical reinforcement learning}{section.A.1}% 27
