1. In my proposal I stated that I wanted to understand __transfer__ and __HRL__. With the goal of ...?
2. I completed four 'sprints', each of two weeks. __HRL, exploration, IRL, disentanglement.__
3. I stumbled across a nice paper connecting disentanglement and ...? Had the idea of __action abstractions__.
4. After blindly pacing around action abstractions for a few weeks, I finally understood that HRL is a special case of __learning abstractions__, just one of a special structure. I also became aware that no abstraction is likely to do better or worse than any other, in the general case. Also, I became less enthused with action abstractions as there were some rather straight forward experiments that could be done, but I couldn't see how they might help me understand when / why action abstractions may or may not help.
5. I explored theoretical results about __Near optimal abstractions__: aka how well you can expect to do given an abstraction with certain properties.
6. __Solveable abstractions__. I spent some time thinking about why we care about abstractions. We want to throw away the unimportant parts, abstract, so we can focus on the essential. Thus making the problem easier to solve, in some sense.
7. After reading a few papers on the theory of RL, I decided I wanted a better understanding of __MDPs__, which were the main setting considered in the proofs I had been attempting to understand.
8. I found a great paper (__Value function polytope__) that gave insight into the structure of the MDP and it's optimisation. I explored this further and combined it with some theoretical work on optimisation in over parameterised spaces.
9. Now, with my new understanding of MDPs, I returned to the problem of abstraction. And what better try of abstraction to try to understand than a linear abstraction? Thus spent some time trying to dissect __Linear MDPs__.
10. My exploration of LMDPs gave me some ideas about a potential __heirarchical LMDP__ that exploits the linearity of an LMDP. This turned out not to be a new idea, but there seemed to be room for improvement.
11. I finally returned to some ideas from action abstractions and near optimal learners about exploiting __symmetries__ within an MDP to reduce the complexity of the problem.
12.
