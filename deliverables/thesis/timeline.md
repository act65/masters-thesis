1. In my proposal I stated that I wanted to understand __transfer__ and __HRL__. With the goal of understanding and improving an agents ability to learn efficiently.
2. I completed four 'sprints'; __HRL, exploration, IRL, disentanglement__, each of two weeks.
3. I stumbled across a nice paper connecting a disentangled representation and independent actions (http://willwhitney.com/assets/papers/Disentangling.video.with.independent.prediction.pdf). This led me to focus on __action abstractions__.
4. After blindly pacing around 'action abstractions' for a few weeks, I finally understood that HRL is a special case of __abstraction__, just one of a special heirarchical structure, and focused on achieving a temporal abstraction. I also became aware that no abstraction is likely to do better or worse than any other, in the general case (once you account for the complexity of the abstraction). Also, I became less enthused with action abstractions as there were some rather straight forward experiments that could be done, but I couldn't see how they might help me understand when / why action abstractions may or may not help.
5. I explored theoretical results about __Near optimal abstractions__: aka how well you can expect to do given an abstraction with certain properties.
6. __Solvable abstractions__. I spent some time thinking about why we care about abstractions. We want to throw away the unimportant parts, so we can focus on the essential. The point is to make the problem easier to solve, in some sense.
7. After reading a few papers on the theory of RL, I decided I wanted a better understanding of __MDPs__, which were the main setting considered in the proofs I had been attempting to understand.
8. I found a great paper (__[Value function polytope](https://arxiv.org/abs/1901.11524)__) that gave insight into the structure of the MDP and it's optimisation. I explored this further and combined it with some theoretical work on [acceleration via overparameterisation](https://arxiv.org/abs/1802.06509).
9. Now, with my new understanding of MDPs, I returned to the problem of abstraction. A simple, and possibly understandable, type of abstraction is a linear abstraction. But what do we mean by linear? I spent some time trying to dissect __'Linear MDPs'__.
10. My exploration of LMDPs gave me some ideas about a potential __heirarchical LMDP__. It exploits the linearity of an LMDP to allow the composition of previously learned optimal controls. This turned out not to be a new idea, but there seemed to be room for improvement.
11. I finally returned to some ideas from action abstractions and near optimal learners about exploiting __symmetries__ within an MDP to reduce the complexity of the problem.
12.
