@article{Box1997,
author = {Box, Skinner},
file = {:home/telfaralex/Downloads/Knock{\_}Wood.pdf:pdf},
number = {1948},
pages = {168--172},
title = {{Skinner, B. F. (1948). Superstition in the pigeon.}},
year = {1997}
}
@article{Yang2019,
abstract = {This work provides theoretical and empirical evidence that invariance-inducing regularizers can increase predictive accuracy for worst-case spatial transformations (spatial robustness). Evaluated on these adversarially transformed examples, we demonstrate that adding regularization on top of standard or adversarial training reduces the relative error by 20{\%} for CIFAR10 without increasing the computational cost. This outperforms handcrafted networks that were explicitly designed to be spatial-equivariant. Furthermore, we observe for SVHN, known to have inherent variance in orientation, that robust training also improves standard accuracy on the test set. We prove that this no-trade-off phenomenon holds for adversarial examples from transformation groups in the infinite data limit.},
archivePrefix = {arXiv},
arxivId = {1906.11235},
author = {Yang, Fanny and Wang, Zuowen and Heinze-Deml, Christina},
eprint = {1906.11235},
file = {:home/telfaralex/Downloads/1906.11235.pdf:pdf},
title = {{Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness}},
url = {http://arxiv.org/abs/1906.11235},
year = {2019}
}
@article{Wang2018,
abstract = {Causal inference from observational data often assumes "ignorability," that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.},
archivePrefix = {arXiv},
arxivId = {1805.06826},
author = {Wang, Yixin and Blei, David M.},
eprint = {1805.06826},
file = {:home/telfaralex/Downloads/1805.06826.pdf:pdf},
keywords = {causal inference,probabilistic models,strong ignorability},
pages = {1--72},
title = {{The Blessings of Multiple Causes}},
url = {http://arxiv.org/abs/1805.06826},
year = {2018}
}
@article{Mooij2016,
abstract = {The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets that elegantly unifies both approaches. JCI is a causal modeling approach rather than a specific algorithm, and it can be used in combination with any causal discovery algorithm that can take into account certain background knowledge. The main idea is to reduce causal discovery from multiple datasets originating from different contexts (e.g., different experimental conditions) to causal discovery from a single pooled dataset by adding auxiliary context variables and incorporating applicable background knowledge on the causal relationships involving the context variables. We propose different flavours of JCI that differ in the amount of background knowledge that is assumed. JCI can deal with several different types of interventions in a unified fashion, does not require knowledge on intervention targets or types in case of interventional data, and allows one to fully exploit all the information in the joint distribution on system and context variables. We explain how some well-known causal discovery algorithms can be seen as implementations of the JCI framework, but we also propose novel implementations that are simple adaptations of existing causal discovery methods for purely observational data to the JCI setting. We evaluate different implementations of the JCI approach on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can outperform state-of-the-art causal discovery algorithms.},
archivePrefix = {arXiv},
arxivId = {1611.10351},
author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
eprint = {1611.10351},
file = {:home/telfaralex/Downloads/1611.10351.pdf:pdf},
title = {{Joint Causal Inference from Multiple Contexts}},
url = {http://arxiv.org/abs/1611.10351},
year = {2016}
}
@article{Alt2019,
abstract = {Many decision-making problems naturally exhibit pronounced structures inherited from the underlying characteristics of the environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations, often implying locally correlated transition dynamics among the states. In order to complete a certain task, an agent acting in such environments needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to account for correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on P$\backslash$'olya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related tasks, such as reinforcement learning, imitation learning and system identification. By explicitly modeling the underlying correlation structures, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are up to an order of magnitude smaller in size.},
archivePrefix = {arXiv},
arxivId = {1909.05106},
author = {Alt, Bastian and {\v{S}}o{\v{s}}i{\'{c}}, Adrian and Koeppl, Heinz},
eprint = {1909.05106},
file = {:home/telfaralex/Downloads/1909.05106v1.pdf:pdf},
pages = {1--14},
title = {{Correlation Priors for Reinforcement Learning}},
url = {http://arxiv.org/abs/1909.05106},
year = {2019}
}
@article{Goyal2019,
abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
archivePrefix = {arXiv},
arxivId = {1909.10893},
author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"{o}}lkopf, Bernhard},
eprint = {1909.10893},
file = {:home/telfaralex/Downloads/1909.10893.pdf:pdf},
pages = {1--33},
title = {{Recurrent Independent Mechanisms}},
url = {http://arxiv.org/abs/1909.10893},
year = {2019}
}
@article{Shu2019,
abstract = {Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.},
archivePrefix = {arXiv},
arxivId = {1910.09772},
author = {Shu, Rui and Chen, Yining and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
eprint = {1910.09772},
file = {:home/telfaralex/Downloads/1910.09772.pdf:pdf},
pages = {1--36},
title = {{Weakly Supervised Disentanglement with Guarantees}},
url = {http://arxiv.org/abs/1910.09772},
year = {2019}
}
@article{Holtzen2019,
abstract = {A key goal in the design of probabilistic inference algorithms is identifying and exploiting properties of the distribution that make inference tractable. Lifted inference algorithms identify symmetry as a property that enables efficient inference and seek to scale with the degree of symmetry of a probability model. A limitation of existing exact lifted inference techniques is that they do not apply to non-relational representations like factor graphs. In this work we provide the first example of an exact lifted inference algorithm for arbitrary discrete factor graphs. In addition we describe a lifted Markov-Chain Monte-Carlo algorithm that provably mixes rapidly in the degree of symmetry of the distribution.},
archivePrefix = {arXiv},
arxivId = {1903.04672},
author = {Holtzen, Steven and Millstein, Todd and den Broeck, Guy Van},
eprint = {1903.04672},
file = {:home/telfaralex/Downloads/HoltzenUAI19.pdf:pdf},
title = {{Generating and Sampling Orbits for Lifted Probabilistic Inference}},
url = {http://arxiv.org/abs/1903.04672},
year = {2019}
}
@article{Wang2019a,
abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/{\~{}}tingwuwang/mbrl.html.},
archivePrefix = {arXiv},
arxivId = {1907.02057},
author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
eprint = {1907.02057},
file = {:home/telfaralex/Downloads/1907.02057.pdf:pdf},
pages = {1--25},
title = {{Benchmarking Model-Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1907.02057},
year = {2019}
}
@article{Abdolhosseini,
author = {Abdolhosseini, Farzad and Peng, Xue Bin},
file = {:home/telfaralex/Downloads/2019-MIG-symmetry.pdf:pdf},
isbn = {9781450369947},
keywords = {acm reference format,and michiel,farzad abdolhosseini,hung yu ling,locomotion,reinforcement learning,symmetry,xue bin peng,zhaoming xie},
title = {{On Learning Symmetric Locomotion}}
}
@article{Du2019,
abstract = {Modern deep learning methods provide an effective means to learn good representations. However, is a good representation itself sufficient for efficient reinforcement learning? This question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work provides strong negative results for reinforcement learning methods with function approximation for which a good representation (feature extractor) is known to the agent, focusing on natural representational conditions relevant to value-based learning and policy-based learning. For value-based learning, we show that even if the agent has a highly accurate linear representation, the agent still needs to sample exponentially many trajectories in order to find a near-optimal policy. For policy-based learning, we show even if the agent's linear representation is capable of perfectly representing the optimal policy, the agent still needs to sample exponentially many trajectories in order to find a near-optimal policy. These lower bounds highlight the fact that having a good (value-based or policy-based) representation in and of itself is insufficient for efficient reinforcement learning. In particular, these results provide new insights into why the existing provably efficient reinforcement learning methods rely on further assumptions, which are often model-based in nature. Additionally, our lower bounds imply exponential separations in the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.},
archivePrefix = {arXiv},
arxivId = {1910.03016},
author = {Du, Simon S. and Kakade, Sham M. and Wang, Ruosong and Yang, Lin F.},
eprint = {1910.03016},
file = {:home/telfaralex/Downloads/1910.03016.pdf:pdf},
title = {{Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?}},
url = {http://arxiv.org/abs/1910.03016},
year = {2019}
}
@article{Anselmi2019,
author = {Anselmi, Fabio and Evangelopoulos, Georgios and Rosasco, Lorenzo and Poggio, Tomaso},
doi = {10.1016/j.patcog.2018.07.025},
file = {:home/telfaralex/Downloads/1-s2.0-S0031320318302620-main.pdf:pdf},
keywords = {Representation learning,Equivariant representation,equivariant representations,invariant representations,representation learning},
pages = {201--208},
publisher = {Elsevier Ltd},
title = {{Symmetry-adapted representation learning}},
volume = {86},
year = {2019}
}
@article{Whitney2019,
abstract = {In complex simulated environments, model-based reinforcement learning methods typically lag the asymptotic performance of model-free approaches. This paper uses two MuJoCo environments to understand this gap through a series of abla-tion experiments designed to separate the contributions of the dynamics model and planner. These reveal the importance of long planning horizons, beyond those typically used. A dynamics model that directly predicts distant states, based on current state and a long sequence of actions, is introduced. This avoids the need for many recursions during long-range planning, and thus is able to yield more accurate state estimates. These accurate predictions allow us to uncover the relationship between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model-free approaches.},
author = {Whitney, William F and Fergus, Rob},
file = {:home/telfaralex/Downloads/Understanding.the.Asymptotic.Performance.of.MBRL.pdf:pdf},
pages = {1--11},
title = {{Understanding the Asymptotic Performance of Model-Based Rl Methods}},
url = {https://trello-attachments.s3.amazonaws.com/5bd177a855f78722ac8df3bc/5bf9a1cdddca69795cb06153/a55081b20aeed2dfcd384d2e6df85a70/Understanding.the.Asymptotic.Performance.of.MBRL.pdf},
year = {2019}
}
@article{Chen2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1907.10905v1},
author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
eprint = {arXiv:1907.10905v1},
file = {:home/telfaralex/Downloads/1907.10905.pdf:pdf},
pages = {1--44},
title = {{Invariance reduces Variance : Understanding Data Augmentation in Deep Learning and Beyond}},
year = {2019}
}
@article{Kaiser2019,
abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of {\$}100{\$}K interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1903.00374},
author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
eprint = {1903.00374},
file = {:home/telfaralex/Downloads/1903.00374.pdf:pdf},
title = {{Model-Based Reinforcement Learning for Atari}},
url = {http://arxiv.org/abs/1903.00374},
year = {2019}
}
@article{Gheshlaghi2017,
author = {Gheshlaghi, Mohammad and Ian, Azar and R{\'{e}}mi, Osband},
file = {:home/telfaralex/Downloads/azar17a.pdf:pdf},
title = {{Minimax Regret Bounds for Reinforcement Learning}},
year = {2017}
}
@article{Alagoz2015,
abstract = {We compare the computational performance of linear programming (LP) and the policy iteration algorithm (PIA) for solving discrete-time infinite-horizon Markov decision process (MDP) models with total expected discounted reward. We use randomly generated test problems as well as a real-life health-care problem to empirically show that, unlike previously reported, barrier methods for LP provide a viable tool for optimally solving such MDPs. The dimensions of comparison include transition probability matrix structure, state and action size, and the LP solution method.},
author = {Alagoz, Oguzhan and Ayvaci, Mehmet U.S. and Linderoth, Jeffrey T},
doi = {10.1016/j.cie.2015.05.031},
file = {:home/telfaralex/Downloads/1-s2.0-S0360835215002545-main (1).pdf:pdf},
issn = {03608352},
journal = {Computers and Industrial Engineering},
keywords = {Linear programming,MDP,Markov decision process,Policy iteration,Total expected discounted reward,Treatment optimization},
pages = {311--316},
publisher = {Elsevier Ltd},
title = {{Optimally solving Markov decision processes with total expected discounted reward function: Linear programming revisited}},
url = {http://dx.doi.org/10.1016/j.cie.2015.05.031},
volume = {87},
year = {2015}
}
@article{Arora2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.06509v2},
author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
eprint = {arXiv:1802.06509v2},
file = {:home/telfaralex/Downloads/1802.06509.pdf:pdf},
title = {{On the Optimization of Deep Networks : Implicit Acceleration by Overparameterization}},
year = {2018}
}
@article{Littman,
author = {Littman, Michael L and Dean, Thomas L and Kaelbling, Leslie Pack},
file = {:home/telfaralex/Downloads/1302.4971.pdf:pdf},
pages = {394--402},
title = {{On the Complexity of Solving Markov Decision Problems}}
}
@article{Dadashi2018,
abstract = {We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective to introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1901.11524},
author = {Dadashi, Robert and Ta{\"{i}}ga, Adrien Ali and Roux, Nicolas Le and Schuurmans, Dale and Bellemare, Marc G},
eprint = {1901.11524},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dadashi et al. - 2018 - The Value Function Polytope in Reinforcement Learning.pdf:pdf},
title = {{The Value Function Polytope in Reinforcement Learning}},
url = {http://arxiv.org/abs/1901.11524},
year = {2019}
}
@article{Mahajan2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.02999v1},
author = {Mahajan, Anuj and Tulabandhula, Theja},
eprint = {arXiv:1706.02999v1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahajan, Tulabandhula - 2017 - Symmetry Learning for Function Approximation in Reinforcement Learning.pdf:pdf},
pages = {1--12},
title = {{Symmetry Learning for Function Approximation in Reinforcement Learning}},
year = {2017}
}
@article{Bubeck2018,
author = {Bubeck, Sebastien and Jordan, Michael I},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bubeck, Jordan - 2018 - Is Q-learning Provably Efficient.pdf:pdf},
number = {NeurIPS},
title = {{Is Q-learning Provably Efficient ?}},
year = {2018}
}
@article{Nachum2019,
abstract = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},
archivePrefix = {arXiv},
arxivId = {1909.10618},
author = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
eprint = {1909.10618},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nachum et al. - 2019 - Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning.pdf:pdf},
title = {{Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?}},
url = {http://arxiv.org/abs/1909.10618},
year = {2019}
}
@article{Wang,
archivePrefix = {arXiv},
arxivId = {arXiv:1907.05388v1},
author = {Wang, Zhaoran and Jordan, Michael I},
eprint = {arXiv:1907.05388v1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Jordan - Unknown - Provably Efficient Reinforcement Learning with Linear Function Approximation.pdf:pdf},
pages = {1--28},
title = {{Provably Efficient Reinforcement Learning with Linear Function Approximation}}
}
@article{Pires2016,
author = {Pires, Bernardo {\'{A}}vila},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pires - 2016 - Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models.pdf:pdf},
pages = {1--31},
title = {{Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models}},
volume = {49},
year = {2016}
}
@article{Zhong,
author = {Zhong, Mingyuan and Todorov, Emanuel},
doi = {10.1109/ADPRL.2011.5967383},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong, Todorov - Unknown - Moving Least-squares Approximations for Linearly-solvable MDP.pdf:pdf},
journal = {2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
pages = {218--225},
publisher = {IEEE},
title = {{Moving Least-squares Approximations for Linearly-solvable MDP}}
}
@article{Todorov2006,
author = {Todorov, Emanuel},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Todorov - 2006 - Linearly-solvable Markov decision problems.pdf:pdf},
number = {1},
title = {{Linearly-solvable Markov decision problems}},
year = {2006}
}
@article{Zhonga,
author = {Zhong, Mingyuan and Todorov, Emanuel},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong, Todorov - Unknown - Aggregation Methods for Lineary-solvable Markov Decision Process ⋆.pdf:pdf},
keywords = {markov desicion processs,optimal control,robotics,stochastic control},
title = {{Aggregation Methods for Lineary-solvable Markov Decision Process ⋆}}
}
@article{Dvijotham,
author = {Dvijotham, Krishnamurthy and Todorov, Emanuel},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dvijotham, Todorov - Unknown - A Unifying Framework for Linearly Solvable Control.pdf:pdf},
title = {{A Unifying Framework for Linearly Solvable Control}}
}
@article{Wozabal,
author = {Wozabal, David and Kiszka, Adriana},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wozabal, Kiszka - Unknown - A Stability Result for Linear Markov Decision Processes.pdf:pdf},
keywords = {scenario,stochastic optimization,wasserstein distance},
title = {{A Stability Result for Linear Markov Decision Processes}}
}
@article{Pashevich,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.00025v1},
author = {Pashevich, Alexander and Davidson, James and Sukthankar, Rahul and Schmid, Cordelia},
eprint = {arXiv:1812.00025v1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pashevich et al. - Unknown - Modulated Policy Hierarchies.pdf:pdf},
title = {{Modulated Policy Hierarchies}}
}
@article{Rafati,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.10096v3},
author = {Rafati, Jacob and Noelle, David C},
eprint = {arXiv:1810.10096v3},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rafati, Noelle - Unknown - Learning Representations in Model-Free Hierarchical Reinforcement Learning.pdf:pdf},
pages = {1--35},
title = {{Learning Representations in Model-Free Hierarchical Reinforcement Learning}}
}
@article{Nachum2018,
abstract = {We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).},
archivePrefix = {arXiv},
arxivId = {1810.01257},
author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
eprint = {1810.01257},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nachum et al. - 2018 - Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.pdf:pdf},
pages = {1--18},
title = {{Near-Optimal Representation Learning for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1810.01257},
year = {2018}
}
@article{Bellemare2019b,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.11530v2},
author = {Bellemare, Marc G and Dabney, Will and Dadashi, Robert and Taiga, Adrien Ali},
eprint = {arXiv:1901.11530v2},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare et al. - 2019 - A Geometric Perspective on Optimal Representations for Reinforcement Learning(2).pdf:pdf},
pages = {1--27},
title = {{A Geometric Perspective on Optimal Representations for Reinforcement Learning}},
year = {2019}
}
@article{Saxea,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.02757v1},
author = {Saxe, Andrew M and Earle, Adam and Rosman, Benjamin},
eprint = {arXiv:1612.02757v1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxe, Earle, Rosman - Unknown - Solvable Markov Decision Processes.pdf:pdf},
title = {{Solvable Markov Decision Processes}}
}
@article{Caselles-Dupre2019,
abstract = {Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on fixed data samples. Agents should interact with the environment to discover its symmetries. Our experiments can be reproduced on Colab: http://bit.do/eKpqv},
archivePrefix = {arXiv},
arxivId = {1904.00243},
author = {Caselles-Dupr{\'{e}}, Hugo and Garcia-Ortiz, Michael and Filliat, David},
eprint = {1904.00243},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caselles-Dupr{\'{e}}, Garcia-Ortiz, Filliat - 2019 - Symmetry-Based Disentangled Representation Learning requires Interaction with Environmen.pdf:pdf},
number = {NeurIPS},
pages = {1--16},
title = {{Symmetry-Based Disentangled Representation Learning requires Interaction with Environments}},
url = {http://arxiv.org/abs/1904.00243},
year = {2019}
}
@article{Scholkopf2001,
abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
author = {Sch{\"{o}}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sch{\"{o}}lkopf, Herbrich, Smola - 2001 - A generalized representer theorem.pdf:pdf},
isbn = {9783540423430},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {416--426},
title = {{A generalized representer theorem}},
volume = {2111},
year = {2001}
}
@article{Kim2018,
abstract = {Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI .},
archivePrefix = {arXiv},
arxivId = {1810.01176},
author = {Kim, Hyoungseok and Kim, Jaekyeom and Jeong, Yeonwoo and Levine, Sergey and Song, Hyun Oh},
eprint = {1810.01176},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - EMI Exploration with Mutual Information.pdf:pdf},
title = {{EMI: Exploration with Mutual Information}},
url = {http://arxiv.org/abs/1810.01176},
year = {2018}
}
@article{Sutton1999,
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges in AI. The ways how to address these challenges within the mathematical framework of reinforcement learning and Markov decision processes (MDP) are discussed. A set options defined over an MDP constitutes a semi-MDP (SMDP), and the theory of SMDPs provides the foundation for the theory of options.},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1016/S0004-3702(99)00052-1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Precup, Singh - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
year = {1999}
}
@book{Putterman2015,
abstract = {Markov decision processes (MDPs), an established framework for specifying task environments for decision-making agents, are valuable in the behavioral sciences as a formal description of sequential problems faced by animals and people. This article provides a mathematical definition of MDPs, an example application, an introduction to algorithms for finding optimal policies in MDPs, and a list of useful extensions to the basic model.},
author = {Putterman, Martin},
booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences: Second Edition},
doi = {10.1016/B978-0-08-097086-8.43055-2},
file = {:home/telfaralex/Downloads/epdf.pub{\_}markov-decision-processes-discrete-stochastic-dyna.pdf:pdf},
isbn = {9780080970875},
keywords = {Algorithms,Markov decision processes},
pages = {573--575},
title = {{Markov Decision Processes}},
year = {1994}
}
@article{Lee2019,
abstract = {To solve tasks with sparse rewards, reinforcement learning algorithms must be equipped with suitable exploration techniques. However, it is unclear what underlying objective is being optimized by existing exploration algorithms, or how they can be altered to incorporate prior knowledge about the task. Most importantly, it is difficult to use exploration experience from one task to acquire exploration strategies for another task. We address these shortcomings by learning a single exploration policy that can quickly solve a suite of downstream tasks in a multi-task setting, amortizing the cost of learning to explore. We recast exploration as a problem of State Marginal Matching (SMM): we learn a mixture of policies for which the state marginal distribution matches a given target state distribution, which can incorporate prior knowledge about the task. Without any prior knowledge, the SMM objective reduces to maximizing the marginal state entropy. We optimize the objective by reducing it to a two-player, zero-sum game, where we iteratively fit a state density model and then update the policy to visit states with low density under this model. While many previous algorithms for exploration employ a similar procedure, they omit a crucial historical averaging step, without which the iterative procedure does not converge to a Nash equilibria. To parallelize exploration, we extend our algorithm to use mixtures of policies, wherein we discover connections between SMM and previously-proposed skill learning methods based on mutual information. On complex navigation and manipulation tasks, we demonstrate that our algorithm explores faster and adapts more quickly to new tasks.},
archivePrefix = {arXiv},
arxivId = {1906.05274},
author = {Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
eprint = {1906.05274},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2019 - Efficient Exploration via State Marginal Matching.pdf:pdf},
month = {jun},
title = {{Efficient Exploration via State Marginal Matching}},
url = {http://arxiv.org/abs/1906.05274},
year = {2019}
}
@article{Mazumdar2019,
abstract = {We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in {\$}N{\$}-player general-sum linear quadratic games. In such games the state and action spaces are continuous and the unique global Nash equilibrium can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi-agent policy gradient. We first prove that the only critical point of the gradient dynamics in these games is the unique global Nash equilibrium. We then give sufficient conditions under which policy gradient will avoid the Nash equilibrium, and generate a large number of general-sum linear quadratic games that satisfy these conditions. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.},
archivePrefix = {arXiv},
arxivId = {1907.03712},
author = {Mazumdar, Eric and Ratliff, Lillian J. and Jordan, Michael I. and Sastry, S. Shankar},
eprint = {1907.03712},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mazumdar et al. - 2019 - Policy-Gradient Algorithms Have No Guarantees of Convergence in Continuous Action and State Multi-Agent Setting.pdf:pdf},
pages = {1--15},
title = {{Policy-Gradient Algorithms Have No Guarantees of Convergence in Continuous Action and State Multi-Agent Settings}},
url = {http://arxiv.org/abs/1907.03712},
year = {2019}
}
@article{Kroemer,
abstract = {A key question in intelligent robotics is how to create robots capable of directly interacting with the world around them to achieve their goals. The last decade has therefore seen substantial growth in research on the problem of robot manipulation. We aim to describe a representative subset of that research on using machine learning for manipulation, describing a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework, and highlighting the many remaining research opportunities and challenges.},
archivePrefix = {arXiv},
arxivId = {arXiv:1907.03146v2},
author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
eprint = {arXiv:1907.03146v2},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kroemer, Niekum, Konidaris - Unknown - A Review of Robot Learning for Manipulation Challenges, Representations, and Algorithms.pdf:pdf},
keywords = {Learning,Manipulation,Review,Robots},
title = {{A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms}}
}
@article{Luo2018a,
abstract = {Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires $\backslash$textit{\{}no explicit{\}} uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves state-of-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {1807.03858},
author = {Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
eprint = {1807.03858},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2018 - Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees.pdf:pdf},
pages = {1--27},
title = {{Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees}},
url = {http://arxiv.org/abs/1807.03858},
year = {2018}
}
@article{Dann2018,
abstract = {The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications, such as healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms achieves regret and PAC bounds that are minimax up to lower-order terms.},
archivePrefix = {arXiv},
arxivId = {1811.03056},
author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
eprint = {1811.03056},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dann et al. - 2018 - Policy Certificates Towards Accountable Reinforcement Learning.pdf:pdf},
title = {{Policy Certificates: Towards Accountable Reinforcement Learning}},
url = {http://arxiv.org/abs/1811.03056},
year = {2018}
}
@article{Krauth2019,
abstract = {We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within {\$}\backslashvarepsilon{\$} of the optimal LQR controller, each step of policy evaluation requires at most {\$}(n+d){\^{}}3/\backslashvarepsilon{\^{}}2{\$} samples, where {\$}n{\$} is the dimension of the state vector and {\$}d{\$} is the dimension of the input vector. On the other hand, only {\$}\backslashlog(1/\backslashvarepsilon){\$} policy improvement steps suffice, resulting in an overall sample complexity of {\$}(n+d){\^{}}3 \backslashvarepsilon{\^{}}{\{}-2{\}} \backslashlog(1/\backslashvarepsilon){\$}. We furthermore build on our analysis and construct a simple adaptive procedure based on {\$}\backslashvarepsilon{\$}-greedy exploration which relies on approximate PI as a sub-routine and obtains {\$}T{\^{}}{\{}2/3{\}}{\$} regret, improving upon a recent result of Abbasi-Yadkori et al.},
archivePrefix = {arXiv},
arxivId = {1905.12842},
author = {Krauth, Karl and Tu, Stephen and Recht, Benjamin},
eprint = {1905.12842},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krauth, Tu, Recht - 2019 - Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator.pdf:pdf},
pages = {1--48},
title = {{Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator}},
url = {http://arxiv.org/abs/1905.12842},
year = {2019}
}
@article{Liu2019a,
abstract = {Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which separates theory from practice. In this paper, we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity, where the gradient and iterate are instantiated by neural networks. In particular, the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.},
archivePrefix = {arXiv},
arxivId = {1906.10306},
author = {Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
eprint = {1906.10306},
file = {:home/telfaralex/Downloads/1906.10306.pdf:pdf},
pages = {1--38},
title = {{Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy}},
url = {http://arxiv.org/abs/1906.10306},
year = {2019}
}
@article{Achiam2019,
abstract = {Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.},
archivePrefix = {arXiv},
arxivId = {1903.08894},
author = {Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
eprint = {1903.08894},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Achiam, Knight, Abbeel - 2019 - Towards Characterizing Divergence in Deep Q-Learning.pdf:pdf},
title = {{Towards Characterizing Divergence in Deep Q-Learning}},
url = {http://arxiv.org/abs/1903.08894},
year = {2019}
}
@article{Lattimore2018,
abstract = {After nearly two years since starting to write the blog we have at last completed a first draft of the book, which is to be published by Cambridge University Press. The book is available for free as a PDF and will remain so after publication. We're grateful to Cambridge for allowing this. Without further ado, here is the link. Although we still have a few things we want to do, the manuscript is sufficiently polished to be useful. Of course we would greatly appreciate any comments you might have, including typos, errors in the proofs, missing references, confusing explanations or anything else you might notice. We will periodically update the book, so it would be helpful if you could quote the revision number on the cover when sending us your comments (banditalgs@gmail.com). The manuscript includes a lot of material not in the blog. The last seven chapters are all new, covering combinatorial (semi-)bandits, non-stationary bandits, ranking, pure exploration, Bayesian methods, Thompson sampling, partial monitoring and an introduction to learning in Markov decision processes. Those chapters that are based on blog posts have been cleaned up and often we have added significant depth. There is a lot of literature that we have not covered. Some of these missing topics are discussed in extreme brevity in the introduction to Part VII. It really is amazing how large the bandit literature has become and we're sorry not to have found space for everything. The book includes around 250 exercises, some of which have solutions. On average the exercises have been proofread less carefully than the rest of the book, so some caution is advised. The solutions to selected exercises are available here. Finally, we're very thankful for all the feedback already received, both on the blog and early drafts of the book.},
author = {Lattimore, Tor and Szepesvari, Csaba},
file = {:home/telfaralex/Downloads/book.pdf:pdf},
journal = {Cambridge University Press},
pages = {542},
title = {{Bandit Algorithms}},
year = {2018}
}
@article{Slivkins2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07272v2},
author = {Slivkins, Aleksandrs},
eprint = {arXiv:1904.07272v2},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Slivkins - 2019 - Introduction to Multi-Armed Bandits.pdf:pdf},
number = {January 2017},
title = {{Introduction to Multi-Armed Bandits}},
year = {2019}
}
@article{Irpan2019,
abstract = {In this work, we consider the problem of model selection for deep reinforcement learning (RL) in real-world environments. Typically, the performance of deep RL algorithms is evaluated via on-policy interactions with the target environment. However, comparing models in a real-world environment for the purposes of early stopping or hyperparameter tuning is costly and often practically infeasible. This leads us to examine off-policy policy evaluation (OPE) in such settings. We focus on OPE for value-based methods, which are of particular interest in deep RL, with applications like robotics, where off-policy algorithms based on Q-function estimation can often attain better sample complexity than direct policy optimization. Existing OPE metrics either rely on a model of the environment, or the use of importance sampling (IS) to correct for the data being off-policy. However, for high-dimensional observations, such as images, models of the environment can be difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper, we focus on the specific case of MDPs with continuous action spaces and sparse binary rewards, which is representative of many important real-world applications. We propose an alternative metric that relies on neither models nor IS, by framing OPE as a positive-unlabeled (PU) classification problem with the Q-function as the decision function. We experimentally show that this metric outperforms baselines on a number of tasks. Most importantly, it can reliably predict the relative performance of different policies in a number of generalization scenarios, including the transfer to the real-world of policies trained in simulation for an image-based robotic manipulation task.},
archivePrefix = {arXiv},
arxivId = {1906.01624},
author = {Irpan, Alex and Rao, Kanishka and Bousmalis, Konstantinos and Harris, Chris and Ibarz, Julian and Levine, Sergey},
eprint = {1906.01624},
file = {:home/telfaralex/Downloads/1906.01624.pdf:pdf},
pages = {1--22},
title = {{Off-Policy Evaluation via Off-Policy Classification}},
url = {http://arxiv.org/abs/1906.01624},
year = {2019}
}
@article{Tennenholtz2019,
abstract = {We introduce Act2Vec, a general framework for learning context-based action representation for Reinforcement Learning. Representing actions in a vector space help reinforcement learning algorithms achieve better performance by grouping similar actions and utilizing relations between different actions. We show how prior knowledge of an environment can be extracted from demonstrations and injected into action vector representations that encode natural compatible behavior. We then use these for augmenting state representations as well as improving function approximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action space domain of StarCraft II.},
archivePrefix = {arXiv},
arxivId = {1902.01119},
author = {Tennenholtz, Guy and Mannor, Shie},
eprint = {1902.01119},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tennenholtz, Mannor - 2019 - The Natural Language of Actions.pdf:pdf},
title = {{The Natural Language of Actions}},
url = {http://arxiv.org/abs/1902.01119},
year = {2019}
}
@article{Zhang2019,
abstract = {We reformulate the option framework as two parallel augmented MDPs. Under this novel formulation, all policy optimization algorithms can be used off the shelf to learn intra-option policies, option termination conditions, and a master policy over options. We apply an actor-critic algorithm on each augmented MDP, yielding the Double Actor-Critic (DAC) architecture. Furthermore, we show that, when state-value functions are used as critics, one critic can be expressed in terms of the other, and hence only one critic is necessary. Our experiments on challenging robot simulation tasks demonstrate that DAC outperforms previous gradient-based option learning algorithms by a large margin and significantly outperforms its hierarchy-free counterparts in a transfer learning setting.},
archivePrefix = {arXiv},
arxivId = {1904.12691},
author = {Zhang, Shangtong and Whiteson, Shimon},
eprint = {1904.12691},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Whiteson - 2019 - DAC The Double Actor-Critic Architecture for Learning Options.pdf:pdf},
title = {{DAC: The Double Actor-Critic Architecture for Learning Options}},
url = {http://arxiv.org/abs/1904.12691},
year = {2019}
}
@article{Russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
eprint = {1707.02038},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russo et al. - 2017 - A Tutorial on Thompson Sampling.pdf:pdf},
pages = {1--96},
title = {{A Tutorial on Thompson Sampling}},
url = {http://arxiv.org/abs/1707.02038},
year = {2017}
}
@article{Chandak2019,
abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
archivePrefix = {arXiv},
arxivId = {1902.00183},
author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
eprint = {1902.00183},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandak et al. - 2019 - Learning Action Representations for Reinforcement Learning.pdf:pdf},
title = {{Learning Action Representations for Reinforcement Learning}},
url = {http://arxiv.org/abs/1902.00183},
year = {2019}
}
@article{Bester2019,
abstract = {Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method, multi-pass deep Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.},
archivePrefix = {arXiv},
arxivId = {1905.04388},
author = {Bester, Craig J. and James, Steven D. and Konidaris, George D.},
eprint = {1905.04388},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bester, James, Konidaris - 2019 - Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces.pdf:pdf},
title = {{Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces}},
url = {http://arxiv.org/abs/1905.04388},
year = {2019}
}
@article{Lewis2008,
author = {Lewis, Catherine},
file = {:home/telfaralex/Downloads/lewis.pdf:pdf},
isbn = {9780511753886},
pages = {15--40},
title = {{Linear Programming : Theory and Applications}},
year = {2008}
}
@misc{,
file = {:home/telfaralex/Downloads/Learning{\_}from{\_}delayed{\_}rewards.pdf:pdf},
title = {{Learning{\_}from{\_}delayed{\_}rewards.pdf}}
}
@article{Sutton2018,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
author = {Sutton, Richard S. and Barto, Andrew G.},
doi = {10.1109/VLSIT.2018.8510680},
file = {:home/telfaralex/Downloads/bookdraft2018jan1.pdf:pdf},
isbn = {9781538642160},
issn = {07431562},
title = {{Reinforcement Learning: An Introduction}},
year = {2018}
}
@article{Agarwal2019,
author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham},
file = {:home/telfaralex/Downloads/rl{\_}main.pdf:pdf},
title = {{Reinforcement Learning : Theory and Algorithms}},
year = {2019}
}
@article{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}
@article{Goldman1995,
abstract = {In this paper, we study the sample complexity of weak learning. That is, we ask how many data must be collected from an unknown distribution in order to extract a small but significant advantage in prediction. We show that it is important to distinguish between those learning algorithms that output deterministic hypotheses and those that output randomized hypotheses. We prove that in the weak learning model, any algorithm using deterministic hypotheses to weakly learn a class of Vapnik-Chervonenkis dimension d(n) requires $\Omega$(√ d(n)) examples. In contrast, when randomized hypotheses are allowed, we show that $\Theta$(1) examples suffice in some cases. We then show that there exists an efficient algorithm using deterministic hypotheses that weakly learns against any distribution on a set of size d(n) with only O(d(n) 2/3 ) examples. Thus for the class of symmetric Boolean functions over n variables, where the strong learning sample complexity is $\Theta$(n), the sample complexity for weak learning using deterministic hypotheses is $\Omega$(√ n) and O(n 2/3 ), and the sample complexity for weak learning using randomized hypotheses is $\Theta$(1). Next we prove the existence of classes for which the distribution-free sample size required to obtain a slight advantage in prediction over random guessing is essentially equal to that required to obtain arbitrary accuracy. Finally, for a class of small circuits, namely all parity functions of subsets of n Boolean variables, we prove a weak learning sample complexity of $\Theta$(n). This bound holds even if the weak learning algorithm is allowed to replace random sampling with membership queries, and the target distribution is uniform on (0, 1) n . {\textcopyright} 1995 Academic Press, Inc.},
author = {Goldman, Sally A. and Kearns, Michel J. and Schapire, Robert E.},
doi = {10.1006/inco.1995.1045},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldman, Kearns, Schapire - 1995 - On the sample complexity of weak learning.pdf:pdf},
issn = {10902651},
journal = {Information and Computation},
number = {2},
pages = {276--287},
title = {{On the sample complexity of weak learning}},
volume = {117},
year = {1995}
}
@article{Ferguson,
author = {Ferguson, Thomas S.},
file = {:home/telfaralex/Downloads/LP (1).pdf:pdf},
title = {{Linear Programming: A Concise Introduction}}
}
@article{,
file = {:home/telfaralex/Downloads/3579dd29ad5b85ac06b49ecd58021821b170.pdf:pdf},
journal = {October},
number = {October 2007},
title = {{Abstraction Using Symmetries in Markov Decision Processes}}
}
@article{Kondor2008,
abstract = {Kondor's Thesis},
author = {Kondor, Risi},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kondor - 2008 - Group theoretical methods in machine learning.pdf:pdf},
number = {May},
title = {{Group theoretical methods in machine learning}},
year = {2008}
}
@article{Jinnai2018,
abstract = {While adding temporally abstract actions, or options, to an agent's action repertoire can often accelerate learning and planning, existing approaches for determining which specific options to add are largely heuristic. We aim to formalize the problem of selecting the optimal set of options for planning, in two contexts: 1) finding the set of {\$}k{\$} options that minimize the number of value-iteration passes until convergence, and 2) computing the smallest set of options so that planning converges in less than a given maximum of {\$}\backslashell{\$} value-iteration passes. We first show that both problems are NP-hard. We then provide a polynomial-time approximation algorithm for computing the optimal options for tasks with bounded return and goal states. We prove that the algorithm has bounded suboptimality for deterministic tasks. Finally, we empirically evaluate its performance against both the optimal options and a representative collection of heuristic approaches in simple grid-based domains including the classic four rooms problem.},
archivePrefix = {arXiv},
arxivId = {1810.07311},
author = {Jinnai, Yuu and Abel, David and Littman, Michael and Konidaris, George},
doi = {10.7748/ns2000.03.14.24.42.c2777},
eprint = {1810.07311},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jinnai et al. - 2018 - Finding Options that Minimize Planning Time.pdf:pdf},
isbn = {1810.07311v1},
issn = {0029-6570},
title = {{Finding Options that Minimize Planning Time}},
url = {http://arxiv.org/abs/1810.07311},
year = {2018}
}
@article{Solway2014,
abstract = {Human behavior has long been recognized to display hierarchical structure: actions fit together into subtasks, which cohere into extended goal-directed activities. Arranging actions hierarchically has well established benefits, allowing behaviors to be represented efficiently by the brain, and allowing solutions to new tasks to be discovered easily. However, these payoffs depend on the particular way in which actions are organized into a hierarchy, the specific way in which tasks are carved up into subtasks. We provide a mathematical account for what makes some hierarchies better than others, an account that allows an optimal hierarchy to be identified for any set of tasks. We then present results from four behavioral experiments, suggesting that human learners spontaneously discover optimal action hierarchies.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Solway, Alec and Diuk, Carlos and C{\'{o}}rdova, Natalia and Yee, Debbie and Barto, Andrew G. and Niv, Yael and Botvinick, Matthew M.},
doi = {10.1371/journal.pcbi.1003779},
eprint = {1703.04730},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Solway et al. - 2014 - Optimal Behavioral Hierarchy.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {8},
pmid = {25122479},
title = {{Optimal Behavioral Hierarchy}},
volume = {10},
year = {2014}
}
@article{Konidaris2018,
abstract = {We consider the problem of constructing abstract representations for planning in high-dimensional, continuous environments. We assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. We first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. We define the specific collection of sets that is necessary and sufficient for planning, and use them to construct a grounded abstract symbolic representation that is provably suitable for deterministic planning. The resulting representation can be expressed in PDDL, a canonical high-level planning domain language; we construct such a representation for the Playroom domain and solve it in milliseconds using an off-the-shelf planner. We then consider probabilistic planning, which we show requires generalizing from sets of states to distributions over states. We identify the specific distributions required for planning, and use them to construct a grounded abstract symbolic representation that correctly estimates the expected reward and probability of success of any plan. In addition, we show that learning the relevant probability distributions corresponds to specific instances of probabilistic density estimation and probabilistic classification. We construct an agent that autonomously learns the correct abstract representation of a computer game domain, and rapidly solves it. Finally, we apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from senso-rimotor data-point clouds, map locations, and joint angles-and then plans using that representation. Together, these results establish a principled link between high-level actions and abstract representations, a concrete theoretical foundation for constructing abstract representations with provable properties, and a practical mechanism for autonomously learning abstract high-level representations.},
author = {Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
doi = {10.1613/jair.5575},
file = {:home/telfaralex/Downloads/11175-Article Text-20703-1-10-20180420.pdf:pdf},
isbn = {9781577356790},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {215--289},
title = {{From skills to symbols: Learning symbolic representations for abstract high-level planning}},
volume = {61},
year = {2018}
}
@article{Abel2018,
abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.pdf:pdf},
isbn = {9781510867963},
issn = {1938-7228},
journal = {35th International Conference on Machine Learning},
pages = {10--19},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18a.html},
volume = {80},
year = {2018}
}
@article{Littman2006,
abstract = {State abstraction (or state aggregation) has been extensively studied in the fields of artificial intel- ligence and operations research. Instead of work- ing in the ground state space, the decision maker usually finds solutions in the abstract state space much faster by treating groups of states as a unit by ignoring irrelevant state information. A num- ber of abstractions have been proposed and studied in the reinforcement-learning and planning litera- tures, and positive and negative results are known. We provide a unified treatment of state abstraction for Markov decision processes. We study five partic- ular abstraction schemes, some of which have been proposed in the past in different forms, and analyze their usability for planning and learning.},
author = {Littman, Lihong Li Thomas J. Walsh Michael L.},
file = {:home/telfaralex/Downloads/P21.pdf:pdf},
title = {{Towards a Unified Theory of State Abstraction for MDPs}},
year = {2006}
}
@article{Konidaris2019,
author = {Konidaris, George},
doi = {10.1016/j.cobeha.2018.11.005},
file = {:home/telfaralex/Downloads/1-s2.0-S2352154618302080-main.pdf:pdf},
issn = {23521546},
journal = {Current Opinion in Behavioral Sciences},
pages = {1--7},
publisher = {Elsevier Ltd},
title = {{On the necessity of abstraction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154618302080},
volume = {29},
year = {2019}
}
@article{Abel2017,
abstract = {The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.},
archivePrefix = {arXiv},
arxivId = {1701.04113},
author = {Abel, David and Hershkowitz, D. Ellis and Littman, Michael L.},
doi = {10.1016/0022-0728(93)03060-3},
eprint = {1701.04113},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abel, Hershkowitz, Littman - 2017 - Near Optimal Behavior via Approximate State Abstraction.pdf:pdf},
isbn = {9781510829008},
issn = {15726657},
pages = {1--18},
title = {{Near Optimal Behavior via Approximate State Abstraction}},
url = {http://arxiv.org/abs/1701.04113},
volume = {48},
year = {2017}
}
@article{Fruit2017,
abstract = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be $\backslash$textit{\{}provably{\}} much smaller than the regret suffered when learning with primitive actions.},
archivePrefix = {arXiv},
arxivId = {1703.08667},
author = {Fruit, Ronan and Lazaric, Alessandro},
eprint = {1703.08667},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fruit, Lazaric - 2017 - Exploration--Exploitation in MDPs with Options.pdf:pdf},
title = {{Exploration--Exploitation in MDPs with Options}},
url = {http://arxiv.org/abs/1703.08667},
year = {2017}
}
@article{Pham2019,
author = {Pham, Vu and Tunyasuvunakool, Saran and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Wayne, Greg},
file = {:home/telfaralex/Downloads/c64a5a1a7586ab5357231dd332bb03f3b34fc184.pdf:pdf},
number = {2002},
pages = {1--19},
title = {{HIERARCHICAL VISUOMOTOR CONTROL OF HUMANOIDS}},
year = {2019}
}
@article{Mankowitz2018,
abstract = {Robust reinforcement learning aims to produce policies that have strong guarantees even in the face of environments/transition models whose parameters have strong uncertainty. Existing work uses value-based methods and the usual primitive action setting. In this paper, we propose robust methods for learning temporally abstract actions, in the framework of options. We present a Robust Options Policy Iteration (ROPI) algorithm with convergence guarantees, which learns options that are robust to model uncertainty. We utilize ROPI to learn robust options with the Robust Options Deep Q Network (RO-DQN) that solves multiple tasks and mitigates model misspecification due to model uncertainty. We present experimental results which suggest that policy iteration with linear features may have an inherent form of robustness when using coarse feature representations. In addition, we present experimental results which demonstrate that robustness helps policy iteration implemented on top of deep neural networks to generalize over a much broader range of dynamics than non-robust policy iteration.},
archivePrefix = {arXiv},
arxivId = {1802.03236},
author = {Mankowitz, Daniel J. and Mann, Timothy A. and Bacon, Pierre-Luc and Precup, Doina and Mannor, Shie},
doi = {10.1021/jz401069f},
eprint = {1802.03236},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mankowitz et al. - 2018 - Learning Robust Options.pdf:pdf},
isbn = {9781577358008},
issn = {1948-7185},
keywords = {Reasoning Under Uncertainty Track},
number = {1},
pages = {6409--6416},
title = {{Learning Robust Options}},
url = {http://arxiv.org/abs/1802.03236},
year = {2018}
}
@article{RichardS.SuttonaDoinaPrecupb1998,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Richard S. Sutton a,∗, Doina Precup b}, Satinder Singh a},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richard S. Sutton a,∗, Doina Precup b - 1998 - Between MDPs and semi-MDPs.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {Option},
number = {1999},
pages = {1--30},
pmid = {25246403},
title = {{Between MDPs and semi-MDPs}},
url = {http://www-anw.cs.umass.edu/{~}barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf{\%}0Ahttp://ebooks.cambridge.org/ref/id/CBO9781107415324A009},
volume = {1},
year = {1998}
}
@article{Bacon2016,
abstract = {We show that the Bellman operator underlying the options framework leads to a matrix splitting, an approach traditionally used to speed up convergence of iterative solvers for large linear systems of equations. Based on standard comparison theorems for matrix splittings, we then show how the asymptotic rate of convergence varies as a function of the inherent timescales of the options. This new perspective highlights a trade-off between asymptotic performance and the cost of computation associated with building a good set of options.},
archivePrefix = {arXiv},
arxivId = {1612.00916},
author = {Bacon, Pierre-Luc and Precup, Doina},
eprint = {1612.00916},
file = {:home/telfaralex/Downloads/1612.00916 (1).pdf:pdf},
pages = {1--6},
title = {{A Matrix Splitting Perspective on Planning with Options}},
url = {http://arxiv.org/abs/1612.00916},
year = {2016}
}
@article{Bacon2018,
author = {Bacon, Pierre-Luc},
doi = {10.1145/1690388.1690416},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacon - 2018 - Temporal Representation Learning.pdf:pdf},
number = {June},
title = {{Temporal Representation Learning}},
year = {2018}
}
@article{Fruit2017a,
author = {Fruit, Ronan and Lazaric, Alessandro and Pirotta, Matteo and Brunskill, Emma},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fruit et al. - 2017 - Empirical Evaluation of Optimism with Options.pdf:pdf},
journal = {NIPS Hierarchical Reinforcement Learning Workshop},
title = {{Empirical Evaluation of Optimism with Options}},
url = {https://drive.google.com/file/d/1KZIXv43JRq9pxgLKD1mEuoJOSHqVBkIE/view},
year = {2017}
}
@article{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
doi = {10.1371/journal.pntd.0000787},
eprint = {1711.03817},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harutyunyan et al. - 2017 - Learning with Options that Terminate Off-Policy.pdf:pdf},
isbn = {1935-2735 (Electronic)$\backslash$r1935-2727 (Linking)},
issn = {1935-2735},
pmid = {20706628},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@article{Riemer2018,
abstract = {Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 1999). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.},
archivePrefix = {arXiv},
arxivId = {1810.11583},
author = {Riemer, Matthew and Liu, Miao and Tesauro, Gerald},
doi = {arXiv:1810.11583v1},
eprint = {1810.11583},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riemer, Liu, Tesauro - 2018 - Learning Abstract Options.pdf:pdf},
isbn = {1810.11583v3},
number = {NeurIPS},
pages = {1--11},
title = {{Learning Abstract Options}},
url = {http://arxiv.org/abs/1810.11583},
year = {2018}
}
@article{Achiam2018,
abstract = {We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1807.10299},
author = {Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
eprint = {1807.10299},
file = {:home/telfaralex/Downloads/1807.10299.pdf:pdf},
pages = {1--29},
title = {{Variational Option Discovery Algorithms}},
url = {http://arxiv.org/abs/1807.10299},
year = {2018}
}
@article{Bacon2016a,
abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup {\&} Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
archivePrefix = {arXiv},
arxivId = {1609.05140},
author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
doi = {10.1109/ICABME.2017.8167529},
eprint = {1609.05140},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacon, Harb, Precup - 2016 - The Option-Critic Architecture.pdf:pdf},
issn = {0193-1849},
pmid = {10913042},
title = {{The Option-Critic Architecture}},
url = {http://arxiv.org/abs/1609.05140},
year = {2016}
}
@article{Todorov2009,
abstract = {Optimal choice of actions is a fundamental problem relevant to fields as diverse as neuroscience, psychology, economics, computer science, and control engineering. Despite this broad relevance the abstract setting is similar: we have an agent choosing actions over time, an uncertain dynamical system whose state is affected by those actions, and a performance criterion that the agent seeks to optimize. Solving problems of this kind remains hard, in part, because of overly generic formulations. Here, we propose a more structured formulation that greatly simplifies the construction of optimal control laws in both discrete and continuous domains. An exhaustive search over actions is avoided and the problem becomes linear. This yields algorithms that outperform Dynamic Programming and Reinforcement Learning, and thereby solve traditional problems more efficiently. Our framework also enables computations that were not possible before: composing optimal control laws by mixing primitives, applying deterministic methods to stochastic systems, quantifying the benefits of error tolerance, and inferring goals from behavioral data via convex optimization. Development of a general class of easily solvable problems tends to accelerate progress--as linear systems theory has done, for example. Our framework may have similar impact in fields where optimal choice of actions is relevant.},
author = {Todorov, Emanuel},
doi = {10.1073/pnas.0710743106},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Todorov - 2009 - Efficient computation of optimal actions.pdf:pdf},
isbn = {0710743106},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {28},
pages = {11478--11483},
pmid = {19574462},
title = {{Efficient computation of optimal actions}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.0710743106},
volume = {106},
year = {2009}
}
@article{Betancourt2018,
abstract = {Accelerated gradient methods have had significant impact in machine learning—in particular the theoretical side of machine learning— due to their ability to achieve oracle lower bounds. But their heuris-tic construction has hindered their full integration into the practi-cal machine-learning algorithmic toolbox, and has limited their scope. In this paper we build on recent work which casts acceleration as a phenomenon best explained in continuous time, and we augment that picture by providing a systematic methodology for converting continuous-time dynamics into discrete-time algorithms while retain-ing oracle rates. Our framework is based on ideas from Hamiltonian dynamical systems and symplectic integration. These ideas have had major impact in many areas in applied mathematics, but have not yet been seen to have a relationship with optimization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.03653v2},
author = {Betancourt, Michael and Jordan, Michael I and Wilson, Ashia C},
eprint = {arXiv:1802.03653v2},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Betancourt, Jordan, Wilson - 2018 - On Symplectic Optimization.pdf:pdf},
title = {{On Symplectic Optimization}},
url = {https://arxiv.org/pdf/1802.03653.pdf},
year = {2018}
}
