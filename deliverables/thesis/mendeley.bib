@article{Caselles-Dupre2019,
abstract = {Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on fixed data samples. Agents should interact with the environment to discover its symmetries. Our experiments can be reproduced on Colab: http://bit.do/eKpqv},
archivePrefix = {arXiv},
arxivId = {1904.00243},
author = {Caselles-Dupr{\'{e}}, Hugo and Garcia-Ortiz, Michael and Filliat, David},
eprint = {1904.00243},
file = {:home/telfaralex/Downloads/1904.00243.pdf:pdf},
number = {NeurIPS},
pages = {1--16},
title = {{Symmetry-Based Disentangled Representation Learning requires Interaction with Environments}},
url = {http://arxiv.org/abs/1904.00243},
year = {2019}
}
@article{Scholkopf2001,
abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
author = {Sch{\"{o}}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sch{\"{o}}lkopf, Herbrich, Smola - 2001 - A generalized representer theorem.pdf:pdf},
isbn = {9783540423430},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {416--426},
title = {{A generalized representer theorem}},
volume = {2111},
year = {2001}
}
@article{Kim2018,
abstract = {Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI .},
archivePrefix = {arXiv},
arxivId = {1810.01176},
author = {Kim, Hyoungseok and Kim, Jaekyeom and Jeong, Yeonwoo and Levine, Sergey and Song, Hyun Oh},
eprint = {1810.01176},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - EMI Exploration with Mutual Information.pdf:pdf},
title = {{EMI: Exploration with Mutual Information}},
url = {http://arxiv.org/abs/1810.01176},
year = {2018}
}
@article{Sutton1999,
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges in AI. The ways how to address these challenges within the mathematical framework of reinforcement learning and Markov decision processes (MDP) are discussed. A set options defined over an MDP constitutes a semi-MDP (SMDP), and the theory of SMDPs provides the foundation for the theory of options.},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1016/S0004-3702(99)00052-1},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Precup, Singh - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
year = {1999}
}
@article{Lee2019,
abstract = {To solve tasks with sparse rewards, reinforcement learning algorithms must be equipped with suitable exploration techniques. However, it is unclear what underlying objective is being optimized by existing exploration algorithms, or how they can be altered to incorporate prior knowledge about the task. Most importantly, it is difficult to use exploration experience from one task to acquire exploration strategies for another task. We address these shortcomings by learning a single exploration policy that can quickly solve a suite of downstream tasks in a multi-task setting, amortizing the cost of learning to explore. We recast exploration as a problem of State Marginal Matching (SMM): we learn a mixture of policies for which the state marginal distribution matches a given target state distribution, which can incorporate prior knowledge about the task. Without any prior knowledge, the SMM objective reduces to maximizing the marginal state entropy. We optimize the objective by reducing it to a two-player, zero-sum game, where we iteratively fit a state density model and then update the policy to visit states with low density under this model. While many previous algorithms for exploration employ a similar procedure, they omit a crucial historical averaging step, without which the iterative procedure does not converge to a Nash equilibria. To parallelize exploration, we extend our algorithm to use mixtures of policies, wherein we discover connections between SMM and previously-proposed skill learning methods based on mutual information. On complex navigation and manipulation tasks, we demonstrate that our algorithm explores faster and adapts more quickly to new tasks.},
archivePrefix = {arXiv},
arxivId = {1906.05274},
author = {Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
eprint = {1906.05274},
file = {:home/telfaralex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2019 - Efficient Exploration via State Marginal Matching.pdf:pdf},
month = {jun},
title = {{Efficient Exploration via State Marginal Matching}},
url = {http://arxiv.org/abs/1906.05274},
year = {2019}
}
@article{Mazumdar2019,
abstract = {We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in {\$}N{\$}-player general-sum linear quadratic games. In such games the state and action spaces are continuous and the unique global Nash equilibrium can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi-agent policy gradient. We first prove that the only critical point of the gradient dynamics in these games is the unique global Nash equilibrium. We then give sufficient conditions under which policy gradient will avoid the Nash equilibrium, and generate a large number of general-sum linear quadratic games that satisfy these conditions. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.},
archivePrefix = {arXiv},
arxivId = {1907.03712},
author = {Mazumdar, Eric and Ratliff, Lillian J. and Jordan, Michael I. and Sastry, S. Shankar},
eprint = {1907.03712},
file = {:home/telfaralex/Downloads/1907.03712.pdf:pdf},
pages = {1--15},
title = {{Policy-Gradient Algorithms Have No Guarantees of Convergence in Continuous Action and State Multi-Agent Settings}},
url = {http://arxiv.org/abs/1907.03712},
year = {2019}
}
@article{Kroemer,
abstract = {A key question in intelligent robotics is how to create robots capable of directly interacting with the world around them to achieve their goals. The last decade has therefore seen substantial growth in research on the problem of robot manipulation. We aim to describe a representative subset of that research on using machine learning for manipulation, describing a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework, and highlighting the many remaining research opportunities and challenges.},
archivePrefix = {arXiv},
arxivId = {arXiv:1907.03146v2},
author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
eprint = {arXiv:1907.03146v2},
file = {:home/telfaralex/Downloads/1907.03146.pdf:pdf},
keywords = {Learning,Manipulation,Review,Robots},
title = {{A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms}}
}
@article{Luo2018a,
abstract = {Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires $\backslash$textit{\{}no explicit{\}} uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves state-of-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {1807.03858},
author = {Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
eprint = {1807.03858},
file = {:home/telfaralex/Downloads/1807.03858.pdf:pdf},
pages = {1--27},
title = {{Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees}},
url = {http://arxiv.org/abs/1807.03858},
year = {2018}
}
@article{Dann2018,
abstract = {The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications, such as healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms achieves regret and PAC bounds that are minimax up to lower-order terms.},
archivePrefix = {arXiv},
arxivId = {1811.03056},
author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
eprint = {1811.03056},
file = {:home/telfaralex/Downloads/1811.03056.pdf:pdf},
title = {{Policy Certificates: Towards Accountable Reinforcement Learning}},
url = {http://arxiv.org/abs/1811.03056},
year = {2018}
}
@article{Krauth2019,
abstract = {We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within {\$}\backslashvarepsilon{\$} of the optimal LQR controller, each step of policy evaluation requires at most {\$}(n+d){\^{}}3/\backslashvarepsilon{\^{}}2{\$} samples, where {\$}n{\$} is the dimension of the state vector and {\$}d{\$} is the dimension of the input vector. On the other hand, only {\$}\backslashlog(1/\backslashvarepsilon){\$} policy improvement steps suffice, resulting in an overall sample complexity of {\$}(n+d){\^{}}3 \backslashvarepsilon{\^{}}{\{}-2{\}} \backslashlog(1/\backslashvarepsilon){\$}. We furthermore build on our analysis and construct a simple adaptive procedure based on {\$}\backslashvarepsilon{\$}-greedy exploration which relies on approximate PI as a sub-routine and obtains {\$}T{\^{}}{\{}2/3{\}}{\$} regret, improving upon a recent result of Abbasi-Yadkori et al.},
archivePrefix = {arXiv},
arxivId = {1905.12842},
author = {Krauth, Karl and Tu, Stephen and Recht, Benjamin},
eprint = {1905.12842},
file = {:home/telfaralex/Downloads/1905.12842.pdf:pdf},
pages = {1--48},
title = {{Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator}},
url = {http://arxiv.org/abs/1905.12842},
year = {2019}
}
@article{Liu2019a,
abstract = {Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which separates theory from practice. In this paper, we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity, where the gradient and iterate are instantiated by neural networks. In particular, the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.},
archivePrefix = {arXiv},
arxivId = {1906.10306},
author = {Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
eprint = {1906.10306},
file = {:home/telfaralex/Downloads/1906.10306.pdf:pdf},
pages = {1--38},
title = {{Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy}},
url = {http://arxiv.org/abs/1906.10306},
year = {2019}
}
@article{Achiam2019,
abstract = {Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.},
archivePrefix = {arXiv},
arxivId = {1903.08894},
author = {Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
eprint = {1903.08894},
file = {:home/telfaralex/Downloads/1903.08894.pdf:pdf},
title = {{Towards Characterizing Divergence in Deep Q-Learning}},
url = {http://arxiv.org/abs/1903.08894},
year = {2019}
}
@article{Lattimore2018,
abstract = {After nearly two years since starting to write the blog we have at last completed a first draft of the book, which is to be published by Cambridge University Press. The book is available for free as a PDF and will remain so after publication. We're grateful to Cambridge for allowing this. Without further ado, here is the link. Although we still have a few things we want to do, the manuscript is sufficiently polished to be useful. Of course we would greatly appreciate any comments you might have, including typos, errors in the proofs, missing references, confusing explanations or anything else you might notice. We will periodically update the book, so it would be helpful if you could quote the revision number on the cover when sending us your comments (banditalgs@gmail.com). The manuscript includes a lot of material not in the blog. The last seven chapters are all new, covering combinatorial (semi-)bandits, non-stationary bandits, ranking, pure exploration, Bayesian methods, Thompson sampling, partial monitoring and an introduction to learning in Markov decision processes. Those chapters that are based on blog posts have been cleaned up and often we have added significant depth. There is a lot of literature that we have not covered. Some of these missing topics are discussed in extreme brevity in the introduction to Part VII. It really is amazing how large the bandit literature has become and we're sorry not to have found space for everything. The book includes around 250 exercises, some of which have solutions. On average the exercises have been proofread less carefully than the rest of the book, so some caution is advised. The solutions to selected exercises are available here. Finally, we're very thankful for all the feedback already received, both on the blog and early drafts of the book.},
author = {Lattimore, Tor and Szepesvari, Csaba},
file = {:home/telfaralex/Downloads/book.pdf:pdf},
journal = {Cambridge University Press},
pages = {542},
title = {{Bandit Algorithms}},
year = {2018}
}
@article{Slivkins2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07272v2},
author = {Slivkins, Aleksandrs},
eprint = {arXiv:1904.07272v2},
file = {:home/telfaralex/Downloads/1904.07272.pdf:pdf},
number = {January 2017},
title = {{Introduction to Multi-Armed Bandits}},
year = {2019}
}
@article{Irpan2019,
abstract = {In this work, we consider the problem of model selection for deep reinforcement learning (RL) in real-world environments. Typically, the performance of deep RL algorithms is evaluated via on-policy interactions with the target environment. However, comparing models in a real-world environment for the purposes of early stopping or hyperparameter tuning is costly and often practically infeasible. This leads us to examine off-policy policy evaluation (OPE) in such settings. We focus on OPE for value-based methods, which are of particular interest in deep RL, with applications like robotics, where off-policy algorithms based on Q-function estimation can often attain better sample complexity than direct policy optimization. Existing OPE metrics either rely on a model of the environment, or the use of importance sampling (IS) to correct for the data being off-policy. However, for high-dimensional observations, such as images, models of the environment can be difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper, we focus on the specific case of MDPs with continuous action spaces and sparse binary rewards, which is representative of many important real-world applications. We propose an alternative metric that relies on neither models nor IS, by framing OPE as a positive-unlabeled (PU) classification problem with the Q-function as the decision function. We experimentally show that this metric outperforms baselines on a number of tasks. Most importantly, it can reliably predict the relative performance of different policies in a number of generalization scenarios, including the transfer to the real-world of policies trained in simulation for an image-based robotic manipulation task.},
archivePrefix = {arXiv},
arxivId = {1906.01624},
author = {Irpan, Alex and Rao, Kanishka and Bousmalis, Konstantinos and Harris, Chris and Ibarz, Julian and Levine, Sergey},
eprint = {1906.01624},
file = {:home/telfaralex/Downloads/1906.01624.pdf:pdf},
pages = {1--22},
title = {{Off-Policy Evaluation via Off-Policy Classification}},
url = {http://arxiv.org/abs/1906.01624},
year = {2019}
}
@article{Tennenholtz2019,
abstract = {We introduce Act2Vec, a general framework for learning context-based action representation for Reinforcement Learning. Representing actions in a vector space help reinforcement learning algorithms achieve better performance by grouping similar actions and utilizing relations between different actions. We show how prior knowledge of an environment can be extracted from demonstrations and injected into action vector representations that encode natural compatible behavior. We then use these for augmenting state representations as well as improving function approximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action space domain of StarCraft II.},
archivePrefix = {arXiv},
arxivId = {1902.01119},
author = {Tennenholtz, Guy and Mannor, Shie},
eprint = {1902.01119},
file = {:home/telfaralex/Downloads/1902.01119.pdf:pdf},
title = {{The Natural Language of Actions}},
url = {http://arxiv.org/abs/1902.01119},
year = {2019}
}
@article{Zhang2019,
abstract = {We reformulate the option framework as two parallel augmented MDPs. Under this novel formulation, all policy optimization algorithms can be used off the shelf to learn intra-option policies, option termination conditions, and a master policy over options. We apply an actor-critic algorithm on each augmented MDP, yielding the Double Actor-Critic (DAC) architecture. Furthermore, we show that, when state-value functions are used as critics, one critic can be expressed in terms of the other, and hence only one critic is necessary. Our experiments on challenging robot simulation tasks demonstrate that DAC outperforms previous gradient-based option learning algorithms by a large margin and significantly outperforms its hierarchy-free counterparts in a transfer learning setting.},
archivePrefix = {arXiv},
arxivId = {1904.12691},
author = {Zhang, Shangtong and Whiteson, Shimon},
eprint = {1904.12691},
file = {:home/telfaralex/Downloads/1904.12691.pdf:pdf},
title = {{DAC: The Double Actor-Critic Architecture for Learning Options}},
url = {http://arxiv.org/abs/1904.12691},
year = {2019}
}
@article{Russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
eprint = {1707.02038},
file = {:home/telfaralex/Downloads/TS{\_}Tutorial (1).pdf:pdf},
pages = {1--96},
title = {{A Tutorial on Thompson Sampling}},
url = {http://arxiv.org/abs/1707.02038},
year = {2017}
}
@article{Chandak2019,
abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
archivePrefix = {arXiv},
arxivId = {1902.00183},
author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
eprint = {1902.00183},
file = {:home/telfaralex/Downloads/1902.00183.pdf:pdf},
title = {{Learning Action Representations for Reinforcement Learning}},
url = {http://arxiv.org/abs/1902.00183},
year = {2019}
}
@article{Bester2019,
abstract = {Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method, multi-pass deep Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.},
archivePrefix = {arXiv},
arxivId = {1905.04388},
author = {Bester, Craig J. and James, Steven D. and Konidaris, George D.},
eprint = {1905.04388},
file = {:home/telfaralex/Downloads/1905.04388.pdf:pdf},
title = {{Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces}},
url = {http://arxiv.org/abs/1905.04388},
year = {2019}
}
@article{Lewis2008,
author = {Lewis, Catherine},
file = {:home/telfaralex/Downloads/lewis.pdf:pdf},
isbn = {9780511753886},
pages = {15--40},
title = {{Linear Programming : Theory and Applications}},
year = {2008}
}
@misc{,
file = {:home/telfaralex/Downloads/Learning{\_}from{\_}delayed{\_}rewards.pdf:pdf},
title = {{Learning{\_}from{\_}delayed{\_}rewards.pdf}}
}
@article{Avasarala2018,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
author = {Avasarala, Naga Sruti and Donadio, G. L. and Witters, T. and Opsomer, K. and Govoreanu, B. and Fantini, A. and Clima, S. and Oh, H. and Kundu, S. and Devulder, W. and {Van Der Veen}, M. H. and {Van Houdt}, J. and Heyns, M. and Goux, L. and Kar, G. S.},
doi = {10.1109/VLSIT.2018.8510680},
file = {:home/telfaralex/Downloads/bookdraft2018jan1.pdf:pdf},
isbn = {9781538642160},
issn = {07431562},
journal = {Digest of Technical Papers - Symposium on VLSI Technology},
pages = {209--210},
title = {{Reinforcement Learning: An Introduction Final Draft}},
volume = {2018-June},
year = {2018}
}
@article{Agarwal2019,
author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham},
file = {:home/telfaralex/Downloads/rl{\_}main.pdf:pdf},
title = {{Reinforcement Learning : Theory and Algorithms}},
year = {2019}
}
@article{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:home/telfaralex/Downloads/RLAlgsInMDPs (1).pdf:pdf},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}
@article{Goldman1995,
abstract = {In this paper, we study the sample complexity of weak learning. That is, we ask how many data must be collected from an unknown distribution in order to extract a small but significant advantage in prediction. We show that it is important to distinguish between those learning algorithms that output deterministic hypotheses and those that output randomized hypotheses. We prove that in the weak learning model, any algorithm using deterministic hypotheses to weakly learn a class of Vapnik-Chervonenkis dimension d(n) requires $\Omega$(√ d(n)) examples. In contrast, when randomized hypotheses are allowed, we show that $\Theta$(1) examples suffice in some cases. We then show that there exists an efficient algorithm using deterministic hypotheses that weakly learns against any distribution on a set of size d(n) with only O(d(n) 2/3 ) examples. Thus for the class of symmetric Boolean functions over n variables, where the strong learning sample complexity is $\Theta$(n), the sample complexity for weak learning using deterministic hypotheses is $\Omega$(√ n) and O(n 2/3 ), and the sample complexity for weak learning using randomized hypotheses is $\Theta$(1). Next we prove the existence of classes for which the distribution-free sample size required to obtain a slight advantage in prediction over random guessing is essentially equal to that required to obtain arbitrary accuracy. Finally, for a class of small circuits, namely all parity functions of subsets of n Boolean variables, we prove a weak learning sample complexity of $\Theta$(n). This bound holds even if the weak learning algorithm is allowed to replace random sampling with membership queries, and the target distribution is uniform on (0, 1) n . {\textcopyright} 1995 Academic Press, Inc.},
author = {Goldman, Sally A. and Kearns, Michel J. and Schapire, Robert E.},
doi = {10.1006/inco.1995.1045},
file = {:home/telfaralex/Downloads/sham{\_}thesis (1).pdf:pdf},
issn = {10902651},
journal = {Information and Computation},
number = {2},
pages = {276--287},
title = {{On the sample complexity of weak learning}},
volume = {117},
year = {1995}
}
@article{Ferguson,
author = {Ferguson, Thomas S.},
file = {:home/telfaralex/Downloads/LP (1).pdf:pdf},
title = {{Linear Programming: A Concise Introduction}}
}
@article{,
file = {:home/telfaralex/Downloads/3579dd29ad5b85ac06b49ecd58021821b170.pdf:pdf},
journal = {October},
number = {October 2007},
title = {{Abstraction Using Symmetries in Markov Decision Processes}}
}
@article{Kondor2008,
abstract = {Kondor's Thesis},
author = {Kondor, Risi},
file = {:home/telfaralex/Downloads/KondorThesis (1).pdf:pdf},
number = {May},
title = {{Group theoretical methods in machine learning}},
year = {2008}
}
@article{Jinnai2018,
abstract = {While adding temporally abstract actions, or options, to an agent's action repertoire can often accelerate learning and planning, existing approaches for determining which specific options to add are largely heuristic. We aim to formalize the problem of selecting the optimal set of options for planning, in two contexts: 1) finding the set of {\$}k{\$} options that minimize the number of value-iteration passes until convergence, and 2) computing the smallest set of options so that planning converges in less than a given maximum of {\$}\backslashell{\$} value-iteration passes. We first show that both problems are NP-hard. We then provide a polynomial-time approximation algorithm for computing the optimal options for tasks with bounded return and goal states. We prove that the algorithm has bounded suboptimality for deterministic tasks. Finally, we empirically evaluate its performance against both the optimal options and a representative collection of heuristic approaches in simple grid-based domains including the classic four rooms problem.},
archivePrefix = {arXiv},
arxivId = {1810.07311},
author = {Jinnai, Yuu and Abel, David and Littman, Michael and Konidaris, George},
doi = {10.7748/ns2000.03.14.24.42.c2777},
eprint = {1810.07311},
file = {:home/telfaralex/Downloads/1810.07311.pdf:pdf},
isbn = {1810.07311v1},
issn = {0029-6570},
title = {{Finding Options that Minimize Planning Time}},
url = {http://arxiv.org/abs/1810.07311},
year = {2018}
}
@article{Solway2014,
abstract = {Human behavior has long been recognized to display hierarchical structure: actions fit together into subtasks, which cohere into extended goal-directed activities. Arranging actions hierarchically has well established benefits, allowing behaviors to be represented efficiently by the brain, and allowing solutions to new tasks to be discovered easily. However, these payoffs depend on the particular way in which actions are organized into a hierarchy, the specific way in which tasks are carved up into subtasks. We provide a mathematical account for what makes some hierarchies better than others, an account that allows an optimal hierarchy to be identified for any set of tasks. We then present results from four behavioral experiments, suggesting that human learners spontaneously discover optimal action hierarchies.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Solway, Alec and Diuk, Carlos and C{\'{o}}rdova, Natalia and Yee, Debbie and Barto, Andrew G. and Niv, Yael and Botvinick, Matthew M.},
doi = {10.1371/journal.pcbi.1003779},
eprint = {1703.04730},
file = {:home/telfaralex/Downloads/journal.pcbi.1003779.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {8},
pmid = {25122479},
title = {{Optimal Behavioral Hierarchy}},
volume = {10},
year = {2014}
}
@article{Konidaris2018,
abstract = {We consider the problem of constructing abstract representations for planning in high-dimensional, continuous environments. We assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. We first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. We define the specific collection of sets that is necessary and sufficient for planning, and use them to construct a grounded abstract symbolic representation that is provably suitable for deterministic planning. The resulting representation can be expressed in PDDL, a canonical high-level planning domain language; we construct such a representation for the Playroom domain and solve it in milliseconds using an off-the-shelf planner. We then consider probabilistic planning, which we show requires generalizing from sets of states to distributions over states. We identify the specific distributions required for planning, and use them to construct a grounded abstract symbolic representation that correctly estimates the expected reward and probability of success of any plan. In addition, we show that learning the relevant probability distributions corresponds to specific instances of probabilistic density estimation and probabilistic classification. We construct an agent that autonomously learns the correct abstract representation of a computer game domain, and rapidly solves it. Finally, we apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from senso-rimotor data-point clouds, map locations, and joint angles-and then plans using that representation. Together, these results establish a principled link between high-level actions and abstract representations, a concrete theoretical foundation for constructing abstract representations with provable properties, and a practical mechanism for autonomously learning abstract high-level representations.},
author = {Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
doi = {10.1613/jair.5575},
file = {:home/telfaralex/Downloads/11175-Article Text-20703-1-10-20180420.pdf:pdf},
isbn = {9781577356790},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {215--289},
title = {{From skills to symbols: Learning symbolic representations for abstract high-level planning}},
volume = {61},
year = {2018}
}
@article{Abel2018,
abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
file = {:home/telfaralex/Downloads/abel18a.pdf:pdf},
isbn = {9781510867963},
issn = {1938-7228},
journal = {35th International Conference on Machine Learning},
pages = {10--19},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18a.html},
volume = {80},
year = {2018}
}
@article{Littman2006,
abstract = {State abstraction (or state aggregation) has been extensively studied in the fields of artificial intel- ligence and operations research. Instead of work- ing in the ground state space, the decision maker usually finds solutions in the abstract state space much faster by treating groups of states as a unit by ignoring irrelevant state information. A num- ber of abstractions have been proposed and studied in the reinforcement-learning and planning litera- tures, and positive and negative results are known. We provide a unified treatment of state abstraction for Markov decision processes. We study five partic- ular abstraction schemes, some of which have been proposed in the past in different forms, and analyze their usability for planning and learning.},
author = {Littman, Lihong Li Thomas J. Walsh Michael L.},
file = {:home/telfaralex/Downloads/P21.pdf:pdf},
title = {{Towards a Unified Theory of State Abstraction for MDPs}},
year = {2006}
}
@article{Konidaris2019,
author = {Konidaris, George},
doi = {10.1016/j.cobeha.2018.11.005},
file = {:home/telfaralex/Downloads/1-s2.0-S2352154618302080-main.pdf:pdf},
issn = {23521546},
journal = {Current Opinion in Behavioral Sciences},
pages = {1--7},
publisher = {Elsevier Ltd},
title = {{On the necessity of abstraction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154618302080},
volume = {29},
year = {2019}
}
@article{Abel2017,
abstract = {The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.},
archivePrefix = {arXiv},
arxivId = {1701.04113},
author = {Abel, David and Hershkowitz, D. Ellis and Littman, Michael L.},
doi = {10.1016/0022-0728(93)03060-3},
eprint = {1701.04113},
file = {:home/telfaralex/Downloads/1701.04113.pdf:pdf},
isbn = {9781510829008},
issn = {15726657},
pages = {1--18},
title = {{Near Optimal Behavior via Approximate State Abstraction}},
url = {http://arxiv.org/abs/1701.04113},
volume = {48},
year = {2017}
}
@article{Fruit2017,
abstract = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be $\backslash$textit{\{}provably{\}} much smaller than the regret suffered when learning with primitive actions.},
archivePrefix = {arXiv},
arxivId = {1703.08667},
author = {Fruit, Ronan and Lazaric, Alessandro},
eprint = {1703.08667},
file = {:home/telfaralex/Downloads/mdps-w-options.pdf:pdf},
title = {{Exploration--Exploitation in MDPs with Options}},
url = {http://arxiv.org/abs/1703.08667},
year = {2017}
}
@article{Pham2019,
author = {Pham, Vu and Tunyasuvunakool, Saran and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Wayne, Greg},
file = {:home/telfaralex/Downloads/c64a5a1a7586ab5357231dd332bb03f3b34fc184.pdf:pdf},
number = {2002},
pages = {1--19},
title = {{HIERARCHICAL VISUOMOTOR CONTROL OF HUMANOIDS}},
year = {2019}
}
@article{Mankowitz2018,
abstract = {Robust reinforcement learning aims to produce policies that have strong guarantees even in the face of environments/transition models whose parameters have strong uncertainty. Existing work uses value-based methods and the usual primitive action setting. In this paper, we propose robust methods for learning temporally abstract actions, in the framework of options. We present a Robust Options Policy Iteration (ROPI) algorithm with convergence guarantees, which learns options that are robust to model uncertainty. We utilize ROPI to learn robust options with the Robust Options Deep Q Network (RO-DQN) that solves multiple tasks and mitigates model misspecification due to model uncertainty. We present experimental results which suggest that policy iteration with linear features may have an inherent form of robustness when using coarse feature representations. In addition, we present experimental results which demonstrate that robustness helps policy iteration implemented on top of deep neural networks to generalize over a much broader range of dynamics than non-robust policy iteration.},
archivePrefix = {arXiv},
arxivId = {1802.03236},
author = {Mankowitz, Daniel J. and Mann, Timothy A. and Bacon, Pierre-Luc and Precup, Doina and Mannor, Shie},
doi = {10.1021/jz401069f},
eprint = {1802.03236},
file = {:home/telfaralex/Downloads/16825-76978-1-PB.pdf:pdf},
isbn = {9781577358008},
issn = {1948-7185},
keywords = {Reasoning Under Uncertainty Track},
number = {1},
pages = {6409--6416},
title = {{Learning Robust Options}},
url = {http://arxiv.org/abs/1802.03236},
year = {2018}
}
@article{RichardS.SuttonaDoinaPrecupb1998,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Richard S. Sutton a,∗, Doina Precup b}, Satinder Singh a},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/telfaralex/Downloads/Sutton-Precup-Singh-AIJ99.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {Option},
number = {1999},
pages = {1--30},
pmid = {25246403},
title = {{Between MDPs and semi-MDPs}},
url = {http://www-anw.cs.umass.edu/{~}barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf{\%}0Ahttp://ebooks.cambridge.org/ref/id/CBO9781107415324A009},
volume = {1},
year = {1998}
}
@article{Bacon2018,
author = {Bacon, Pierre-Luc},
doi = {10.1145/1690388.1690416},
file = {:home/telfaralex/Downloads/bacon2018thesis.pdf:pdf},
number = {June},
title = {{Temporal Representation Learning}},
year = {2018}
}
@article{Bacon2016,
abstract = {We show that the Bellman operator underlying the options framework leads to a matrix splitting, an approach traditionally used to speed up convergence of iterative solvers for large linear systems of equations. Based on standard comparison theorems for matrix splittings, we then show how the asymptotic rate of convergence varies as a function of the inherent timescales of the options. This new perspective highlights a trade-off between asymptotic performance and the cost of computation associated with building a good set of options.},
archivePrefix = {arXiv},
arxivId = {1612.00916},
author = {Bacon, Pierre-Luc and Precup, Doina},
eprint = {1612.00916},
file = {:home/telfaralex/Downloads/1612.00916 (1).pdf:pdf},
pages = {1--6},
title = {{A Matrix Splitting Perspective on Planning with Options}},
url = {http://arxiv.org/abs/1612.00916},
year = {2016}
}
@article{Fruit2017a,
author = {Fruit, Ronan and Lazaric, Alessandro and Pirotta, Matteo and Brunskill, Emma},
file = {:home/telfaralex/Downloads/11{\%}5CCameraReadySubmission{\%}5Cmain.pdf:pdf},
journal = {NIPS Hierarchical Reinforcement Learning Workshop},
title = {{Empirical Evaluation of Optimism with Options}},
url = {https://drive.google.com/file/d/1KZIXv43JRq9pxgLKD1mEuoJOSHqVBkIE/view},
year = {2017}
}
@article{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
doi = {10.1371/journal.pntd.0000787},
eprint = {1711.03817},
file = {:home/telfaralex/Downloads/1711.03817.pdf:pdf},
isbn = {1935-2735 (Electronic)$\backslash$r1935-2727 (Linking)},
issn = {1935-2735},
pmid = {20706628},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@article{Riemer2018,
abstract = {Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 1999). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.},
archivePrefix = {arXiv},
arxivId = {1810.11583},
author = {Riemer, Matthew and Liu, Miao and Tesauro, Gerald},
doi = {arXiv:1810.11583v1},
eprint = {1810.11583},
file = {:home/telfaralex/Downloads/8243-learning-abstract-options.pdf:pdf},
isbn = {1810.11583v3},
number = {NeurIPS},
pages = {1--11},
title = {{Learning Abstract Options}},
url = {http://arxiv.org/abs/1810.11583},
year = {2018}
}
@article{Achiam2018,
abstract = {We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1807.10299},
author = {Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
eprint = {1807.10299},
file = {:home/telfaralex/Downloads/1807.10299.pdf:pdf},
pages = {1--29},
title = {{Variational Option Discovery Algorithms}},
url = {http://arxiv.org/abs/1807.10299},
year = {2018}
}
@article{Bacon2016a,
abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup {\&} Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
archivePrefix = {arXiv},
arxivId = {1609.05140},
author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
doi = {10.1109/ICABME.2017.8167529},
eprint = {1609.05140},
file = {:home/telfaralex/Downloads/1609.05140.pdf:pdf},
issn = {0193-1849},
pmid = {10913042},
title = {{The Option-Critic Architecture}},
url = {http://arxiv.org/abs/1609.05140},
year = {2016}
}
