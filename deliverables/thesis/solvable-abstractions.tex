\section{Solvable representations}

\begin{displayquote}
  \textit{Abstraction for efficient optimisation.}
\end{displayquote}



% bellman eqn is hard to solve (how hard?)
Existing solvers of MDPs such as are not efficient: value iteration requires at
least \href{https://en.wikipedia.org/wiki/EXPTIME}{EXPTIME} iterations,
policy iteration requires ???, or policy gradients,
require a ??? amount of computation.
Yet, recent work \cite{} has shown that MDPs require at least $\Omega(|S|^2|A|).$

% (lower bound computational complexity?!)
Is there a way to turn this into an easier problem to solve?

% maybe we can find an abstraction that allows more powerful solvers?

% what are more powerful sovlers?
Well, which problems are easy to solve? Convex ones, linear ones, continuous ones, ... \cite{ODonoghue2012a}

% how can we use these more powerful solvers for RL?
So, is it possible to convert the
 linear problem? What sacrifices need to be made to
achieve this?

\subsection{Linear Markov decision problems (LMDPs)}

\begin{displayquote}
Why linear?
\end{displayquote}

% How are we exploiting linearity?
% Linearisation around the optima? Intuition. How to see this? Stephen!?
Intuition.

\subsubsection{Existing linear methods}

% many mathematical tools for analysis.
% we know linear systems can be solved efficiently.

% Linearity in MDPs
% Polytope. LPs


\subsubsection{Compositionality}

Imagine if your life were linear, in the sense of effort and achieving goals.
More work in equals proportionally more rewards. This makes decision making
a lot more simple. Pick the most rewarding ???, and work hard.

Saxe el al. observe the power of linearity: \textit{"Consider two standard MDPs
$M_1 = \{S, A, P, R_1\}$ and $M_2 = \{S, A, P, R_2\}$ which have
identical state spaces, action sets, and transition dynamics but differ in their
instantaneous rewards $R_1$ and $R_2$. These may be solved independently to
yield value functions $V_1$ and $V_2$. [If the problem were linear then] the value function of the MDP
$M_{1+2} = \{S, A, P, R_1 +R_2\}$, whose instantaneous rewards are the sum of the
first two, [would be] $V_{1+2} = V_1 + V_2$.} \cite{Saxea}

% why do we care about multitask!?
The ability to recycle knowledge from a task to a new task is !!!



% are there other famous examples of linearising something to make it easy to solve?!?

\subsubsection{What do we mean by a linear MDP?}

What properties should a LMDP have? Solving for the optimal policy is a linear problem.
What does this require??

% alternate versions of LMDPs
It turns out that linearising a MDP is not a new idea.

\begin{itemize}
  \tightlist
  \item Jin et al. construct a MDP where the value is a linear function of a state-action representation. \cite{Wang}
  \item Levine et al. contstruct a learner that assumes there exists a linear transition function. \cite{Levine2019}
  \item Todorov costructs a linearised MDP that can be solved via a linear equation. \cite{Todorov2006}
  \item Pires et al. construct a factored linear MDP that allows the TD operator to be applied in a smaller latent space. \cite{Pires2016}
\end{itemize}


% pick the todorov ones.
We explore the LDMPs that are defined in ???. ({\color{red}why?})
For the rest of this work we will refer to these as LMDPs.

\subsubsection{Constructing a linear MDP}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/drawings/abstract-representations-solve.png}
  \caption{'Solving the LMDP'}
\end{figure}

% intuition
To build a LMDP that acts similarly to a MDP, Todorov \cite{Todorov2006} uses a few main tricks;

\begin{itemize}
\tightlist
  \item
  they allow the agent to pick actions in the space of possible transitions, which they name controls, $u: S \to S$, rather than the action space.
  \item
  to ensure that chosen controls are possible under the given transition function, a new reward is added.
  Controls are rewarded if they are close to the 'unconstrained dynamics', $p(s' | s)$.
  \item
  rather than maximizing the cumulative reward, we maximise the exponentiated cumulative rewards.
  $\mathop{\text{max}}_{\pi} \mathop{\mathbb E}_{\zeta \sim D(\pi)} e^{R(\zeta)}$ \cite{EricWarrenFox2016}
\end{itemize}

% formal definition
Putting these together, a linear markov decision process can be formulated as;
LMDP = $\{S, U, p, q, \gamma\}$. Where $p: S \to S$ is the unconditioned dynamics, and $q \in \mathbb R^{|S|}$ are state rewards.
Our goal is to find the control, $u: S \to S$, that gvies the highest exponentiated cumlative reward $z \in \mathbb R_+^{|S|}$.

\begin{align}
V(s) &= \mathop{\text{max}}_{u} q(s) - \text{KL}(u(\cdot| s) \parallel p(\cdot | s)) + \gamma \mathop{\mathbb E}_{s' \sim u(\cdot | s)} V(s') \tag{1}\\
u^{* }(\cdot | s) &= \frac{p(\cdot | s)\cdot z(\cdot)^{\gamma}}{\sum_{s'} p(s' | s) z(s')^{\gamma}} \tag{2}\\
z_{u^{* }} &= e^{q(s)}\cdot P z_{u^{* }}^{\gamma} \tag{3}
\end{align}

By definition, an LMDP is the optimisation problem in (1). After some algebra,
it can be shown (see appendix [] for a derivation) that the optimal policy has the form in (2).
And we can solve for the value of this policy (aka the policy itself in this case)
by solving the linear equation in (3).

\begin{displayquote}
\textit{Let's try and understand this thing we have contructed.}
\end{displayquote}

\subsubsection{Unconstrained dynamics and state rewards}

The state rewards are not capable of giving rewards for actions taken.
Rather, the differences in reward, by taking another action, is captured
by the KL divergence between the control and the unconstrained dynamics.

\begin{itemize}
\tightlist
\item
  What is their function?
\item
  What do they look like?
\end{itemize}

Does it make sense to treat the q(s) like rewards?! They reward for being
in state $s$. But cant capture action specific rewards!?

We will revisit to these questions in section [].

\subsubsection{The optimal policy}

The form of this optimal policy ...

\subsubsection{Solving for the optimal value}

Why is it we can just solve this equenction? Relation to eigen analysis?

\subsection{Solving a MDP}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/drawings/abstract-representations-linear.png}
\caption{''}
\end{figure}

\begin{displayquote}
\textit{So, LMDPs can be easily solved. But how does solving LMDPs help us solve MDPs?}
\end{displayquote}

We want a way to transform a MDP into a LMDP, while preserving the
`structure' of the original MDP. But what do we mean by a MDP's structure?
The LMDP should;

\begin{enumerate}
\tightlist
\item
  be able to represent the same transition dynamics as the original MDP,
\item
  give the the same rewards was the original MDP,
\item
  have the same optima.
\end{enumerate}

% (It turns out that (1) and (2) imply (3) given some assumptions. See
% \href{}{Optimality}) ?????

So, given a reward function, $r$, and a transition function, $P$,
from an MDP, we must translate them into unconditioned dynamics $p$ and a state reward $q$.
While satisfying 1,2,3.

\begin{align*}
\forall s, s' \in S, \forall a \in A, \exists u_a& \;\;\text{such that;} \\
P(s' | s, a) &= u_a(s'|s)p(s'|s) \tag{1} \\
r(s, a) &= q(s) - \text{KL}(P(\cdot | s, a) \parallel u_a(\cdot| s) ) \tag{2}
\end{align*}

% we have not guaranteed that they have the same optima!?!?!

This transformation, from $P, r \to p, q$ requires $|S| \times |A|$ computations, as for each state in the
MDP \(|A|\) linear equations to solve. For more details see appendix [].

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Initially, I thought of the unconstrained dynamics as the expected result of using random policy
$p(s' | s) = \int_a P(s' | s, a)U(a|s)$, a random walk using the transition function.
However, while this make sense intuitively, it turns out to be wrong. (more details here...)

\subsection{Decoding the optimal LMDP policy}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/drawings/abstract-representations-project.png}
\caption{''}
\end{figure}


We transformed our MDP into a LMDP, we have solved it. Now, how can we use $u^{* }$
to construct a policy that is optimal in the original MDP, $\pi_{u^* }$?

Here we get a glimpse at why LMDPs are a potentially interesting abstraction.
The LMDP has disentangled the search for the optimal controls (\textbf{P1}) (go to this or
that state) and the search for behaviour \textbf{P2} (how to actually
realise that control). This can be seen in the decoding step. As we know which
states we want to be in, via the optimal control from solving the LMDP,
\(u^{* }\), but, we do not know how to implement those controls using
the actions we have available.

\begin{align}
P_{\pi}(\cdot | s) = \sum_a P(\cdot | s, a) \pi(a | s) \\
\pi = \mathop{\text{argmin}}_{\pi} \text{KL}\Big(u(\cdot | s))\parallel P_{\pi}(\cdot | s)\Big)
\end{align}

Maybe this isnt enough? Do we need to add a reward sensitive part as
well?!? (but what if the actual path we take to get there has a neg
rewards?!?)

It turns out that solving \textbf{P2} is asymptotically as hard as solving a MDP.
So nothing has been gained... (proof?!?)

\subsubsection{Optimality of solutions via LMDPs}

\begin{quote}
Do these two paths lead to the same place?
\end{quote}

One of the main questions we have not addressed yet is; if we solve the
MDP directly, or linearise, solve and project, do we end up in the same
place? This is a question about the completeness of our abstraction. Can
our abstraction represent (and find) the same solutions that the
original can?

\begin{align*}
\parallel V_{\pi^{* }} - V_{\pi_{u^{* }}} \parallel_{\infty}&= \epsilon\\
\end{align*}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  We want to compare the optimal policies value and the value achieved
  by the optimal LDMP solution.
\item
  Assume that there exists a policy that can generate the optimal
  control dynamics (as given by the LMDP). In that case we can set
  \(P_{\pi_{u^{* }}} = U^{* }\).
\item
  \(r_{u^{* }}\) doesnt really make sense as the reward is action
  dependent. We could calculate it as \(r_{\pi_{u^{* } }}\), but we dont
  explicity know \(\pi_{u^{* }}\). \((I - \gamma P_{\pi^{* }})^{-1}r\)
  represents the action-values, or \(Q\) values. By doing this exhange,
  we might over estimate the diffference under the infinity norm as two
  non-optimal actions may have larger difference. Also, use the element
  wise infinity norm.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Ok, great. Insights from optimality bounds.

Need to be able to approximate the optimal controls. When is it hard to
approximate the optimal controls? When our basis set of distributions
oer future states (aka our actions) have little weight\ldots{}?

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth,height=0.5\textheight]{../../pictures/figures/LBO_BO.png}
\caption{'For the same MDP, shown is a comparison of the linear temporal difference operator (left), versus the true, Bellman, temporal difference operator (right). As expected, the Bellman temporal difference operator points towards the optimal value. But, ther linear temporal difference operator points elsewhere...'}
\end{figure}

\subsubsection{Option decoding}

What about using options to help solve the optimal control decoding?
Does this actually help?!

\begin{align}
P_{\pi}(\cdot | s) = \sum_\omega P_k(\cdot | s, \omega) \pi(\omega | s) \\
\pi = \mathop{\text{argmin}}_{\pi} \text{KL}\Big(u(\cdot | s))\parallel P_{\pi}(\cdot | s)\Big)
\end{align}

Options would allow greater flexibility in the \(P_{\pi}(\cdot | s)\)
distribution, making is possible to match \(u(s'|s)\) with greater
accuracy (and possibly cost).

\begin{itemize}
\tightlist
\item
  First need to demonstrate that action decoding is lossy.
\item
  Then show that using options is less lossy.
\end{itemize}

This introduces dangers?!? As an option might accumulate unknown rewards
along the way!??

\subsubsection{The complexity of solutions via LMDPs}

\begin{quote}
Is my path actually shorter?
\end{quote}

The whole point of this abstraction was to make the problem easier to
solve. So hasit actually made it any easier?

The complexity of solving our abstraction can be broken down into the
three steps;

\begin{itemize}
\tightlist
\item
  linearisation: \(|S| \times \text{min}(|S|,|A|)^{2.3}\)
\item
  solve the LMDP: \(\text{min}(|S|,|A|)^{2.3}\)
\item
  project back: \(???\)
\end{itemize}

Giving a total complexity of \ldots{}

Contrasted with the complexity of solving an MDP.

\subsection{Scaling to more complex problems}

Now that we have some evidence that this LMDP solution strategy makes
sense, it efficiently (see \href{}{complexity}) yields high value (see
\href{}{optimality}) policies. We want to test it out on some real world
problems. But the real world isn't as nice as the setting we have been
working in. There are a few added complexities;

\begin{itemize}
\tightlist
\item
  sample based / incremental
\item
  large / cts state spaces
\item
  sparse rewards
\end{itemize}

So now that we have explored LMDPs, how can we extract their nice
properties into an architecture that might scale to more complex
problems: larger state spaces and action spaces, sparse rewards,
\ldots{}?

\subsubsection{Incremental implementation}

Generalise to a more complex problem. We are only given samples. A first
step to tackling more complex problems.


\subsubsection{Model based}

Learn \(p, q\) based on samples.

\begin{align}
\mathcal L(\theta, \phi) = \mathop{\mathbb E}_{s, a,} \bigg[ r(s, a) - q_\theta(s) + \text{KL}(p_\phi(\cdot | s) \parallel P(\cdot | s, a)) \bigg]\\
\mathcal L(\theta, \phi) = \mathop{\mathbb E}_{s, r, s'} \bigg[r - q_\theta(s) - p_\phi(s' | s) \log \frac{1}{ p_\phi(s' | s)} \bigg] \\
\end{align}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Ok. Lets take a different approach. \textbf{Q:} Why is it a bad idea to
try to do incremental RL with this linearisation trick? Not sure.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Alternative perspective. The high value trajectories are the most likely
ones.

\subsubsection{Distributions over states}

What if we wanted to approximate these distributions? Generalise subgoal
methods to work with distributions? The distribution could be
constructed via; parzen window / GMM, neural flow, ?!.

Connections to distributional RL?

Questions

\begin{itemize}
\tightlist
\item
  What is p(s'\textbar{}s)!?!?
\item
  Want some examples of MDPs they cannot solve.
\item
  What is the relationship to other action embedding strategies?
\item
  How does p(s'\textbar{}s) bias the controls found??? I can imagine the
  unconstrained dynamics acting as a prior and prefering some controls
  over others.
\item
  If we have m states and n actions. Where m
  \textgreater{}\textgreater{} n. Then \(u(s'|s)\) is much larger than
  \(\pi(a|s)\). Also, \(u(s'|s)\) should be low rank?!
  \(u_{s's} = \sum_a u_a \alpha_a u_a^T\)
\end{itemize}

\subsection{Other properties}

LMDPs have the property that if we have already solved two LMDPs, with
the same state space, action space, unconditioned transition dynamics,
but different state rewards, \(q_1, q_2\). Then we can solve a new LMDP,
again with the same, \ldots{}, and state rewards in the span of
\(q_1, q_2\), \(z_3 = w_1 z_1 + w_2 z_2\), \ldots{}

Problem. What does it even mean for two LMDPs to have the same
unconditioned dynamics but different state rewards? The MDPs must have
been the same up to some additive constant (constant in the actions),
\(r(s, a)=r(s, a) + c(s)\). Does this really capture what we mean by
different tasks?!?

AND HRL!?!?

Refs \cite{Todorov2006,Todorov2009,Zhong,Zhonga,Dvijotham,Wozabal}



\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.35\textheight]{../../pictures/figures/chain-test-zero-rewards.png}
\caption{A chain problem with zero reward on all states except the last two.
The optimal control to this problem is not sensible: in every state, jump to the state with positive reward.
However, it is not possible to make those transitions as }
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.35\textheight]{../../pictures/figures/chain-test-pos-rewards.png}
\caption{A chain problem with positive rewards applied to all states.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.35\textheight]{../../pictures/figures/chain-test-neg-rewards.png}
\caption{A chain problem with negative rewards applied to all states}
\end{figure}

The point is, it is not entierly clear how to embed an MDP into a LMDP.


\subsection{Related work}

Recently there have been a few other attempts to add linearity into MDPs.
\cite{Pires2016} factored

\cite{Wang} Reward and transitions are a linear function of features of a state-action pair.

Levine et al. \cite{Levine2019} build a latent representation such that the transition fn (in the latent space) is approximately linear. This allows ...

The common theme amongst all these attempts is that they exploit linearity in the transition function to allow ?!?.
