TODOs

* [ ] Review RL for RL. (representation learning for reinforcement learning)
* [ ] Relate homomorphisms to similarity measures
* [ ] Section on Evaluating efficient exploration / control
* [ ] Abstraction Complexity. single task / multi task
* [ ] Construct family of abstractions
* [ ] Sort out footnote ordering
* [ ] Make sure i introduce the TD operator. esp notation $T(Q)$
* motivate and explain the bellman eqn
* write abstract
* fix timeline. add some other points
* fix all citations
* double check the uniqueness of the optimal policy
* explain why VI goes outside the polytope
* tidy / finish abstraction intro section
* regenerate trajectory pictures!?



Done

* [x] Finish / tidy section of coarseness.
* [x] Abstraction - learnability experiments (off policy etc...)
* [x] Add pictures for polytope graph visuals
* [x] Temporal abstraction example
* [x] Picture for state abstraction
* [x] Discuss the discovery problem and its relation to complexity
* [x] Write intro for solvable representations
* [x] Tidy section on LMDP claims and what my experiments say
* [x] Organise similarity measures for abstraction
* [x] Tidy explanation of geometry of polytope.
* [x] Trajectory pics for PI, PG, VI
