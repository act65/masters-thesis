\chapter{Related work}

How do the topics considered in this thesis relate to the work done in the wider scientific community and to society? Which work uses similar tools, which works build on the same foundations, which works have the same goals? How will this help? What can it be used for?

Decision theory (economics and psychology), ...?
Control theory

\section{Search spaces}


Recently there has been work investigating the properties of overparameterised search spaces.
Many \cite{Arora2018} (and others?!?!?) claim that overparameterisation yeilds acceleration, however,
their explanation of the acceleration is not entirely convincing.

\cite{Arora2018}'s proofs ...??


\section{Abstraction}

\hypertarget{representation-learning-and-abstraction}{%
\subsection{Representation learning and abstraction}\label{representation-learning-and-abstraction}}

The goal is to find a representation that decomposes knowledge into its parts.

Another way to frame this is: trying to find the basis with the right
properties.

\begin{itemize}
\tightlist
\item
  sparsity,
\item
  independence,
\item
  multi scale,
\item
  locality/connectedness
\item
  ???
\end{itemize}


Types of abstraction for RL. Abstraction for efficient;

\begin{itemize}
\tightlist
\item
  exploration, Learning latent state representation for speeding up exploration \cite{Vezzani2019}
\item
  optimal control,
\item
  ???,
\end{itemize}

Unsupervised State Representation Learning in Atari  \cite{Anand2019}
State Aggregation Learning from Markov Transition Data  !!!\cite{Duan2018}

\subsection{Linear RL}

Recently there have been a few other attempts to exploit linearity for RL.
\cite{Pires2016} factored

\cite{Wang} Reward and transitions are a linear function of features of a state-action pair.

Levine et al. \cite{Levine2019} build a latent representation such that the transition fn (in the latent space) is approximately linear. This allows ...

These attempts all focus on linearity in the transition function...

\subsection{Heirarchical reinforcement learning}

What is HRL? Really it is just temporal abstraction, using a heirarchy.

As far as I know, there are three main apporaches to HRL.
Equip an agent with options, learning to generate sub goals, pretrained low level policies, .


Between MDPs and Semi-MDPs \cite{RichardS.SuttonaDoinaPrecupb1998}
Reinforcement learning with structured hierarchical grammar representations of actions  \cite{Christodoulou2019}  !!! theory for this tho?!
Latent Space Policies for Hierarchical Reinforcement Learning \cite{Haarnoja}

Two main approaches. Subgoals and options. To achieve temporal abstraction.

- Does it help?
- Why does it help?
- ?

We don't really have a principled approach to HRL?!

- How does the heirarchy help?
- When can we expect transfer?

Discovery! And no free lunches!?

\begin{displayquote}
  The challenge in the single-task case is overcoming the additional cost of discovering the options; this results in a narrow opportunity for performance improvements, but a well-defined objective. In the skill transfer case, the key challenge is predicting the usefulness of a particular option to future tasks, given limited data. \cite{Konidaris2019}
\end{displayquote}


Temoral abstractions of actions.(how does this related to a
decomposition of rewards) Ok, so we wany a multiscale representation?
Understanding how actions combine (this is necessary knowledge for HRL?)

Reasons to do HRL??? (want to verify these claims - and have refs for
them)

\begin{itemize}
\item
  credit assignment over long time periods (learning faster in one env)
\item
  exploration
\item
  transfer
\item
  To learn action abstractions they must capture info about the model.
  How much harder is it to learn action abstractions in model-free vs
  model-based settings?
\item
  Reward as a function of a subspace of the state space. (this is
  important for learning abstract representations and actions!?)
\item
  What do cts linear heirarchical actions look like!? and their loss
  surface!?
\item
  HLMDPs \cite{Saxea}
\item
  Modulated policy heirarchies \cite{Pashevich}
\item
  Model free representations for HRL \cite{Rafati}
\item
  \href{https://blog.aqnichol.com/2019/04/03/prierarchy-implicit-hierarchies/}{Prierarchy:
  Implicit Hierarchies}
\item
  Options
\item
  Near optimal representation learning for heirarchical RL \cite{Nachum2018}
\end{itemize}

In "Why does Heirarchy (sometimes) work so well in reinforcement learning?" \cite{Dadashi2018}
authors claim that the benefits of HRL can be explained by better
exploration. However, I would interpret their results as saying; "for
2D environments with walls, better exploration (aka larger steps, aka actions that are more temporally abstract), result in greater
explration". But what if the walls were replaced by cliffs? I imagine
this algorithm might do a lot worse!? Also, they don't seem to consider the main problem with HRL: discovery.
Once you have discovered a nice set of abstract actions or states, then yeah,
you get faster reward propagation, better exploration, \ldots{} etc.
