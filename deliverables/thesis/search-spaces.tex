% \chapter{Search spaces}

\newpage

Before going further with our quest for efficient RL. Let's try to understand
some properties of our setting, MDPs.


\section{The value function polytope}

The Value Function Polytope \cite{Dadashi2018} provides some great intuition
about the structure of a MDP and the dynamics and complexity of solvers.
Let's take a look: consider a two state, two action MDP.

\begin{figure}[hb!]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/2-state-automata.png}
\caption{The simplest possible MDP has two states and two actions. (Any simpler setting is entirely uninteresting. A single state means actions do nothing.
And a single action means all policies are the same.).}
\end{figure}

The space of possible policies is a 2D surface in a 4D space. For each state, we
can pick $\text{action1}$ or $\text{action2}$, with some probability, $p$. For some intuition
about this policy space see \ref{high-D-policies}.

\begin{align}
\pi &=
\begin{bmatrix}
  p(a=a_1|s=s_1) & p(a=a_2|s=s_1) \\
  p(a=a_1|s=s_2) & p(a=a_2|s=s_2)\\
\end{bmatrix} \\
&=
\begin{bmatrix}
p(a=a_1|s=s_1) & 1-p(a=a_1|s=s_1) \\
p(a=a_1|s=s_2) & 1-p(a=a_1|s=s_2)\\
\end{bmatrix}
\end{align}

Since the policies are a 2D space, we can visualise them. This square of all possible policies is not particularly interesting.

Rather, we can evaluate (calculate the expected return) each each policy (using the \eqref{eq:value-functional}).
Since there are two states, the evaluation returns a 2D vector of values, one value for each state.
Therefore, we can visualise the value of each policy.
\begin{figure}[!hb]
\centering
\includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/figures/value-polytope.png}
\caption{For every policy, we can plot a dot where that value of that policy lies in 'value space'.
The red dots are deterministic policies.}
\end{figure}

Dadashi et al. \cite{Dadashi2018} explored a few properties of the polytope.
Sepcifically they focused on its geometry and dynamics. And, in
\ref{polytope-extras} you can find further exploration of other properties of the value polytope.

\subsubsection{Geometry of the polytope}

Dadashi et al. remark, the polytope gives a clear illustration of the following classic results regarding MDPs \cite{Bertsekas1996}.

\begin{enumerate}
\tightlist
  \item (Dominance of $V^*$) The optimal value function $V^*$ is the unique dominating vertex of $V$;
  \item (Monotonicity) The edges of V are oriented with the positive orthant;
  \item (Continuity) The space V is connected.
\end{enumerate}

\textbf{1)} As explained in ref. There is a unique optimal policy, which ...

\textbf{2)} A change in $\pi(a_1|s_1)$ which increases the value of state $1$, can, at worst, do nothing for
state two, aka we would see a flat line. But the value of state $2$
increases and there is a non zero chance of transitioning to that state, from your current state,
then that will increase the value of your current state.
This explains why the edges of the polytope by be aligned with the positive orthant, why they slant upward. $\hat V(s_1) = V(s_1)P(s_1|s_1, a)\pi(a|s_1) + V(s_2)P(s_2|s_1, a)\pi(a|s_1)$


\textbf{3)} ???


% Further more, Dadashi et al show that ... Line theorem. s-deterministic policies.



\subsubsection{Dynamics on the polytope}

Furthermore, Dadashi et al. \cite{Dadashi2018} were interested in three aspects of different algorithmsâ€™ learning dynamics:

\begin{itemize}
\tightlist
  \item the path taken through the value polytope,
  \item the speed at which they traverse the polytope,
  \item and any accumulation points that occur along this path.
\end{itemize}

% Why do they care?
% How quickly do these learners traverse the polytope? Do algorithms take the shortest path? Where are the accumulation points?

They consider value iteration, policy iteration, policy gradients, entropy regularized policy gradients,
natural policy gradients and the cross entropy method.

Their results are intriguing. They show that different RL algorithms traverse the polytope in vastly different ways.
Some are not even constrined to the polytope. This left me wondering;

{\color{red}I feel like this needs more???}

\begin{displayquote}
  \textit{How does a search algorithm interact with its search space to yield efficient search?}
\end{displayquote}

\section{Search spaces for MDPs}\label{search-spaces-mdps}

We want to efficiently find the optimal policy for a given MDP. But where and how should we
search for this policy? We could search within;

\begin{itemize}
\tightlist
  \item the set of potentially optimal policies, the $|A|^{|S|}$ discrete policies,
  \item the set of all possible policies $\pi \in \mathbb R^{|S| \times |A|}: \forall s \int_a \pi(a|s) = 1$
  \item the set of possible state-action value functions, $\mathbb R^{|S|\times|S|}$,
  which we could then use to construct the optimal policy,
  \item Or maybe some other space.
\end{itemize}

\begin{displayquote}
  \textit{Which space is best? Which space allows us to find the optimal policy in the 'cheapest' manner?}
\end{displayquote}

Naively, we think smaller search spaces are better. We would rather
search for our keys in a few rooms, rather than many. But added
structure (for example, an ordering) can be exploited to yield faster
search, even when there are infinitely more states to search. For example,
we might be able to order the rooms based on how recently we visited them.
This should help us retrace our steps and find our keys, rather than arbitrarily
picking rooms to search.

\subsection{Policy search}

We can search through policies. In my opinion, this feels like the most 'natural' type of search for RL.
As, after all, we are searching for the optimal \underline{policy}.

Searching through the space of policies supports a couple of modes of travel:
policy iteration and policy gradients

\subsubsection{Policy iteration}

In our tabular setting, policy iteraction can be written as:

\begin{algorithm}
\caption{Policy iteration}
\begin{algorithmic}[1]

\Procedure{PI}{$P, r, \gamma$}
    \State $\pi \leftarrow \text{rnd init}$
    \While{not converged}
      \State $V(\pi) =  (I - \gamma P_{\pi})^{-1} r_{\pi}$ \Comment{Evaluate policy}
      \State $\pi = \mathop{\text{argmax}}_\pi \big[r_{\pi} + \gamma P_{\pi}V(\pi) \big]$ \Comment{Greedy update}
    \EndWhile
    \State \algorithmicreturn{ $\pi$}
\EndProcedure

\end{algorithmic}
\end{algorithm}

As can be seen below PI jumps between deterministic policies (the verticies of our polytope).

{\color{red}figures}
{\color{red}ref for PI.}

Actually has some connections to the simplex method. ref

\subsubsection{Policy gradients}

This search is most related to the deep learning / end-to-end paradigm.
Simply write down what you want (the loss function -- to maximise the expected return),
estimate its derivative and apply gradient descent. (ref)

% However, has problems with sparse rewards / exploration (not considered in this setting).
% (but dont they all have problems with sparse rewards!?)

\begin{algorithm}
\caption{Policy gradients}
\begin{algorithmic}[1]

\Procedure{PG}{$P, r, \gamma, \eta$}
  \State t = 0
  \State $\pi_t = \text{init}()$
  \While{not converged}
    \State $\pi_{t+1} = \pi_t + \eta \nabla_{\pi} V(\pi)$ \Comment{Gradient update}
    \State t += 1
  \EndWhile
  \State \algorithmicreturn{ $\pi_t$}
\EndProcedure

\end{algorithmic}
\end{algorithm}

takes small steps in the 'right' direction
and {\color{red}often traverses through the center of the polytope}.
add ref

{\color{red}figures}

% Want to include upper / lower bounds!?

Converges at a rate of $\frac{1}{t}$. As can be seen by ...
\cite{Agarwal2019a}.

\subsection{Value search}

This search is guided by the temporal different operator. !?!?

Alternatively, we can search through possible values. But how can we ensure that our search will
converge to a value that corresponds to a realisable policy? We can use Bellman's
optimality operator to guide the search.

% (need to explain? why does stationarity mean optimality...?!)
Intution about why it converges!? Contraction. Banach fixed-point theorem.

% \begin{align}
% T(V) &= \mathop{\text{max}}_a \big[r + \gamma PV\big] \\
% \end{align}

\begin{algorithm}
\caption{Value iteration}
\begin{algorithmic}[1]

\Procedure{VI}{$P, r, \gamma, \eta$}
  \State t = 0
  \State $V_t = \text{init}()$
  \While{not converged}
    \State $\hat V = r + \gamma PV_t$ \Comment{Evaluate}
    \State $V_{t+1} = V_t + \eta (\hat V - V_t)$ \Comment{Average}
    \State t += 1
  \EndWhile
  \State $\pi = \mathop{\text{argmax}}_{\pi} r_{\pi} + \gamma P_{\pi}V_t$
  \State \algorithmicreturn{ $\pi$}
\EndProcedure

\end{algorithmic}
\end{algorithm}

{\color{red}figures}

% Want to include upper / lower bounds!? On complexity. Sample / computational!?

Observe that the value iterations are not constrained to refer to any policy,
and thus can go outside of the polytope. \cite{Dadashi2018}

{\color{red}mportant. Also, why does it go outside?!?}

We are constrained to move around by only using the Bellman optimality operator.
To move proportionally to value improvement of the best actions ...
Why is this good / bad. What dynamics can / cannot be achieved?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

So there are different classes of search space: each imbued with special
structure from the Bellman equation or expected return. Each with different types of search they
support.

\begin{displayquote}
\textit{Which spaces support efficient search for the optimal policy? Can we characterise the properties of each space?}
\end{displayquote}

See appendix for an experiment exploration of the iteration complexity and dynamics of these different alforithms.
{\color{red}want to include in main section. but needs more work. not really sure what i am trying to say...}
