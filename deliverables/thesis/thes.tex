\documentclass[12pt, a4paper, twoside, openright]{book}

\usepackage{vuwthesis} % sets up some local things, mostly the front page

\usepackage{palatino} % sets palatino as the default font

\usepackage{url} % for typesetting urls

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{epigraph}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8x]{inputenc}
\DeclareUnicodeCharacter{2212}{-}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

%\renewcommand{\baselinestretch}{1.00}
\newenvironment{changemargin}[2]{%
\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{#1}%
\setlength{\rightmargin}{#2}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{\parskip}%
}%
\item[]}{\end{list}}


\begin{document}

\frontmatter
% Book style knows about front matter
% Report style doesn't so you need to set roman numbering etc yourself :-(

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ABSTRACTION FOR EFFICIENT REINFORCEMENT LEARNING}
\author{Alexander Telfar}

\subject{Computer Science}
\abstract{
Successful reinforcement learning requires large amounts of data, compute, and some luck.
We explore the ability of abstraction(s) to reduce these dependencies.

Abstractions for reinforcement learning share the goals of this abstract:
to capture essential details, while leaving out the unimportant.
By throwing away inessential details,
there will be less to compute, less to explore, and less variance in observations.
But, does this always aid reinforcement learning?

More specifically, we start by looking for abstractions that are easily solvable.
This leads us to a type of linear abstraction. We show that, while it does allow efficient solutions, it also gives erroneous solutions, in the general case.

We then attempt to improve the sample efficiency of a reinforcment learner.
We do so by constructing a measure of symmetry and using it as an inductive bias.
% Given uncertainty, this learner prefers symmetric hypotheses.
We design and run experiments to test the advantage provided by this inductive bias,
but must leave conclusions to future work.
}
% Books don't normally have abstracts, and this is a bit of a hack

% Uncomment the appropriate degree
% \phd
\mscthesisonly
% \mscwithhonours
%\mscbothparts
% \otherdegree{DEGREE OR DIPLOMA NAME}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\maketitle

\include{thesis-acknowledge}

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% book style knows about mainmatter
% if you are using report style you will have to rest page numbering etc.
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% individual chapters included here

\include{thesis-introduction}

\include{mdps}
\include{search-spaces}

\include{abstraction}
\include{solvable-abstractions}
\include{symmetric-abstractions}
% \include{experiments}

\include{thesis-conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% and of course book style knows about backmatter
% \backmatter caused problems with appendices :-(
% and of course report style doesn't
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{ieeetr}
% \bibliographystyle{acm}
\bibliography{mendeley}

\appendix
\newpage

\label{appendicies}
As these appendices make up a significant amount of this thesis, it may be worth
having a guide.
% although, they are referenced through the thesis.
% Much of the content here could have been in the main text,
% however, it was decided that the thesis would be easier to read if every second
% page did not contain some side note or out-of-scope thought.

\begin{itemize}
\tightlist
\item In \ref{vf-neumann} we give an alternative derivation of the value functional, a tabular form of the state valued Bellman equation.
\item In \ref{high-D-policies} we give some intuition about the abstract structure of policies.
\item In \ref{polytope-extras} we explore the density of policies and effect of discounting on the value polytope.
\item In \ref{model-iteration} we demonstrate a new (so far as we know) type of model based reinforcement learner.
\item In \ref{graph-vis} we present a method of visualising the search for the optimal policy in high dimensions.
\item In \ref{ss-extras} we ask questions about over-parameterisation and re-parameterisation in the context of policy gradients.
\item In \ref{lmdp-derivation} we offer a modified derivation of a linear Markov decision problem.
\item In \ref{mdp-homomorphism} we explore related work combining symmetries and Markov decision problem, and add a new definition of a temporal homomorphism.
\item In \ref{game-invariants} we construct a set of symmetric reinforcement learning examples, in the hope of understanding how to identify relevant symmetries from invariants.
\item In \ref{symmetric-exploitation} we review related work on symmetries in machine learning and draw connections between some of these methods.
\item In \ref{construct-actions} we provide an example of how there can be many (group) actions for even a simple group.
\item In \ref{race-grid-world} we describe a toy problem used in testing our symmetry biased reinforcement learner.
\item In \ref{action-space-experiments} we describe experiments done to test a learners ability to exploit symmetries.
\end{itemize}

\include{mdp-extras}
\include{abstraction-extras}

\end{document}
