\chapter{Markov Decision Problems}

Reinforcement learning (RL) refers to the set of solutions to a type of problem.
This general, reinforcement learning, problem, has two main properties;
\textit{"trial-and-error search and delayed rewards"} \cite{Sutton2018}.

Unlike supervised learning, which gives the learner feedback (\textit{Student: "I think that digit
is a 5". Teacher: "No, it's a 6"}), in RL the learner only receives evaluations (\textit{Student: "I think
that digit is a 5". Teacher: "No."}). This means the student needs to explore the possible answers via some trial-and-error search.
(\textit{Student: "Is it a 4?". Teacher: "No." Student: "How about a 0?". Teacher: "No." ... Student: "A 6?". Teacher: "Yes."})

Ontop of terse teachers, many actions may be taken before any evaluation is received, thus requiring credit to be assigned to past actions,
(\textit{Student: "Is it a 4? How about a 0? A 6? Maybe a 7?". ... Teacher: "No".})
often leaving the learner wondering: "what did I do to deserve this?" (see
\href{https://www.youtube.com/watch?v=Qv4H81gEGDQ}{pigeon superstition} for an amusing
example of credit assignment gone wrong \cite{Box1997}).

% Your teaher might only give you evaluations for sequences of actions, rather than individual actions.
% Thus you are left with trying to infer how these sequence evaluations tells you about which actions you should take.

\vspace{5mm}

The above definition of reinforcement learning is quite general. There are many
different dimensions to problems that require trial-and-error search and give
delayed rewards. For example we could make a RL problem that is;

\begin{itemize}
\tightlist
\item
  Observable or un-observable \cite{Kaelbling1998}
\item
  Deterministic or stochastic \cite{Putterman2015}
\item
  Synchronous or asynchronous \cite{Bertsekas1995}
\item
  Terminating or infinite \cite{Putterman2015}
\item
  Discrete versus continuous \cite{Bertsekas1995}
\item
  Given knowledge of the underlying model \cite{Sutton1991}
\end{itemize}

\begin{displayquote}
  \textit{But, which setting should we study?}
\end{displayquote}

\begin{displayquote}
  A better question might be: \textit{What is the simplest setting we can
  consider that still poses an interesting challenge to the ML and / or RL communities?}
\end{displayquote}

Markov decision problems (MDPs) seem like a good candidate. Let's go through some definitions so we an more clearly understand how they can be used as a
simple setting to analyse RL.

Formally, a MDP, which is a type of sequential decision problem, is defined
as a tuple, $\{\mathcal S, \mathcal A, P(s_{t+1} \mid s_t, a_t),R(s_t, a_t), \gamma, d_0\}$.
Where \(s \in \mathcal S\) is the set of possible states (\textit{for example arrangements of chess pieces}),
\(a \in \mathcal A\) is the set of actions (\textit{the different possible moves, left,
right, diagonal, weird L-shaped thing, ...}),  \(P(s_{t+1} \mid s_t, a_t)\)
is the transition function which describes how the environment acts in response
to the past (\(s_t\)) and to your actions (\(a_t\)) (\textit{in this case, your
opponent's moves, taking one of your pieces, and the results of your actions}),
and finally, \(r(s_t, a_t)\) is the reward function, \textit{whether you won (+1) or lost (-1) the game }.
The objective when solving a MDP is to find a policy, $\pi(a_t | s_t)$,
that maximises the expected return (aka the value). {\color{red}needs more explanation}

\begin{align*}
V(\pi) = \mathop{\mathbb E}_{\zeta \sim D(\pi, \tau)} [R(\zeta)]
\end{align*}


Where, $d_0$ is the initial state distribution, $\zeta$ collects the $(s_t, a_t, r_t)$ triples of a game (aka trajectory or rollout),
$R(\zeta)$ is the sum of discounted rewards, and $D$ is the probability of a trajectory under the chosen policy and MDP.

\begin{align}
\zeta &= \{(s_t, a_t, r_t) : t \in [0, H]\} \tag{trajectory} \\
R(\zeta) &=\sum_{t=0}^H \gamma^t \zeta[r, t] \tag{Return} \\
D(\zeta, \pi, \tau, d_0) &= d_0(\zeta[s, 0]) \prod_{t=1}^{H} \pi(\zeta[a, t]|\zeta[a, t]) \tau(\zeta[s, t+1]|\zeta[s, t], \zeta[a, t]) \tag{p(traj)}
\end{align}

% If we wanted we could pick our actions before we make observations,
% reducing the search space to only \(|A| \times T\). But this is a bad idea\ldots{} example.

You can find examples of some simple MDPs in \ref{MDP-examples}.

\section{Sequential decision problems}

A general intuition for what is going on when solving a sequential decision problem is: decisions (aka actions) are made
sequentially (\textit{eg.}) and need to be conditioned on observations (\textit{eg.}).
The goal is to take actions that achieve higher rewards (\textit{eg.}). While instantaneous
rewards are good, we really care about long term cumulative rewards (\textit{eg.}).

% Maze with pendulums / doors. When moving through the maze, you must
% swing the pendulums. In the future you must avoid being hit. (maybe make
% a picture of this?) also, is there a more general way to think about it?

\begin{displayquote}
  \textit{What does the M in MDP really mean?}
\end{displayquote}

When we say a decision problem is Markovian, we mean that the transition
function generates a Markov chain \cite{Markov2006}. The next transition step depends only
on the current state and action. It is invariant to any and all histories that do not
change the current state. Or another way of saying the same thing, there is no hidden state
that effects future transitions.

This is not to say that past actions do not effect the future. Rather,
it is a special type of dependence on the past. Where the dependence is
totally described by changes to the observable state, $s\in S$.

{\color{red}For example...}

% Can easily make a sequence Markovian by adding information. E.g.
% time

\section{Solving a MDP}

\begin{displayquote}
  \textit{What does it mean to solve a MDP?}
\end{displayquote}

\subsection{Optimality}

A MDP is considered solved when we have found the 'optimal' policy. The 'optimal'
policy is the policy that gives the highest
expected return (value). More formally, the optimal policy, $\pi^{* }$ is defined as

\begin{align*}
\pi^{*} : \;\; V^{\pi^* }(s) \ge V^{\pi}(s) \quad \forall \pi\in \Pi \;\;\forall s\in S\\
\end{align*}

Before we start searching for the optimal policy, it would be nice to know some of its properties.

\begin{itemize}
\tightlist
  \item Does it always exist?
  \item It is unique, or are there many optimal policies?
  \item Does it necessarily have any specific properties?
\end{itemize}

{\color{red} And explanation, examples and refs}

\subsection{The Bellman Equations}

{\color{red}how to intro / motivate bellman and TD??}

An alternative, recursive, definition of the expected return

% DP!
% The Bellman equation (ref) ...

\begin{align*}
Q^{\pi}(s, a) = r(s, a) + \gamma \mathop{\mathbb E}_{s' \sim P(\cdot|s, a)} [V(s')] \label{eq:bellman-eqn}\tag{Bellman equation}\\
V^{\pi}(s) = \mathop{\mathbb E}_{a \sim \pi(\cdot|s)} [Q^{\pi}(s, a)]
\end{align*}


\subsection{Complexity}

\begin{displayquote}
  \textit{How hard is it to find the optimal policy?}
\end{displayquote}

% Insert lower bound and some intution
The complexity of estimating the value of a state-action under the optimal policy, ie solving the Bellman optimality
equation, can be glimpsed if we unroll its recursive definition.
Here we can see a series of nested maximisation problems, where the former
maximisation problems are conditional on the results of the latter maximisation problems.

\begin{align*}
Q^{\pi}(s_0, a_0) = r(s_0, a_0) &+ \gamma \mathop{\text{max}}_{a_1} \mathop{\mathbb E}_{s_1\sim p(\cdot | s_0, a_0)} \Bigg[ \\
r(s_1, a_1)  &+ \gamma \mathop{\text{max}}_{a_2} \mathop{\mathbb E}_{s_2\sim p(\cdot | s_1, a_1)} \bigg[\\
r(s_2, a_2)  &+ \gamma \mathop{\text{max}}_{a_3} \mathop{\mathbb E}_{s_3\sim p(\cdot | s_2, a_2)} \Big[
\dots \Big] \bigg] \Bigg]
\end{align*}

\section{A tabular representation of MDPs}

Back to constructing a simple RL setting.

Imagine a MDP that can be described with tables (aka arrays). A table of
three dimensions can describe the transition probabilities, $P[s_{t+1}, s_t, a_t]$,
and a table of two dimensions can describe the rewards, $r[s_t, a_t]$: the
states and actions act as indexes to locations in the tables.
Let's formally define our tabular MDP.

\begin{align}
\mathcal M &= \{S, A, P, r, \gamma\}\; \tag{the MDP}\\
S &= [0:n-1] \tag{the state space}\\
A &= [0:m-1] \tag{the action space}\\
P &\in [0,1]^{n\times n \times m}, \;\;\forall j, k : \sum_i P[i, j, k] = 1 \tag{the transition fn}\\
r &\in \mathbb R^{n\times m} \tag{the reward fn}
\end{align}

A result of this formulation is that we concisely write and solve the \eqref{eq:bellman-eqn}. \footnotemark[0]

\footnotetext[0]{Note that the ability to solve the Bellman equation analytically does not allow
us to solve for the optimal policy analytically.}

\begin{align}
V &= r_{\pi} + \gamma P_{\pi} V \tag{tabular Bellman eqn}\\
V &= (I-\gamma P_{\pi})^{-1}r_{\pi}  \label{eq:value-functional}\tag{Value functional}
\end{align}

The values are written as a vector, $V \in \mathbb R^n$.
The reward under a given policy is written $r_{\pi}[s, a] = \pi[s, a] r[s, a]$.
And the transitions under a given policy is written $P_{\pi}[s', s] = \sum_a P[s', s, a]\pi[s, a]$.

An alternative derivation of the value functional, which is more verbose and more enlightening, can be found in \ref{vf-neumann}

\begin{displayquote}
\textit{But why is the tabular MDP considered 'simple' enough?}
\end{displayquote}

Consider a MDP with deterministic actions, where $P(s_{t+1}|s_t, a_t) \in \{ 0, 1\}$.
This RL problem can be efficiently solved by non-statistical
methods: dynamic programming and related planning techniques \cite{Bertsekas1995}.

But, a MDP with stochastic actions, $P(s_{t+1}|s_t, a_t) \in [0, 1]$,
seems to retain much of the complexity we care about. This setting does not allow
efficient solutions via dynamic programming. And can be approached with algorithms
that are used for state-of-the-art DRL such as;
policy gradients\cite{Schulman2015a}, Q-learning \cite{Mnih2015}, ??? (more!?)

{\color{red}!!!}
Finally, it should be noted that this tabular MDP setting ignores a crucial
aspects of RL: exploration, estimation error, . It is hoped that we can study
optimisation dynamics,  abstraction,
