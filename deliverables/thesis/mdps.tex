\chapter{Markov Decision Problems}\label{mdps}

Reinforcement learning (RL) refers to the set of solutions to a type of problem.
This general, reinforcement learning, problem, has two main properties;
\textit{"trial-and-error search and delayed rewards"} \cite{Sutton2018}.

Unlike supervised learning, which gives the learner feedback (\textit{Student: "I think that digit
is a 5". Teacher: "No, it's a 6"}), in RL the learner only receives evaluations (\textit{Student: "I think
that digit is a 5". Teacher: "No."}). This means the learner needs to explore the possible answers via some trial-and-error search.
(\textit{Student: "Is it a 4?". Teacher: "No." Student: "How about a 0?". Teacher: "No." ... Student: "A 6?". Teacher: "Yes."})

On top of terse teachers, many actions may be taken before any evaluation is received, thus requiring credit to be assigned to past actions,
(\textit{Student: "Is it a 4? How about a 0? A 6? Maybe a 7?". ... Teacher: "No".})
often leaving the learner wondering: "what did I do to deserve this?" (see
\href{https://www.youtube.com/watch?v=Qv4H81gEGDQ}{pigeon superstition} for an amusing
example of credit assignment gone wrong \cite{Box1997}).

% Your teaher might only give you evaluations for sequences of actions, rather than individual actions.
% Thus you are left with trying to infer how these sequence evaluations tells you about which actions you should take.

\vspace{5mm}

The above definition of reinforcement learning is quite general. There are many
different dimensions to problems that require trial-and-error search and give
delayed rewards. For example we could make a RL problem that is;

\begin{itemize}
\tightlist
\item
  Observable or un-observable \cite{Kaelbling1998}
\item
  Deterministic or stochastic \cite{Putterman2015}
\item
  Synchronous or asynchronous \cite{Bertsekas1995}
\item
  Terminating or infinite \cite{Putterman2015}
\item
  Discrete versus continuous \cite{Bertsekas1995}
\item
  Given knowledge of the underlying model or not \cite{Sutton1991}
\end{itemize}

\begin{displayquote}
  \textsl{But, which setting should we study?\footnotemark}
\end{displayquote}

\footnotetext{I will often start a chapter / section / paragraph with a question like this. These
questions are not meant as research questions. Rather, they are designed to orient the reader.}

\begin{displayquote}
  A better question might be: \textsl{What is the simplest setting we can
  consider that still poses a challenge to the ML and / or RL communities?}
\end{displayquote}

Markov decision problems (MDPs) appear to be a good candidate. Let's go through some definitions so we an more clearly understand how they can be used as a
simple setting to analyse RL.

Formally, a MDP, which is a type of sequential decision problem, is defined
as a tuple, $\{\mathcal S, \mathcal A, \tau,r, \gamma, d_0\}$.
Where $S$ is the set of possible states (\textit{for example arrangements of chess pieces}),
$A$ is the set of actions (\textit{the different possible moves, left,
right, diagonal, L-shaped step, ...}),  $\tau: S \times A \to \Delta(S)$\footnotemark
is the transition function which describes how the environment acts in response
to the current state and to actions (\textit{You play pawn to pawn to D4, in response your
opponent moves, knight to D4, taking your pawn.}). Next is the reward function, $r: S\times A \to \mathbb R$,
(\textit{whether you won (+1) or lost (-1) the game }).
Lastly, the policy, $\pi: S \to \Delta(A)$, is what the learner gets to choose, aka the learners strategy.
It decides which action to take in different states.

\footnotetext{The notation $\Delta(S)$ represents a distribution over S.}

\vspace{5mm}

The objective when solving a MDP is to find a policy
that maximises the expected cumulative discounted reward $V^{\pi}$ (aka the value). This
can be written as, maximising the expected return.

\begin{align*}
V^{\pi} &= \mathop{\mathbb E}_{\zeta \sim D(\pi, \tau)} [R(\zeta)] \tag{state value}\label{state-value}\\
\pi^{* } &= \mathop{\text{max}}_{\pi}V^{\pi}
\end{align*}


Where, $d_0$ is the initial state distribution, $\zeta$ collects the $(s_t, a_t, r_t)$ triples of a game (aka trajectory or rollout) \footnotemark,
$R(\zeta) =\sum_{t=0}^H \gamma^t \zeta^r_t$ is cumulative discounted reward (aka the 'return' of a single game), and $D$ is the probability of a trajectory under the chosen policy and MDP.

\footnotetext{We allow $\zeta$ to be indexed by time and $\{s, a, r\}$. For example; $\zeta_t^s=s_t$.}

\begin{align}
\zeta &= \{(s_t, a_t, r_t) : t \in [0, H]\} \tag{trajectory} \\
D(\zeta, \pi, \tau, d_0) &= d_0(\zeta^s_0) \prod_{t=1}^{\infty} \pi(\zeta^a_t|\zeta^s_t) \tau(\zeta^s_{t+1}|\zeta^s_t, \zeta^a_t) \tag{p($\zeta$)}
\end{align}

% If we wanted we could pick our actions before we make observations,
% reducing the search space to only \(|A| \times T\). But this is a bad idea\ldots{} example.

\section{Sequential decision problems}

A general intuition for the problem of solving a sequential decision problem is: actions (aka decisions) are made
sequentially (\textit{e.g. First we put on our socks then we put on our shoes}).
These actions need to be conditioned on the current state of the world (\textit{e.g. It is morning and time to go to work.}).
The goal is to take actions that achieve higher rewards (\textit{e.g. Lying in bed is quite rewarding...}). While instantaneous
rewards are good, we really care about long term cumulative rewards (\textit{e.g. Having a job and thus being able to afford a bed is more rewarding.}).

% Maze with pendulums / doors. When moving through the maze, you must
% swing the pendulums. In the future you must avoid being hit. (maybe make
% a picture of this?) also, is there a more general way to think about it?

\begin{displayquote}
  \textsl{What does the M in MDP really mean?}
\end{displayquote}

When we say a decision problem is Markovian, we mean that the transition
function generates a Markov chain \cite{Markov2006}. The next transition step depends only
on the current state and action. It is invariant to any and all histories that do not
change the current state. \footnotemark

\footnotetext{Or another way of saying the same thing, there is no hidden state
that effects future transitions.}

This is not to say that past actions do not effect the future. Rather,
it is a special type of dependence on the past. Where the dependence is
totally described by changes to the state, $s\in S$. We can return to chess for
an example: in chess there are no hidden pieces, or
private knowledge about the current state. I know everything there is to know.

% Can easily make a sequence Markovian by adding information. E.g. time

\section{Solving a MDP}

\begin{displayquote}
  \textsl{What does it mean to solve a MDP?}
\end{displayquote}

A MDP is considered solved when we have found the 'optimal' policy. As above,
the 'optimal' policy is the policy that gives the highest expected return (value).
This means that;

\begin{align*}
\pi^{*} : \;\; V^{\pi^* }(s) \ge V^{\pi}(s) \quad \forall \pi\in \Pi \;\;\forall s\in S\\
\end{align*}

% The optimal policy has other properties of note; it always exists,
% it is unique, and it is deterministic \cite{Bertsekas1996, Putterman2015}.
% We will return to these properties in \ref{geom-polytope}.

\begin{displayquote}
  \textsl{But, how can we (efficiently) find the optimal policy?}
\end{displayquote}

If we randomly pick policies and evaluate them, we would need to test (in the worst case),
all the deterministic policies, $\mathcal O(|A|^{|S|})$ . However, we can use the \ref{eq:bellman-eqn} to
guide our search. The expected return can be rewritten in a recursive manner, to give the Bellman equations.

\begin{align*}
Q^{\pi}(s, a) &= r(s, a) + \gamma \mathop{\mathbb E}_{s' \sim \tau(\cdot|s, a)} [V^{\pi}(s')] \label{eq:bellman-eqn}\tag{Bellman equation}\\
V^{\pi}(s) &= \mathop{\mathbb E}_{a \sim \pi(\cdot|s)} [Q^{\pi}(s, a)]
\end{align*}

Where $Q$ is the state-action values.
The Bellman equation is sometimes written as an operator, $T$.

\begin{align*}
T(Q^{\pi}) = r(s, a) + \gamma \mathop{\mathbb E}_{s' \sim \tau(\cdot|s, a)}\mathop{\mathbb E}_{a'\sim \pi(\cdot | s')} [Q(s', a')]
\end{align*}

\subsection{Complexity}

\begin{displayquote}
  \textsl{How hard is it to find the optimal policy?}
\end{displayquote}

% Insert lower bound and some intution
The complexity of estimating the value of a state-action under the optimal policy, ie solving the Bellman optimality
equation, can be glimpsed if we unroll its recursive definition.
Here we can see a series of nested maximisation problems, where the former
maximisation problems are conditional on the results of the latter maximisation problems.

\begin{align*}
Q^{\pi}(s_0, a_0) = r(s_0, a_0) &+ \gamma \mathop{\text{max}}_{a_1} \mathop{\mathbb E}_{s_1\sim p(\cdot | s_0, a_0)} \Bigg[ \\
r(s_1, a_1)  &+ \gamma \mathop{\text{max}}_{a_2} \mathop{\mathbb E}_{s_2\sim p(\cdot | s_1, a_1)} \bigg[\\
r(s_2, a_2)  &+ \gamma \mathop{\text{max}}_{a_3} \mathop{\mathbb E}_{s_3\sim p(\cdot | s_2, a_2)} \Big[
\dots \Big] \bigg] \Bigg]
\end{align*}

% {\color{red}TODO add some complexity bounds}

For the final maximisation problem we need to find the best action ($|A|$) for each potential final state we might be in ($|S|$).
Then we need to do this again for each maximisation problem (of which there are $|S|$).
So the computational complexity is $\mathcal O(|S|^2|A|)$.

\section{A tabular representation of MDPs}

Back to constructing a simple RL setting.

Imagine a MDP that can be described with tables (aka arrays). A table of
three dimensions can describe the transition probabilities, $P[s_{t+1}, s_t, a_t]$,
and a table of two dimensions can describe the rewards, $r[s_t, a_t]$: the
states and actions act as indexes to locations in the tables.
Let's formally define our tabular MDP. \footnotemark

\footnotetext{It should be noted that this tabular MDP setting ignores an important
aspect of RL: exploration, estimation error.}

\begin{align}
\mathcal M &= \{S, A, P, r, \gamma\}\; \tag{the MDP}\\
S &= [0:n-1] \tag{the state space}\\
A &= [0:m-1] \tag{the action space}\\
\tau &\in [0,1]^{n\times n \times m}, \;\;\forall j, k : \sum_i \tau[i, j, k] = 1 \tag{the transition fn.}\\
r &\in \mathbb R^{n\times m} \tag{the reward fn.}
\end{align}

A result of this formulation is that we concisely write and solve the \eqref{eq:bellman-eqn}.
However, it should be noted that the ability to solve the Bellman equation analytically (via the value functional)
does not allow us to solve for the optimal policy analytically. The value functional allows us to evaluate a single policy.
To find the optimal policy, we many need to make many evaluations.

\begin{align}
V &= r_{\pi} + \gamma \tau_{\pi} V \tag{tabular Bellman eqn}\\
V &= (I-\gamma \tau_{\pi})^{-1}r_{\pi}  \label{eq:value-functional}\tag{Value functional}
\end{align}

The values are written as a vector, $V \in \mathbb R^n$.
The reward under a given policy is written $r_{\pi}[s, a] = \pi[s, a] r[s, a]$.
And the transitions under a given policy is written $\tau_{\pi}[s', s] = \sum_a \tau[s', s, a]\pi[s, a]$.

An alternative derivation of the value functional, which is more verbose and more enlightening, can be found in \ref{vf-neumann}.

\begin{displayquote}
\textsl{But why is the tabular MDP considered 'simple' enough?}
\end{displayquote}

Consider a MDP with deterministic actions, where $\tau(s_{t+1}|s_t, a_t) \in \{ 0, 1\}$.
This RL problem can be efficiently solved by non-statistical
methods: dynamic programming and related planning techniques \cite{Bertsekas1995}.
This setting is too simple.

Rather, a MDP with stochastic actions, $\tau(s_{t+1}|s_t, a_t) \in [0, 1]$,
seems to retain much of the complexity we care about: this setting does not allow
efficient solutions via dynamic programming. Further, it can be approached with algorithms
that are used for state-of-the-art deep RL such as;
policy gradients \cite{Schulman2015a} and Q-learning \cite{Mnih2015}.
