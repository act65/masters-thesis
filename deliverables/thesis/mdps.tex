\chapter{Markov Decision Problems}

Reinforcement learning (RL) refers to the set of solutions to a type of problem.
This general, reinforcement learning, problem, has two main properties;
\textit{"trial-and-error search and delayed rewards"} \cite{Sutton2018}.

Unlike supervised learning, which gives the learner feedback (\textit{Student: "I think that digit
is a 5". Teacher: "No, it's a 6"}), in RL the learner only receives evaluations (\textit{Student: "I think
that digit is a 5". Teacher: "No."}). This means the student needs to explore the possible answers via some trial-and-error search.
(\textit{Student: "Is it a 4?". Teacher: "No." Student: "How about a 0?". Teacher: "No." ... Student: "A 6?". Teacher: "Yes."})

Ontop of terse teachers, many actions may be taken before any evaluation is received, thus requiring credit to be assigned to past actions,
(\textit{Student: "Is it a 4? How about a 0? A 6? Maybe a 7?". ... Teacher: "No".})
often leaving the learner wondering: "what did I do to deserve this?" (see
\href{https://www.youtube.com/watch?v=Qv4H81gEGDQ}{pigeon superstition} for an amusing
example of credit assignment gone wrong \cite{Box1997}).

% Your teaher might only give you evaluations for sequences of actions, rather than individual actions.
% Thus you are left with trying to infer how these sequence evaluations tells you about which actions you should take.

\vspace{5mm}

The above definition of reinforcement learning is quite general. There are many
different dimensions to problems that require trial-and-error search and give
delayed rewards. For example we could make a RL problem that is;

\begin{itemize}
\tightlist
\item
  Observable or un-observable \cite{Kaelbling1998}
\item
  Deterministic or stochastic \cite{Putterman2015}
\item
  Synchronous or asynchronous \cite{Bertsekas1995}
\item
  Terminating or infinite \cite{Putterman2015}
\item
  Discrete versus continuous \cite{Bertsekas1995}
\item
  Given knowledge of the underlying model  or not\cite{Sutton1991}
\end{itemize}

\begin{displayquote}
  \textit{But, which setting should we study?}
\end{displayquote}

\begin{displayquote}
  A better question might be: \textit{What is the simplest setting we can
  consider that still poses an interesting challenge to the ML and / or RL communities?}
\end{displayquote}

Markov decision problems (MDPs) appear to be a good candidate. Let's go through some definitions so we an more clearly understand how they can be used as a
simple setting to analyse RL.

Formally, a MDP, which is a type of sequential decision problem, is defined
as a tuple, $\{\mathcal S, \mathcal A, P,r, \gamma, d_0\}$.
Where $S$ is the set of possible states (\textit{for example arrangements of chess pieces}),
$A$ is the set of actions (\textit{the different possible moves, left,
right, diagonal, L-shaped step, ...}),  $P: S \times A \to \Delta(S)$\footnotemark[16]
is the transition function which describes how the environment acts in response
to the current state and to actions (\textit{You play pawn to pawn to D4, in response your
opponent moves, knight to D4, taking your pawn.}). Next is the reward function, $r: S\times A \to \mathbb R$,
(\textit{whether you won (+1) or lost (-1) the game }).
Lastly, the policy, $\pi: S \to \Delta(A)$, is what the learner gets to choose, aka the learners strategy.
It decides which action to take in different states.

\footnotetext[16]{The notation $\Delta(S)$ represents a distribution over S.}

\vspace{5mm}

The objective when solving a MDP is to find a policy
that maximises the expected cumulative discounted reward $V^{\pi}$ (aka the value). This
can be written as, maximising the expected return.

\begin{align*}
V^{\pi} &= \mathop{\mathbb E}_{\zeta \sim D(\pi, \tau)} [R(\zeta)] \\
\pi^{* } &= \mathop{\text{max}}_{\pi}V^{\pi}
\end{align*}


Where, $d_0$ is the initial state distribution, $\zeta$ collects the $(s_t, a_t, r_t)$ triples of a game (aka trajectory or rollout) \footnotemark[17],
$R(\zeta) =\sum_{t=0}^H \gamma^t \zeta^r_t$ is cumulative discounted reward (aka the 'return' of a single game), and $D$ is the probability of a trajectory under the chosen policy and MDP.

\footnotetext[17]{We allow $\zeta$ to be indexed by time and $\{s, a, r\}$. For example; $\zeta_t^s=s_t$.}

\begin{align}
\zeta &= \{(s_t, a_t, r_t) : t \in [0, H]\} \tag{trajectory} \\
D(\zeta, \pi, \tau, d_0) &= d_0(\zeta^s_0) \prod_{t=1}^{\infty} \pi(\zeta^a_t|\zeta^s_t) \tau(\zeta^s_{t+1}|\zeta^s_t, \zeta^a_t) \tag{p($\zeta$)}
\end{align}

% If we wanted we could pick our actions before we make observations,
% reducing the search space to only \(|A| \times T\). But this is a bad idea\ldots{} example.

You can find examples of some simple MDPs in \ref{MDP-examples}.

\section{Sequential decision problems}

A general intuition for the problem of solving a sequential decision problem is: actions (aka decisions) are made
sequentially (\textit{e.g. first we put on our socks then we put on our shoes}).
These actions need to be conditioned on the current state of the world (\textit{e.g. It is morning and time to go to work.}).
The goal is to take actions that achieve higher rewards (\textit{e.g. Lying in bed is quite rewarding...}). While instantaneous
rewards are good, we really care about long term cumulative rewards (\textit{e.g. Having a job and thus being able to afford a bed is more rewarding.}).

% Maze with pendulums / doors. When moving through the maze, you must
% swing the pendulums. In the future you must avoid being hit. (maybe make
% a picture of this?) also, is there a more general way to think about it?

\begin{displayquote}
  \textit{What does the M in MDP really mean?}
\end{displayquote}

When we say a decision problem is Markovian, we mean that the transition
function generates a Markov chain \cite{Markov2006}. The next transition step depends only
on the current state and action. It is invariant to any and all histories that do not
change the current state. \footnotemark[18]

\footnotetext[18]{Or another way of saying the same thing, there is no hidden state
that effects future transitions.}

This is not to say that past actions do not effect the future. Rather,
it is a special type of dependence on the past. Where the dependence is
totally described by changes to the state, $s\in S$.

We can return to chess for an example: in chess there are no hidden pieces, or
private knowledge about the current state. I know everything there is to know.

% Can easily make a sequence Markovian by adding information. E.g. time

\section{Solving a MDP}

\begin{displayquote}
  \textit{What does it mean to solve a MDP?}
\end{displayquote}

A MDP is considered solved when we have found the 'optimal' policy. As above,
the 'optimal' policy is the policy that gives the highest expected return (value).
This means that;

\begin{align*}
\pi^{*} : \;\; V^{\pi^* }(s) \ge V^{\pi}(s) \quad \forall \pi\in \Pi \;\;\forall s\in S\\
\end{align*}

Which is implied by our earlier definition, but the optimal policy also has some other properties of note.\cite{Bertsekas1996}

\begin{itemize}
\tightlist
  \item it always exists,
  \item it is deterministic, {}
  \item it is unique, or are there many optimal policies?{\color{red}unsure about this}
\end{itemize}

{\color{red}do I need to explain these?}

\subsection{The Bellman Equations}

{\color{red}how to intro / motivate bellman and TD??}

The expected return (value) can also be rewritten in a recursive manner,
which is known as the Bellman equation.

\begin{align*}
Q^{\pi}(s, a) &= r(s, a) + \gamma \mathop{\mathbb E}_{s' \sim P(\cdot|s, a)} [V(s')] \label{eq:bellman-eqn}\tag{Bellman equation}\\
V^{\pi}(s) &= \mathop{\mathbb E}_{a \sim \pi(\cdot|s)} [Q^{\pi}(s, a)]
\end{align*}

The formulation suggests a potential way to solve a MDP, through generalise policy iteration.
Pick a policy randomly, estimate its value, apply the Bellman operator, to get state-action values.
Construct a new policy by acting greedily with resepct to the state-action value. Repeat.

\begin{align*}
\pi(a| s) &= \mathop{\text{argmax}}_a Q(s, a) \label{eq:greedy}\tag{greedy}\\
\end{align*}

% \begin{align}
% T(V) &= \mathop{\text{max}}_a \big[r + \gamma PV\big] \\
% \end{align}

\subsection{Complexity}

\begin{displayquote}
  \textit{How hard is it to find the optimal policy?}
\end{displayquote}

% Insert lower bound and some intution
The complexity of estimating the value of a state-action under the optimal policy, ie solving the Bellman optimality
equation, can be glimpsed if we unroll its recursive definition.
Here we can see a series of nested maximisation problems, where the former
maximisation problems are conditional on the results of the latter maximisation problems.

\begin{align*}
Q^{\pi}(s_0, a_0) = r(s_0, a_0) &+ \gamma \mathop{\text{max}}_{a_1} \mathop{\mathbb E}_{s_1\sim p(\cdot | s_0, a_0)} \Bigg[ \\
r(s_1, a_1)  &+ \gamma \mathop{\text{max}}_{a_2} \mathop{\mathbb E}_{s_2\sim p(\cdot | s_1, a_1)} \bigg[\\
r(s_2, a_2)  &+ \gamma \mathop{\text{max}}_{a_3} \mathop{\mathbb E}_{s_3\sim p(\cdot | s_2, a_2)} \Big[
\dots \Big] \bigg] \Bigg]
\end{align*}

% {\color{red}TODO add some complexity bounds}

For the final maximisation problem we need to find the best action ($|A|$) for each potential final state we might be in ($|S|$).
Then we need to do this again for each maximisation problem (of which there are $|S|$).
So the computational complexity is $\mathcal O(|S|^2|A|)$.

\section{A tabular representation of MDPs}

Back to constructing a simple RL setting.

Imagine a MDP that can be described with tables (aka arrays). A table of
three dimensions can describe the transition probabilities, $P[s_{t+1}, s_t, a_t]$,
and a table of two dimensions can describe the rewards, $r[s_t, a_t]$: the
states and actions act as indexes to locations in the tables.
Let's formally define our tabular MDP. \footnotemark[23]

\footnotetext[23]{It should be noted that this tabular MDP setting ignores an important
aspect of RL: exploration, estimation error.}

\begin{align}
\mathcal M &= \{S, A, P, r, \gamma\}\; \tag{the MDP}\\
S &= [0:n-1] \tag{the state space}\\
A &= [0:m-1] \tag{the action space}\\
P &\in [0,1]^{n\times n \times m}, \;\;\forall j, k : \sum_i P[i, j, k] = 1 \tag{the transition fn}\\
r &\in \mathbb R^{n\times m} \tag{the reward fn}
\end{align}

A result of this formulation is that we concisely write and solve the \eqref{eq:bellman-eqn}. \footnotemark[0]

\footnotetext[0]{Note that the ability to solve the Bellman equation analytically does not allow
us to solve for the optimal policy analytically.}

\begin{align}
V &= r_{\pi} + \gamma P_{\pi} V \tag{tabular Bellman eqn}\\
V &= (I-\gamma P_{\pi})^{-1}r_{\pi}  \label{eq:value-functional}\tag{Value functional}
\end{align}

The values are written as a vector, $V \in \mathbb R^n$.
The reward under a given policy is written $r_{\pi}[s, a] = \pi[s, a] r[s, a]$.
And the transitions under a given policy is written $P_{\pi}[s', s] = \sum_a P[s', s, a]\pi[s, a]$.

An alternative derivation of the value functional, which is more verbose and more enlightening, can be found in \ref{vf-neumann}.

\begin{displayquote}
\textit{But why is the tabular MDP considered 'simple' enough?}
\end{displayquote}

Consider a MDP with deterministic actions, where $P(s_{t+1}|s_t, a_t) \in \{ 0, 1\}$.
This RL problem can be efficiently solved by non-statistical
methods: dynamic programming and related planning techniques \cite{Bertsekas1995}.

Rather, a MDP with stochastic actions, $P(s_{t+1}|s_t, a_t) \in [0, 1]$,
seems to retain much of the complexity we care about: this setting does not allow
efficient solutions via dynamic programming. And it can be approached with algorithms
that are used for state-of-the-art deep RL such as;
policy gradients \cite{Schulman2015a} and Q-learning \cite{Mnih2015}.
