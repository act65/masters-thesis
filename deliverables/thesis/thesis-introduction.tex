\chapter{Introduction}\label{C:intro}

Blah blah. \href{https://arxiv.org/abs/1904.12901}{Challenges of
Real-World Reinforcement Learning}. - Learning on the real system from
limited samples. - High-dimensional continuous state and action spaces.

\hypertarget{reinforcement-learning}{%
\section{Reinforcement learning}\label{reinforcement-learning}}

\begin{quote}
Reinforcement learning (RL) defines a type of problem, closely related
to Markov decision problems (MDPs).
\end{quote}

A Markov decision problem is defined as the tuple,
\(\{\mathcal S, \mathcal A, P, r\}\). Where \(s \in \mathcal S\) is the
set of possible states (\emph{for example arrangements of chess
pieces}), \(a \in \mathcal A\) is the set of actions (\emph{the
different possible moves, left, right, diagonal, weird L-shaped thing,
\ldots{}}),
\(P: \mathcal S\times \mathcal A \times \mathcal S \to [0:1]\) is the
transition function which describes how the environment acts in response
to the past (\(s_t\)) and to your actions (\(a_t\)) (\emph{in this case,
your opponent's moves, taking one of your pieces, and the results of
your actions}), and finally,
\(r: \mathcal S\times \mathcal A \to \mathbb R\) is the reward function,
(\emph{whether you won (+1) or lost (-1) the game}) and
\(R = \sum_{t=0}^T \gamma^t r(s_t, a_t)\) is the discounted cumulative
reward, or return. The player's goal, is to find a policy \(\pi\),
(which chooses actions, \(a_t = \pi(s_t)\)) that yields the largest
return (\(\text{max } R\)).

A RL problem is an extension of the MDP definition adove. Where, rather
than the learner being provided the state space, action space,
transition function and reward function
(\(\{\mathcal S, \mathcal A, P,r\}\)), the learner recieves samples
\((s_t, a_t, r_t)\). From these samples the learner can either; -
attempt to infer the transition and reward functions (known as
model-based reinforcement learning), or attempt to estimate value
directly (model-free reinforcement learning). - collect the samples in
memory and use them to find a policy (offline learning), or - on / off
policy - bootstrap / not - types of model (fn approximators)

For example \_``Dynamic programming is one type of RL. More
specifically, it is a value-based, model-based, bootstrapping and
off-policy algorithm. All of those traits can vary. Probably
the''opposite" of DP is REINFORCE which is policy-gradient, model-free,
does not bootstrap, and is on-policy. Both DP and REINFORCE methods are
considered to be Reinforcement Learning methods.``\_
\href{https://datascience.stackexchange.com/questions/38845/what-is-the-relationship-between-mdp-and-rl}{SE}

\hypertarget{understanding-theoretical-reinforcement-learning}{%
\subsubsection{Understanding Theoretical Reinforcement
learning}\label{understanding-theoretical-reinforcement-learning}}

What are its goals. Its definitions. It methods?

\begin{itemize}
\tightlist
\item
  Optimality
\item
  Model based
\item
  Complexity
\item
  Abstraction
\end{itemize}

Recent work has bounded the error of representation learning for RL.
\href{}{Abel et al.~2017}, \href{}{Abel et al.~2019}

But. It is possible that this representation achieves no compression of
the state space, making the statement rather vacuous. Further more, it
consider how easy it is to find the optimal policy in each of the two
representations. It is possible to learn a representation that makes the
optimal control problem harder. For example. TODO

Current theory does not take into account the structure within a RL
problem.

The bounds are typically for the worst case. But these bounds could be
tighter if we exploited the structure tht exists in natural problems.
The topology of the transition function; its, sparsity, low rankness,
locality, The symmetries of the reward function. ??? (what about both?!)

\hypertarget{understanding-markov-decision-problems}{%
\subsubsection{Understanding Markov decision
problems}\label{understanding-markov-decision-problems}}

\begin{itemize}
\tightlist
\item
  Properties of the polytope
\item
  Search dynamics on the polytope
\item
  ??? LPs? Convergence? Exploration? \ldots{}?
\end{itemize}

\hypertarget{abstraction}{%
\subsubsection{Abstraction}\label{abstraction}}

\begin{itemize}
\tightlist
\item
  Near optimal representations
\item
  Solvable representations (LMDPs)
\item
  Invariant representations (TODO)
\end{itemize}

\hypertarget{algorithms}{%
\subsubsection{Algorithms}\label{algorithms}}

We explore four algorithms.

\begin{itemize}
\tightlist
\item
  Memorizer: This learner memorizes everything it sees, and uses this
  knowledge as an expensive oracle to train a policy.
\item
  Invariant. This learner discovers symmetries in its evironment and
  uses this knowledge to design an invariant representation.
\item
  Tabular. \ldots{}
\item
  MPC. \ldots{}
\end{itemize}


\hypertarget{related-work}{%
\section{Related work}\label{related-work}}

\hypertarget{mdps}{%
\paragraph{MDPs}\label{mdps}}

Dynamic programming, linear programming, \ldots{}?

\[
Q^{\pi}(s_0, a_0) = r(s_0, a_0)
+ \gamma \mathop{\text{max}}_{a_1} \mathop{\mathbb E}_{s_1\sim p(\cdot | s_0, a_0)} \Bigg[ r(s_1, a_1)
+ \gamma \mathop{\text{max}}_{a_2} \mathop{\mathbb E}_{s_2\sim p(\cdot | s_1, a_1)} \bigg[r(s_2, a_2)
+ \gamma \mathop{\text{max}}_{a_3} \mathop{\mathbb E}_{s_3\sim p(\cdot | s_2, a_2)} \Big[
\dots \Big] \bigg] \Bigg]
\]

\hypertarget{hrl}{%
\subparagraph{HRL}\label{hrl}}

Temoral abstractions of actions.(how does this related to a
decomposition of rewards) Ok, so we wany a multiscale representation?
Understanding how actions combine (this is necessary knowledge for HRL?)

Reasons to do HRL??? (want to verify these claims - and have refs for
them)

\begin{itemize}
\item
  credit assignment over long time periods (learning faster in one env)
\item
  exploration
\item
  transfer
\item
  To learn action abstractions they must capture info about the model.
  How much harder is it to learn action abstractions in model-free vs
  model-based settings?
\item
  Reward as a function of a subspace of the state space. (this is
  important for learning abstract representations and actions!?)
\item
  What do cts linear heirarchical actions look like!? and their loss
  surface!?
\item
  \href{https://arxiv.org/abs/1612.02757}{HLMDPs}
\item
  \href{https://arxiv.org/abs/1812.00025}{Modulated policy heirarchies}
\item
  \href{https://arxiv.org/abs/1810.10096}{Model free representations for
  HRL}
\item
  \href{https://blog.aqnichol.com/2019/04/03/prierarchy-implicit-hierarchies/}{Prierarchy:
  Implicit Hierarchies}
\item
  Options
\item
  \href{https://openreview.net/forum?id=H1emus0qF7}{Near optimal
  representation learning for heirarchical RL}
\end{itemize}

Relation to pretraining / conditioning?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Why does Heirarchy (sometimes) work so well in reinforcement learning?

The authors claim that the benefits of HRL can be explained by better
exploration. However, I would interpret their results as saying; ``for
2D environments with walls, larger steps / actions result in greater
explration''. But what if the walls were replaced by cliffs? I imagine
this algorithm would do a lot worse!?

They also seem to misunderstand the main problem with HRL, discovery.
Once you have discovered a nice set of abstracted actions / a
representation, then yeah, you get faster reward propagation, better
exploration, \ldots{} etc.

\hypertarget{dynamic-programming}{%
\paragraph{Dynamic programming}\label{dynamic-programming}}

What is it? Memoized search. Why should we care?

\hypertarget{model-based-rl}{%
\subsection{Model-based RL}\label{model-based-rl}}

Pros and cons.

Model-based learning can be bad\ldots{} There may be many irrelevant
details in the environment that do not need to be modelled. A model-free
learning naturally ignores these things.

The importance of having an accurate model!

For example, let \(S\in R^n\) and \(A\in [0, 1]^n\). Take a transition
function that describes how a state-action pair generates a distribution
over next states \(\tau: S \times A \to \mathcal D(S)\). The reward
might be invariant to many of the dimensions.
\(r: X \times A -> \mathbb R\), where \(X \subset S\).

Thus, a model mased learner can have arbitrarily more to learn, by
attempting to learn the transition function. But a model-free learner
only focuses on \ldots{}

This leads us to ask, how can we build a representation for model-based
learning that matches the invariances in the reward function. (does it
follow that the invariances in reward fn are the invariances in the
value fn. i dont think so!?)

Take \(S \in R^d\) and let \(\hat S = S \times N, N \in R^k\). Where
\(N\) the is sampled noise. How much harder is it to learn
\(f: S \to S\) versus \(\hat f: \hat S \to \hat S\)?

https://arxiv.org/pdf/1903.00374v3.pdf https://arxiv.org/abs/1907.02057

\hypertarget{representation-learning-and-abstraction}{%
\subsection{Representation learning and
abstraction}\label{representation-learning-and-abstraction}}

The goal is to find a representation that decomposes knowledge into its
parts.

Another way to frame this is: trying to find the basis with the right
properties.

\begin{itemize}
\tightlist
\item
  sparsity,
\item
  independence,
\item
  multi scale,
\item
  locality/connectedness
\item
  ???
\end{itemize}
