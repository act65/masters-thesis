\chapter{Introduction}\label{C:intro}

% RL is inefficient
Reinforcement learning has an efficiency problem: AlphaGo \cite{Silver2016a}, the Go
playing AI that beat world champion Lee Sedol, needed 1.28 million games, with
extra supervision from another 29.4 million positions, using 50 GPUs.
Libratus \cite{Brown2018b}, the poker playing AI that beat a table of professionals,
evolved its poker-playing strategy by running for 15 million processor-core-hours.
OpenAI Five \cite{Berner2019}, the Dota 2 playing AI that beat OG, the winners of TI8/9, was
trained over 10 months, at it peak, collecting 900 years of experience per day, using
128,000 CPUs and 256 GPUs. Is RL fundamentally expensive, or can we do better?

% Existing theory tells us ...
Bounds on sample and computational complexity tell us that !??!

% What strategies are there to improve the efficiency of RL?


Existing theory has derived bounds (such as...). But these bounds are quite pessimistic (loose / not to be trusted) because;
don't exploit the structure within 'natural' MDPs. Sparsity, symmetry, continuity, ...

\section{Overview}

???

\section{Timeline}

???


\section{Contributions}

\begin{itemize}
  \item A way to visualise the dynamics of MDPs in higher dimensions. [ref to Graphs]
  \item Generalise the classes of abstraction to action and temporal abstraction []
  \item Construct a framework for reasoning about similarity in RL. []
  \item Construct three new tasks for understanding a learners ability to generalise []
  \item Show that an existing method of abstraction doesn't work in general. []
\end{itemize}
