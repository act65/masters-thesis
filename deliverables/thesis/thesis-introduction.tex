\chapter{Introduction}\label{C:intro}

% RL is inefficient
Reinforcement learning has an efficiency problem: AlphaGo \cite{Silver2016a}, the Go
playing AI that beat world champion Lee Sedol, needed 1.28 million games, with
extra supervision from another 29.4 million positions, using 50 GPUs.
Libratus \cite{Brown2018b}, the poker playing AI that beat a table of professionals,
evolved its poker-playing strategy by running for 15 million processor-core-hours.
OpenAI Five \cite{OpenAI2018}, the Dota 2 playing AI that beat OG, the winners of TI8/9, was
trained over 10 months, at it peak, collecting 900 years of experience per day, using
128,000 CPUs and 256 GPUs. Is RL fundamentally expensive, or can we do better?

% Existing theory tells us ...
Bounds on sample and computational complexity tell us that !??!

% What strategies are there to improve the efficiency of RL?
