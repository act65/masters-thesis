\chapter{Introduction}\label{C:intro}

% RL is inefficient
Reinforcement learning has an efficiency problem: AlphaGo \cite{Silver2016a}, the Go
playing AI that beat world champion Lee Sedol, played 1.28 million games, with
extra supervision from another 29.4 million positions, using 50 GPUs.
Libratus \cite{Brown2018b}, the poker playing AI that beat a table of professionals,
constructed its poker-playing strategy over 15 million processor-core-hours.
OpenAI Five \cite{Berner2019}, the Dota 2 playing AI that beat OG, the winners of The International 8 and 9, was
trained over 10 months, at its peak, it collected 900 years of experience per day using
128,000 CPUs and 256 GPUs.

\begin{displayquote}
\textit{Is reinforcement learning fundamentally inefficient, or can we do better?}
\end{displayquote}


\section{Our Motivation}

We think (more) efficient reinforcement learning might be achieved by the use of abstraction.
Abstractions allow a learner to preserve essential structure, while discarding inessential details.

\begin{displayquote}
\textit{By throwing away inessential details, there is less to compute. \newline
By throwing away inessential details, we don't need to explore them. \newline
By throwing away inessential details, we reduce the variance of our \newline
observations (allowing quicker learning).}
\end{displayquote}

% 1) Abstractions appear to essential for intelligent behaviour (ref some psyc / neuro paper?!).
%
% 2) Dimensionality reduction is not enough. Want to be able to preserve the important structure.
%
% % What strategies are there to improve the efficiency of RL?
%
% %  research question goes here!

Thus, the main goal of this thesis is to understand:

\begin{displayquote}
\textit{to what extent can abstractions increase the efficiency of reinforcement learning?}
\end{displayquote}

\section{Overview}

Given the goal above, we pick Markov Decision Problems as our setting for studying abstractions\footnotemark[43] and give a slightly non-standard, but general introduction to them, see section \ref{mdps}.
Then we explore abstractions and existing theory for understanding abstraction and attempt to organise it in to a clearer framework, see section \ref{abstraction-rl}.
Next, we analyse an existing method of abstraction for efficient control and find that it does not work in general, see section \ref{solveable-abstractions}.
Finally, we build an abstraction using symmetries, see section \ref{symmetric-abstractions}.

\footnotetext[34]{In general, when we state abstractions, we mean abstractions for reinforcement learning.}

\section{Contributions}

In this thesis, we;

\begin{itemize}
  \tightlist
  % \item Construct a family of similarity measures for building abstractions for RL.
  \item Clarify that a well cited method of linear abstraction doesn't work in general. \ref{lmdp-validation}
  \item Build a Thompson sampler that prefers symmetric MDPs. \ref{thompson-sampling}
  % \item Explore methods of discovering symmetries and a complexity measure \ref{????} {\color{red}WIP}
  % \item Generalise the notion of Markov decision problem homomorphism to temporal abstractions \ref{temporal-abstraction} {\color{red}WIP}
  \item We explore the iteration complexity, effect of discounting, and the density of the Value functional polytope. \ref{polytope-extras}
  \item Provide a way to visualise the dynamics of learning an Markov decision problem in higher dimensions \ref{graph-vis}. It's utility is unclear.
  \item Formulate a type of model based learner \ref{model-iteration}, which has some serious, but potentially solvable, issues with scaling to larger problems.
  \item Explore a new task for understanding a learners ability to generalise \ref{action-space-experiments}
\end{itemize}
