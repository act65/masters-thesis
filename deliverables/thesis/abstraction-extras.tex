\chapter{Abstraction}

\section{Similarity}

\subsection{Pairing abstractions with similarity measures}

% we have a similarity measure and a way to allow the learner to use it. now ...

Want to remove unimportant structure, while keeping the structure that allows us
to exploit the bellman equation (and thus more efficient search).

Different abstraction spaces support different similarity measures.
State similarity based on value fails in rather simple settings.
But works with state-action abstractions!?

Weaker notions of similarity.
\begin{itemize}
\tightlist
  \item Preserving the ordering of policies. Rather than their absolute value.
  \item Preserving ordering of value. $\phi(s) > \phi(s') \implies Q_{\pi}(s, a) > Q_{\pi}(s', a)$
  \item Preserving the neighbourhoods of policies and their values.
  \item ?
\end{itemize}


\section{LMDPs}

\subsection{LMDP solutions}\label{lmdp-derivation}

% Pick $a \in A$, versus, pick $\Delta(S)$. $f: S\to A$ vs $f:S \to \Delta(S)$.

In the original Todorov paper \cite{Todorov2009}, they derive the LMDP equations for minimising a cost function. This maximisation derivation just changes a few negative signs around.
% Although there is also a subtle change in the interpretation of what the unconstrained dynamics are doing. (??? explain)

\begin{align*}
V(s) &= \mathop{\text{max}}_{u} q(s) - \text{KL}(u(\cdot| s) \parallel p(\cdot | s)) + \gamma \mathop{\mathbb E}_{s' \sim u(\cdot | s)} V(s') \tag{1}\\
\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s) }{ u(s' | s)}+\gamma \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \big[V(s')\big] \bigg] \tag{2}\\
\log(z_{u^{* }}(s)) &= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s) }{ u(s' | s)}+\mathop{\mathbb E}_{s' \sim u(\cdot | s)} \big[\log(z_{u^{* }}(s')^{\gamma})\big] \bigg] \tag{3}\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s)z_{u^{* }}(s')^{\gamma} }{ u(s' | s)} ) \bigg] \tag{4}\\
G(s) &= \sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma} \tag{5}\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s)z_{u^{* }}(s')^{\gamma} }{ u(s' | s)} \cdot \frac{G(s)}{G(s)} ) \bigg] \tag{6}\\
&= q(s) + \log G(s) + \mathop{\text{min}}_{u} \bigg[\text{KL}\big(u(\cdot | s) \parallel \frac{p(\cdot | s)\cdot z_{u^{* }}(\cdot)^{\gamma}}{G(s)} \big) \bigg] \tag{7}\\
u^{* }(\cdot | s) &= \frac{p(\cdot | s)\cdot z_{u^{* }}(\cdot)^{\gamma}}{\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}} \tag{8}\\
\log(z_{u^{* }}(s)) &= q(s) + \log \big(\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}\big) \tag{9}\\
z_{u^{* }}(s) &= e^{q(s)}\big(\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}\big) \tag{10}\\
z_{u^{* }} &= e^{q(s)}\cdot P z_{u^{* }}^{\gamma} \tag{11}\\
\end{align*}

By definition, an LMDP is the optimisation problem in (1). We can move the $\text{max}$ in (2), as $q(s)$ is not a function of $u$. Also in (2), expand the second term using the definiton of KL divergence, the negative from the KL cancels the second terms negative. (3) Define a new variable, $z(s) = e^{v(s)}$. Also, use the log rule to move the discount rate. (4) Both expectations are under the same distribution, therefore they can be combined. Also, using log rules, combine the log terms. (5) Define a new variable that will be used to normalise $p(s' | s)z(s')^{\gamma}$. (6) Multiply and divide by $G(s)$. This allows us to rewrite the log term as a KL divergence as now we have two distributions, $u(\cdot | s)$ and $\frac{p(\cdot | s)z(\cdot)^{\gamma}}{G(s)}$. (7) The change to a KL term introduces a negative, instead of maximising the negative KL, we minimise the KL. Also in (7) the extra G(s) term can be moved outside of the expectation as it is not dependent in $s'$. (8) Set the optimal policy to minimise the KL distance term. (9) Since we picked the optimal control to be the form in (8), the KL divergence term is zero. (10) Move the log. (11) Rewrite the equations for the tabular setting, where $z$ is vector, and the uncontrolled dynamics are a matrix.

\subsection{MDP Linearisation}\label{mdp-Linearisation}

The ability to solve LMDPs is great, but it's only useful if we can map MDPs into LMDPs, solve them, and map the solution back.
Our goal here is to find a LMDP that has 'similar' structure to the original MDP we were given.\footnotemark[1]

\footnotetext[1]{This derivation is not the same as in Todorov. He sets $b_a \neq r, b_a = r - \sum P \log P$.}

\begin{align*}
\forall s, s' \in S, \forall a \in A, \exists u_a& \;\;\text{such that;} \tag{1}\\
P(s' | s, a) &= u_a(s'|s)p(s'|s) \tag{2}\\
r(s, a) &= q(s) - \text{KL}(P(\cdot | s, a) \parallel u_a(\cdot| s) ) \tag{3}\\
\\
r(s, a) &= q(s) - \text{KL}(P(\cdot | s, a)\parallel\frac{P(\cdot | s, a)}{p(\cdot|s)}) \tag{4}\\
r(s, a) &= q(s) - \sum_{s'}P(s' | s, a) \log(p(s'|s)) \tag{5}\\
\\
m_{s'}[s]&:= \log p(s' | s) \tag{6}\\
D_{as'}[s] &:= p(s'|s, a) \tag{7}\\
c_{s'}[s] &:= q[s] \mathbf 1 - m_{s'}[s] \;\;\text{such that} \;\; \sum_{s'} e^{m_{s'}[s]} = 1 \tag{8}\\
\\
r_a &= D_{as'} ( q \mathbf 1 - m_{s'}) \;\;\forall s \tag{9}\\
r_a &= D_{as'}c_{s'}  \;\;\forall s \tag{10}\\
c_{s'} &= r_aD_{as'}^{\dagger} \;\;\forall s\tag{11}\\
q &= \log \sum_{s'} e^{c_{s'}} \;\;\forall s\tag{12}\\
m_{s'} &= q - c_{s'} \;\;\forall s\tag{13}\\
\end{align*}

We want to pick $p, q$ such that the dynamics of every action in the original MDP can be represented with a control (2),
and every reward generated by an action, can be given by a combination of the
state rewards and the $\text{KL}$-divergence between the true dynamics and a control (3).
Combine (2), (3) to yeild (4). Expand the definition of $\text{KL}$-divergence to get (5).
Now, we move to a tabular representation., where $m_{s'}[s]$ and $c_{s'}[s]$ are vectors, and
$D_{as'}[s]$ is a matrix, defined in (6), (7), (8). With these new definitions, we can rewrite equation (5)
as (9). The expectation can be moved to include $q$ because it sums to one.
Substitute equation (8) into (9) to get (10). Solve the linear equation in (11) to get the value of $c_{s'}$.
Use the value of $c_{s'}$ to calculate the state rewards and unconditioned dynamics by using equations (12), (13) and (6).

\section{Symmetry}

\subsection{Efficient inference using symmetries}\label{symmetry-inference}

\paragraph{Output coupling}

Mahajan et al. propose to use knowledge similarity to share couple outputs.
\cite{Mahajan2017}

\begin{align*}
\chi: (S\times A) \times (S\times A) \to [0, 1] \\
\mathcal L(\theta) = \mathop{\mathbb E}_{\chi} \Big[(Q(s, a, \theta)-Q(s', a', \theta))^2 \Big]
\end{align*}

Coupling the outputs of similar state-actions. This is nice because, we might know
that $s, a$ and $s', a'$ are similar. Yet we many not have observed $s', a'$.

% Equivalent to sharing training data?!?

However, how did we know that $s, a$ and $s', a'$ are similar without seeing $s', a'$?

- Data sharing (aka duplicate tuples \cite{Ho2019a})

\paragraph{Gradient coupling}

\begin{align*}
\nabla_{\theta} \ell(s, a, \theta) = \nabla_{\theta} \ell(s', a', \theta) \\
\chi \cdot \nabla_{\theta} \mathcal L(\theta)
\end{align*}

If $\chi$ has the structure $\chi((s, a), (s', a')) = \langle \nabla_{\theta}f(s, a, \theta), \nabla_{\theta} f(s', a', \theta) \rangle$ then will couple gradient in a certain way. NTK.

\cite{Ho2019a}

\paragraph{Data augmentation}

Sharing targets between 'similar' inputs.

\paragraph{Architecture}

Weight sharing?!? Weight coupling!?
\cite{Ravanbakhsh2017a,Abdolhosseini}
\cite{Anselmi2019}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{displayquote}
\textit{Which method of incorporating knowledge of symmetries is best?}
\end{displayquote}

\begin{itemize}
\tightlist
  \item Data augmentation: flips.
  \item Gradient coupling: ??? gets complicated?
  \item Output coupling: regulariser on similar outputs.
  \item Architecture: mirror weights.
\end{itemize}

Equivalent?! (in this case, or in general?)
What other ways are there? Want to construct the space of possible ways to incorporate symmetries into a learner.


\subsection{Cart pole} \label{game-invariants}

\subsubsection{Mirror symmetry}

What are the invariants of the mirror cart pole example?

\begin{itemize}
	\tightlist
	\item the expected return is invariant to
	\item the Bellman residual is invariants to ...
	\item the reachable rewards are invariant to flips around the mirror plane
\end{itemize}
There is also an equivariant quantity, the change in state (assuming the representation of the state is equal to ...)


\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-mirror.png}
	\caption{Two mirror symmetric states of a cart pole.}
\end{figure}

Define the action of $f \in S_2$ on the;

\begin{itemize}
	\tightlist
	\item state space $f \circ s = -s$
	\item action space $f \circ a = -a$
 	\item policy to be $f \circ \pi(a | s) = \pi(f \circ a | f \circ s)$.
	\item transition function to be $(f \circ P)(\cdot | s, a) = P(\cdot| f \circ s, f \circ a)$. ???
\end{itemize}

Therefore, we can write $(f \circ Q^{\pi})(s, a) = Q^{f \circ \pi}(f \circ s, f \circ a)$.
We can write the properties above as;

\begin{align*}
Q^\pi(s, a) &= (f \circ Q^{\pi})(s, a) \tag{expected return}\\
T(Q^\pi)(s,a) - Q^\pi(s,a) &=T(f \circ Q^\pi)(s, a) - (f \circ Q^\pi)(s,a) \tag{Bellman residual}\\
\mathop{\mathbb E}_{s' \sim P(\cdot| s, a)} (s' - s) &= f \circ \mathop{\mathbb E}_{s' \sim (f \circ P)(\cdot| s, a)} (s' - f \circ s) \tag{change in state}\\
\end{align*}
\footnotemark[14]

The rewards (/ value) reachable from $s_1$ are also reachable from $s_2$ (which also implies the converse).
\footnotetext[14]{Note, this assumes we have a 'nice' state representation where differences make sense}

{\color{red}TODO verify these invariants!?!?}

Is this combination of invariants unique to the mirror symmetry? (how to prove this?!)

\subsubsection{Translational symmetry}

What are the invariants of the translated cart pole example?

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-translation.png}
\caption{Two similar cart poles, which are only a translation different from each other.}
\end{figure}

(special case of 'regular actions')

How to prove that this is a cts symmetry!?

Define the action of $g \in R$ on the;

\begin{itemize}
	\tightlist
	\item state space $g \circ s = g+s$
	\item action space $g \circ a = a$
 	\item policy to be $g \circ \pi(a | s) = \pi(a | g \circ s)$.
	\item transition function to be $(g \circ P)(\cdot | s, a) = P(\cdot| g \circ  s,  a)$. ???
	\item value function to be $(g \circ  Q^{\pi})(s, a) = Q^\pi(g \circ  s,  a)$
\end{itemize}

{\color{red}Huh, the key is the action. These have the same invariants!??!}

\begin{align}
Q^\pi(s, a) &= (g \circ Q^{\pi})(s, a) \tag{expected return}\\
T(Q^\pi)(s,a) - Q^\pi(s,a) &=T(g \circ Q^\pi)(s, a) - (g \circ Q^\pi)(s,a) \tag{Bellman residual}\\
\mathop{\mathbb E}_{s' \sim P(\cdot| s, a)} (s' - s) &= f \circ \mathop{\mathbb E}_{s' \sim (f \circ P)(\cdot| s, a)} (s' - f \circ s) \tag{change in state}\\
\end{align}

\subsubsection{Future translational symmetry}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-state.png}
\caption{???}
\end{figure}

Different states, different actions, result in the same next state.
(ahh. $Q$ values are not equivalent if we are have a reward function that punishes larger actions!)

\begin{itemize}
\tightlist
  \item the state space to be $h\circ s = ??$ (related to lagrange transform / permutation!? conservation of momentum?!)
  \item the action space to be $h\circ a = ??$
  \item the transition function to be $h\circ P(\cdot|s, a) = P(\cdot|h \circ s, h \circ a)$
  \item the reward function to be $(h \circ r)(s, a) = $
  \item the value function to be $(h\circ Q)(s, a) = (h \circ r)(s, a) + \gamma \mathop{\mathbb E}_{s' \sim (h\circ P)(\cdot|s, a)} V(s')$
\end{itemize}

This implies. $Q(s, a) = $


\subsubsection{Temporal symmetry}

{\color{red} this one seems hard. not sure i am going to be able to explicitly construct this one?!}


\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-temporal-approx.png}
\caption{???}
\end{figure}

\begin{align}
p(\cdot|s_1, \omega_1) = p(\cdot|s_1, \omega_2) \\
Q_{\pi}(s_1, \omega_1) = Q_{\pi}(s_1,\omega_2) \\
\end{align}


\subsection{Pong}

\subsubsection{Mirror symmetry (vertical)}

{\color{red}Need to be more careful with the state / action spaces here. Need to define properly!?}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-vert-flip.png}
\caption{???}
\end{figure}


Define the action of $f \in S_2$ on the;

\begin{itemize}
	\tightlist
	\item state space $f \circ s = I(x, -y)$
	\item action space $f \circ a = -a$
 	\item policy to be $f \circ \pi(a | s) = \pi(f \circ a | f \circ s)$.
	\item transition function to be $(f \circ P)(\cdot | s, a) = P(\cdot| f \circ s, f \circ a)$. ???
	\item value function to be $(f \circ Q^{\pi})(s, a) = Q^{f \circ \pi}(f \circ s, f \circ a)$
\end{itemize}

\begin{align}
\Delta_{T}(s, a) = (T \circ Q)(s,a) - Q(s,a)\\
\Delta_{T}(s_1, a_1) = \Delta_{T}(s_2, a_2) \\
\end{align}

\begin{align}
\forall a \text{ set}\;\;\pi(a | s_1) = \pi(-a| s_2) \\
Q_\pi(s_1, a_1) = Q_\pi(s_2, a_2) \\
Q_\pi(s_1, a_2) = Q_\pi(s_2, a_1) \\
\end{align}

\hypertarget{mirror-symmetry-horizontal}{%
\paragraph{Mirror symmetry
(horizontal)}\label{mirror-symmetry-horizontal}}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-horz-flip.png}
\caption{???}
\end{figure}

Upon pretending to play as your opponent (flipping the image and
inverting the colors, via \(\rho: O \to O\), and ??? the actions)

\begin{align}
Q(s_1, a_1) = - V(\rho(s_2)) \\
\end{align}

(\emph{this really requires you to disentangle the model from your
opponent!?})

\begin{align}
\tau(s'|s, a_{p=0}) &= f_{p=0}(s''|s, a) \cdot f_{p=1}(s'|s'', \hat a) \cdot \pi_{p=1}(\hat a|s'') \\
Q_{p=0}(s, a) &= -Q_{p=1}(s, a)
\end{align}

\hypertarget{translational-symmetry-1}{%
\paragraph{Translational symmetry}\label{translational-symmetry-1}}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-trans.png}
\caption{???}
\end{figure}

(\emph{although there are boundary cases which cannot be ignored. how
can they be dealt with!?})

\begin{align}
\forall a: Q_{\pi_1}(s_1, a) = Q_{\pi_2}(s_2, a) \\
\forall a, t: \pi_1(a|s^t_{s^0=s_1}) = \pi_2(a|s^t_{s^0=s_2})
\end{align}

If we take the same actions, in translated states, we get the same
outcome (up to the boundary conditions).

\hypertarget{temporal-symmetries}{%
\paragraph{Temporal symmetries}\label{temporal-symmetries}}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-reach.png}
\caption{???}
\end{figure}

\begin{align}
\exists \pi_1, \pi_2 \;\;\text{s.t.} \;\; Q^{\pi_1}(s_1, a_1) = Q^{\pi_2}(s_2, a_2)
\end{align}

The same future state can be reached, and thus the same rewards can be
achieved.
