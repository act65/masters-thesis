\chapter{Abstraction}

\section{LMDPs}

\subsection{LMDP solutions}\label{lmdp-derivation}

% Pick $a \in A$, versus, pick $\Delta(S)$. $f: S\to A$ vs $f:S \to \Delta(S)$.

In the original Todorov paper \cite{Todorov2009}, they derive the LMDP equations for minimising a cost function. This maximisation derivation just changes a few negative signs around.
% Although there is also a subtle change in the interpretation of what the unconstrained dynamics are doing. (??? explain)

\begin{align*}
V(s) &= \mathop{\text{max}}_{u} q(s) - \text{KL}(u(\cdot| s) \parallel p(\cdot | s)) + \gamma \mathop{\mathbb E}_{s' \sim u(\cdot | s)} V(s') \tag{1}\\
\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s) }{ u(s' | s)}+\gamma \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \big[V(s')\big] \bigg] \tag{2}\\
\log(z_{u^{* }}(s)) &= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s) }{ u(s' | s)}+\mathop{\mathbb E}_{s' \sim u(\cdot | s)} \big[\log(z_{u^{* }}(s')^{\gamma})\big] \bigg] \tag{3}\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s)z_{u^{* }}(s')^{\gamma} }{ u(s' | s)} ) \bigg] \tag{4}\\
G(s) &= \sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma} \tag{5}\\
&= q(s) + \mathop{\text{max}}_{u} \bigg[ \mathop{\mathbb E}_{s' \sim u(\cdot | s)} \log(\frac{p(s' | s)z_{u^{* }}(s')^{\gamma} }{ u(s' | s)} \cdot \frac{G(s)}{G(s)} ) \bigg] \tag{6}\\
&= q(s) + \log G(s) + \mathop{\text{min}}_{u} \bigg[\text{KL}\big(u(\cdot | s) \parallel \frac{p(\cdot | s)\cdot z_{u^{* }}(\cdot)^{\gamma}}{G(s)} \big) \bigg] \tag{7}\\
u^{* }(\cdot | s) &= \frac{p(\cdot | s)\cdot z_{u^{* }}(\cdot)^{\gamma}}{\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}} \tag{8}\\
\log(z_{u^{* }}(s)) &= q(s) + \log \big(\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}\big) \tag{9}\\
z_{u^{* }}(s) &= e^{q(s)}\big(\sum_{s'} p(s' | s) z_{u^{* }}(s')^{\gamma}\big) \tag{10}\\
z_{u^{* }} &= e^{q(s)}\cdot P z_{u^{* }}^{\gamma} \tag{11}\\
\end{align*}

By definition, an LMDP is the optimisation problem in (1). We can move the $\text{max}$ in (2), as $q(s)$ is not a function of $u$. Also in (2), expand the second term using the definition of KL divergence, the negative from the KL cancels the second terms negative. (3) Define a new variable, $z(s) = e^{v(s)}$. Also, use the log rule to move the discount rate. (4) Both expectations are under the same distribution, therefore they can be combined. Also, using log rules, combine the log terms. (5) Define a new variable that will be used to normalise $p(s' | s)z(s')^{\gamma}$. (6) Multiply and divide by $G(s)$. This allows us to rewrite the log term as a KL divergence as now we have two distributions, $u(\cdot | s)$ and $\frac{p(\cdot | s)z(\cdot)^{\gamma}}{G(s)}$. (7) The change to a KL term introduces a negative, instead of maximising the negative KL, we minimise the KL. Also in (7) the extra G(s) term can be moved outside of the expectation as it is not dependent in $s'$. (8) Set the optimal policy to minimise the KL distance term. (9) Since we picked the optimal control to be the form in (8), the KL divergence term is zero. (10) Move the log. (11) Rewrite the equations for the tabular setting, where $z$ is vector, and the uncontrolled dynamics are a matrix.

\subsection{MDP Linearisation}\label{mdp-Linearisation}

The ability to solve LMDPs is great, but it's only useful if we can map MDPs into LMDPs, solve them, and map the solution back.
Our goal here is to find a LMDP that has 'similar' structure to the original MDP we were given.\footnotemark

\footnotetext{This derivation is not the same as in Todorov. He sets $b_a \neq r, b_a = r - \sum P \log P$.}

\begin{align*}
\forall s, s' \in S, \forall a \in A, \exists u_a& \;\;\text{such that;} \tag{1}\\
\tau(s' | s, a) &= u_a(s'|s)p(s'|s) \tag{2}\\
r(s, a) &= q(s) - \text{KL}(\tau(\cdot | s, a) \parallel u_a(\cdot| s) ) \tag{3}\\
\\
r(s, a) &= q(s) - \text{KL}(\tau(\cdot | s, a)\parallel\frac{\tau(\cdot | s, a)}{p(\cdot|s)}) \tag{4}\\
r(s, a) &= q(s) - \sum_{s'}\tau(s' | s, a) \log(p(s'|s)) \tag{5}\\
\\
m_{s'}[s]&:= \log p(s' | s) \tag{6}\\
D_{as'}[s] &:= p(s'|s, a) \tag{7}\\
c_{s'}[s] &:= q[s] \mathbf 1 - m_{s'}[s] \;\;\text{such that} \;\; \sum_{s'} e^{m_{s'}[s]} = 1 \tag{8}\\
\\
r_a &= D_{as'} ( q \mathbf 1 - m_{s'}) \;\;\forall s \tag{9}\\
r_a &= D_{as'}c_{s'}  \;\;\forall s \tag{10}\\
c_{s'} &= r_aD_{as'}^{\dagger} \;\;\forall s\tag{11}\\
q &= \log \sum_{s'} e^{c_{s'}} \;\;\forall s\tag{12}\\
m_{s'} &= q - c_{s'} \;\;\forall s\tag{13}\\
\end{align*}

We want to pick $p, q$ such that the dynamics of every action in the original MDP can be represented with a control (2),
and every reward generated by an action, can be given by a combination of the
state rewards and the $\text{KL}$-divergence between the true dynamics and a control (3).
Combine (2), (3) to yield (4). Expand the definition of $\text{KL}$-divergence to get (5).
Now, we move to a tabular representation., where $m_{s'}[s]$ and $c_{s'}[s]$ are vectors, and
$D_{as'}[s]$ is a matrix, defined in (6), (7), (8). With these new definitions, we can rewrite equation (5)
as (9). The expectation can be moved to include $q$ because it sums to one.
Substitute equation (8) into (9) to get (10). Solve the linear equation in (11) to get the value of $c_{s'}$.
Use the value of $c_{s'}$ to calculate the state rewards and unconditioned dynamics by using equations (12), (13) and (6).

\section{Symmetries for RL}

\subsection{MDP homomorphisms}\label{mdp-homomorphism}

As pointed out in \ref{C:abstraction}, the notion of an abstraction is captured by a homomorphism.
Given this, it seems natural to extent the definition to MDPs.

A MDP homomorphism is a transformation of a MDP, $\mathcal H: \mathcal M\to \mathcal M$, that preserves the transition and reward function \cite{Ravindran2002}. We can describe this MDP homomorphism as $\mathcal H = (f, g)$ such that;

\begin{align*}
\tau(f(s')|f(s), g_s(a)) = \sum_{s''\in [s']_f} \tau(s''| a, s) \\
r(f(s), g_s(a)) = r(s, a)
\end{align*}

This MDP homomorphism framework yields state-action abstraction, that uses a model based notion of similarity.
However, as pointed out in earlier sections, there are many other possible
notions of abstraction and similarity that can make sense for RL. Specifically, the MDP homomorphism framework
could be generalised in the following ways;

\begin{enumerate}
\tightlist
  \item approximate symmetries
  \item complexity measure / inductive bias
  \item inference of symmetries under uncertainty
  \item temporal symmetries
\end{enumerate}

Indeed, some work has extended the notion of symmetric model-based abstraction (1)
to approximately symmetric model-based abstraction \cite{Ravindran2004}. However, this has yet to be generalised to other types of symmetry with (say) the state-action values.

Section \ref{symmetric-abstractions} is our not-so-humble attempt to achieve a symmetric inductive bias (2). And, as far as we know, little progress has been made on (3).

Finally, there has been at least one attempt \cite{Ravindran2003} to build a framework capable of exploiting temporal symmetries (4). And while their proposal makes sense, we offer an alternative below.

\subsection{Temporal symmetries}\label{temporal-homomorphism}

Consider a transformation on a MDP that takes sequences of actions, an option, $a_1, a_2, \dots, a_k$, and relabels them as actions in a new MDP.

\begin{align*}
\Omega_k&: M \to M \\
\Omega_k&:\{S, A, r, P, \gamma\} \to \{S, A^k, \hat T_k(r), \hat T_k(P), \gamma\} \\
\hat T_k(r) &= \sum_{i=t}^{t+k} \gamma^{i-t} r(s_i, a_i) \\
\hat T_k(P) &= \prod_{i=t}^{t+k} \tau(s_{i+1} | s_i, a_i) \\
\end{align*}

% (is this actually a homomorphism!? what do I need to prove?)
It seems easy to see that the transformation of the actions is bijective\footnotemark (each option is identified with one action), and therefore we have an isomorphism (which is a type of homomorphism).

\footnotetext{Although, care does need to be taken with the transitions and rewards. We must ensure that the transitions / rewards for an options are the same as the transition / reward for the relabelled option (now a single action).}

% Need to show two temporally transformed MDPs can be composed? This allows us to construct options of various length!? $\Omega_i(M) \wedge \Omega_j(M) \to \tilde M$.

% What about a goal-like verion of this?

We can now define a temporal homomorphism as the composition of our temporal transformation and the MDP homomorphism.

\begin{align*}
\mathcal H: M \to M \tag{MDP homomorphism}\\
\Omega_k: M \to M \tag{temporal transformation}\\
\mathcal H\circ \Omega_k : M \to M \tag{temporal homomorphism}
\end{align*}

Rather than finding model-based symmetries in the state-actions. This construction will find symmetries
in state-options (of a specific length).

\subsection{Invariants}\label{game-invariants}

An important property of a group is that they can be (uniquely) identified by their invariant relations \cite{PeterOlver1999}.
Together, the invariant relations and generators can be use to construct a group.
For example, the cyclic groups can be generated by a single element, which represents an action like $+1$.
And the invariant relation is $(+1)^n = e$. This relation says, that after applying the element to itself $n$ times,
it loops back to the identity element, $e$. Thus it makes a cycle.
And the group of order $n$, with only the single invariant relation, $a^n = e$, must be \underline{the} cyclic group of order $n$.

% https://en.wikipedia.org/wiki/Presentation_of_a_group

So, how can we use invariant relations (and the generators) to help infer symmetries in reinforcement learning problems?

\begin{itemize}
	\tightlist
  \item What does an invariant in the transition function (or reward function) imply about the value function?
	\item Which symmetries can we identify by using invariants in the value function?
  \item How hard is (computationally) it to find these invariants?
  % \item Given a set of invariants that allow many possible symmetries, which symmetries should we prefer?
  % \item How are generators related to disentangled dimensions?
\end{itemize}

\subsubsection{Cart pole}

Let's work through an example, the cart-pole control problem. The goal is to balance a pole on a cart.
Where, the cart can be moved left or right.

The states of the cart pole problem are described by $s_i = [p_c^i, v_c^i, p_p^i, v_p^i]$\footnotemark. Where, $p_c$ is the position of the cart, $v_c$ is the velocity of the cart, $p_p$ is the position of the pole, $v_p$ is the angular velocity of the pole. And the actions are $-1$ for left and $1$ for right.

\footnotetext{Where the cart's position is centered around some fixed starting point. And the rotations are measured relative up being upright.}

\paragraph{Mirror Symmetry}

\begin{displayquote}
  \textit{We can 'flip' the cart pole problem, and it remains the same problem.}
\end{displayquote}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-mirror.png}
	\caption{Two mirror symmetric state-actions of a cart pole.}
\end{figure}

Let the group $g\in S_2$ act on the

\begin{itemize}
	\tightlist
	\item state space as $g \circ s = -s$
	\item action space as $g \circ a = -a$
\end{itemize}

Therefore, we can write the;
\begin{itemize}
	\tightlist
 	\item policy as $g \circ \pi(a | s) = \pi(g \circ a | g \circ s) = \pi(a | s)$.
	\item transition function as $(g \circ \tau)(\cdot | s, a) = \tau(\cdot| g \circ s, g \circ a) = \tau(\cdot | s, a)$.
	\item reward function as $(g \circ r)(s, a) = r(g \circ s, g \circ a) = r(s, a)$.
  \item state-action values as $(g \circ Q^{\pi})(s, a) = Q^{g \circ \pi}(g \circ s, g \circ a) = Q^{\pi}(s, a)$.
\end{itemize}

So, what are some invariants we might care about in the RL context?

\begin{align*}
Q^\pi(s, a) &= (g \circ Q^{\pi})(s, a) \tag{expected return}\\
T(Q^\pi)(s,a) - Q^\pi(s,a) &=T(g \circ Q^\pi)(s, a) - (g \circ Q^\pi)(s,a) \tag{Bellman residual}\\
\mathop{\mathbb E}_{s' \sim \tau(\cdot| s, a)} (s' - s) &= g \circ \mathop{\mathbb E}_{s' \sim (g \circ \tau)(\cdot| s, a)} (s' - g \circ s) \tag{change in state}\\
\end{align*}

So, the cart pole problem is invariant to $S_2$. But, how could this have been identified from the invariants above?

% The rewards (/ value) reachable from $s_1$ are also reachable from $s_2$ (which also implies the converse).
% So the expected return, Bellman residual and state transition are invariant to the action of $S_2$.
% Is this combination of invariants unique to the mirror symmetry? (how to prove this?!)

\paragraph{Translational Symmetry}

\begin{displayquote}
  \textit{The problem is essentially the same if we move to the left or right.}
\end{displayquote}

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-translation.png}
\caption{Two similar cart poles, which are a translation different from each other.}
\end{figure}

% This is actually a Lie group, ...? cyclic continuous symmetry, $C_R$.
% How to prove that this is a cts symmetry!?

Define the action of $g \in G$ on the

\begin{itemize}
	\tightlist
	\item state space $g \circ s = [g+p_c, v_c, p_p, v_p]$
	\item action space $g \circ a = a$
\end{itemize}

Therefore, we can write the;
\begin{itemize}
	\tightlist
 	\item policy as $g \circ \pi(a | s) = \pi(a | g \circ s) = \pi(a | s)$.
	\item transition function as $(g \circ \tau)(\cdot | s, a) = \tau(\cdot| g \circ s, a) = \tau(\cdot | s, a)$.
	\item reward function as $(g \circ r)(s, a) = r(g \circ s, a) = r(s, a)$.
  \item state-action values as $(g \circ Q^{\pi})(s, a) = Q^{g \circ \pi}(g \circ s, a) = Q^{\pi}(s, a)$.
\end{itemize}


So, what are some invariants we might care about in the RL context?

\begin{align*}
Q^\pi(s, a) &= (g \circ Q^{\pi})(s, a) \tag{expected return}\\
T(Q^\pi)(s,a) - Q^\pi(s,a) &=T(g \circ Q^\pi)(s, a) - (g \circ Q^\pi)(s,a) \tag{Bellman residual}\\
\mathop{\mathbb E}_{s' \sim \tau(\cdot| s, a)} (s' - s) &= g \circ \mathop{\mathbb E}_{s' \sim (g \circ P)(\cdot| s, a)} (s' - g \circ s) \tag{change in state}\\
\end{align*}

Despite two different groups acting on the cart pole problem, they have the same invariants (that we considered).
Which other invariants should we be measuring, and how would they help us narrow the possible symetries?

We consider some other examples as well. But don't construct their invariants.

\paragraph{Transition Symmetry}

\begin{displayquote}
  \textit{Despite starting in different states, and applying different actions, we might end up in the same next state.}
\end{displayquote}

If we allow actions to be continuous, where we get to choose the impulse $a \in [-c, c]$.
Then we we get a new set of invariants of the transition function $\tau(s'|s, a) = \tau(s'|g\circ (s, a))$.
But what do these invariants of the transition function imply about invariants in the state (and action) values?

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-state.png}
\caption{Invariants of the transition function. Two different actions take two different states to the same one.
Roughly, the intuition is; a small push of a fast moving object achieves the same results a large push of a slow moving object.}
\end{figure}


\paragraph{Temporal Symmetry}

\begin{displayquote}
  \textit{There exist multiple ways of achieving the same thing.}
\end{displayquote}

This is a temporally extended version of the Transition Symmetry (above).

\begin{align*}
\Upsilon(s_t | s, a_1, \dots, a_t) &= \sum_{s_1, s_2, \dots s_{t-1}}\prod_{i=0}^t \tau(s_t| s_{t-1}, a_{t-1}) \\
\Upsilon(s' | s, a_1, \dots a_t) &= \Upsilon(s' | s, g\circ (a_1, \dots a_t)) \\
\end{align*}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/cart-pole-temporal-approx.png}
\caption{Temporal invariants. For example, we might be able to reorder actions, without changing the effect (\textit{putting your left sock on before you put on your right sock}).
Or we might be able to use different numbers of actions to achieve the same thing (\textit{three left turns makes a right}).}
\end{figure}

\newpage
\subsubsection{Pong}

The states of Pong are described by $s = [p_1, v_1, p_2, v_2, p^x_b, p^y_b, v^x_b, v^y_b]$ (all of these are centered around the middle of the table). Where, $p_1$ is the position of player 1's paddle, $v_1$ is the velocity of player 1's paddle, $p^x_b$ is the $x$ position of the ball, $v^y_b$ is the $y$ component of the ball's velocity.
And the actions are $-1$ for left and $1$ for right.

Pong also has mirror symmetry in the same sense as the cart pole problem.
And, it has another type of mirror symmetry.

% \paragraph{Mirror symmetry (vertical)}
%
% \begin{figure}[!h]
% \centering
% \includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-vert-flip.png}
% \caption{Mirror sym}
% \end{figure}
%
%
% Define the action of $f \in S_2$ on the;
%
% \begin{itemize}
% 	\tightlist
% 	\item state space $f \circ [p_1, v_1, p_2, v_2, p^x_b, p^y_b, v^x_b, v^y_b] := [-p_1, -v_1, -p_2, -v_2, -p^x_b, p^y_b, -v^x_b, v^y_b]$
% 	\item action space $f \circ a := -a$
%  	\item policy to be $f \circ \pi(a | s) = \pi(f \circ a | f \circ s)$.
% 	\item transition function to be $(f \circ P)(s' | s, a) = \tau(f \circ s'| f \circ s, f \circ a)$.
% 	\item value function is $(f \circ Q^{\pi})(s, a) = Q^{f \circ \pi}(f \circ s, f \circ a)$
% \end{itemize}
%
% \begin{align*}
% Q^\pi(s, a) &= (g \circ Q^{\pi})(s, a) \tag{expected return}
% \end{align*}


\paragraph{Mirror symmetry (player perspective / horizontal)}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.25\textheight]{../../pictures/drawings/pong-horz-flip.png}
\caption{The invariants of a shift in player perspective.}
\end{figure}

Because Pong is a zero sum, two player game, there is a symmetry of perspective.
Whether you are playing as player 1 or player two, you are still playing the same game of Pong,
but with inverted pay-offs.

Let $G = (\{e, g\}, \circ)$. And define the action of $g$ on the;

\begin{itemize}
	\tightlist
	\item state space $g \circ [p_1, v_1, p_2, v_2, p^x_b, p^y_b, v^x_b, v^y_b] := [p_2, v_2, p_1, v_1, -p^x_b, p^y_b, -v^x_b, v^y_b]$
	\item action space $g \circ a := a$
  \item rewards $g \circ r(g \circ s, g \circ a) := -r(g \circ s, g \circ a)$
\end{itemize}

Therefore, we can write the;
\begin{itemize}
  \tightlist
 	\item policy to be $g \circ \pi(a | s) = \pi(g \circ a | g \circ s) = \pi(s, a)$.
	\item transition function to be $(g \circ \tau)(s' | s, a) = \tau(g \circ s'| g \circ s, g \circ a) = \tau(s' | s, a)$.
	\item value function is $(g \circ Q^{\pi})(s, a) = g \circ r(s, a)+ \gamma (g \circ \tau)(s'|s, a) (g \circ Q)^{\pi}(s, a) = Q^{\pi}(s, a)$
\end{itemize}

\section{Symmetry and machine learning}

All of these methods rely on a similar type of method: use the symmetry to construct orbits, and then average or pick a representative.

\subsection{Exploitation} \label{symmetric-exploitation}

\begin{displayquote}
\textsl{Once we have discovered a symmetry, how might we exploit that discovery?}
\end{displayquote}

Similar to how we considered how to exploit an abstraction in section \ref{exploit-abstraction-rl},
let's review some existing methods for exploiting the knowledge of a symmetry.

% note: what has been given to these methods of exploitation?
% knowledge of the group, or its actions, or ...?

\subsubsection{Exploiting symmetry for efficient control}

If we have a MDP, $M_1$, then solving it via value iteration requires $\mathcal O(\epsilon |S|^2|A|)$ iterations.
However, if we know that there exists symmetry of order $k$ in the state space, then we can 'minimise the model',
by applying the MDP homomorphism $\mathcal H: \mathcal M\to \mathcal M$.
This new, minimised, MDP, $M_2$ has a smaller state space, as $|S_{M_2}| = \frac{|S_{M_1}|}{k}$
and essentially the same dynamics and rewards. Thus we can solve $M_2$, with cost $\mathcal O(\epsilon \frac{|S|^2|A|}{k^2})$
and then lift the solution back to $M_1$. \cite{Dean1997, NARAYANAMURTHY}

\subsubsection{Exploiting symmetry for efficient inference}\label{symmetry-inference}

There has been a large amount of work (that we are familiar with) exploring
the exploitation of symmetries for faster learning. The essence of the idea is
\textit{"invariance reduces variance"} \cite{Chen2019}.

Possibly the most famous exploitation strategy is data augmentation \cite{Simard2003}.
But, there are other techniques;

\begin{itemize}
  \tightlist
  \item Use the known symmetries to build invariant network architectures \cite{Abdolhosseini, Anselmi2019}
  \item By sharing weights according to group structure \cite{Yann1995,Ravanbakhsh2017a}
  \item Output coupling \cite{Mahajan2017,Abdolhosseini}
  \item Gradient coupling
\end{itemize}

\paragraph{Gradient coupling}

Inspired by the view of neural network updates as being controlled by a 'neural tangent kernel' \cite{Jacot2018},
here we present another way to exploit symmetries for machine learning.

Let $f: X \to Y$ be some trainable function.

\begin{align*}
\dot y_j = \sum_{i\neq j} \alpha_{ij} \nabla_{\theta} \ell(y_i, \theta)
\end{align*}

Then neural networks share updates between examples according to the neural tangent kernel
$\alpha_{ij} = \langle \nabla_{\theta} \ell(x_i, \theta), \nabla_{\theta} \ell(x_j, \theta)\rangle$.
However, we could pick another way to do this update. Possibly using the symmetries to group $x_i$s and $x_j$s.

% \subsubsection{Exploiting symmetry for efficient exploration}
%
% Holtzen et al. \cite{Holtzen2019} show that by grouping together variables according to
% the knowledge of a symmetry, their \textit{orbit-jump MCMC mixes rapidly in the number of orbits}.
% In other words, symmetries allow the efficient estimation of distributions.
% % Want to demonstrate this.
% % {\color{red}TODO max ent + abstraction experiments}
%
% Also. \cite{Campbell2019}

\subsubsection{A note about discovery}

The discovery of symmetries within data has had little success.
However, with the framing of data augmentation as a form of symmetry exploitation,
this implies that automated data augmentation \cite{Ho2019a, Lim2019, Cubuk2018, Cubuk2019} is a form of symmetry discovery.

These methods discover which symmetries apply to a given domain, and at what magnitude.
They tend to frame the optimisation problem as picking the probability of a set of given op and their magnitude.
For instance, \cite{Ho2019a} provides a small set of operations (aka symmetries):
\textit{Identity, AutoContrast, Equalize, Rotate, Solarize, Color, Posterize, Contrast,
	Brightness, Sharpness, ShearX, ShearY, TranslateX, TranslateY.}
Validation error is then used as a reward for learning.

While this approach does work, like other 'meta-learning techniques', it does not scale well.

\subsection{Actions}\label{construct-actions}

Let $G$ be a group. Where we represent the action of $g\in G$ on the real vectors, $x\in R^n$
as permutation matrices, $\phi(g, x) = P_g \circ x$, where the composition operator $\circ$ becomes matrix multiplication, $\cdot$. Therefore $e = I_n$ and $g$ can be one of many possible actions. For example, consider the representations of the n-gram swaps \footnotemark of $g$ when applied $R^4$.

\footnotetext{There are no others. Proof by construction. Enumerate all permutations and check which ones are idempotent.}

\begin{align*}
 \begin{bmatrix}
 1 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 1 \\
 0 & 0 & 1 & 0 \\
 \end{bmatrix}
 &\begin{bmatrix}
 1 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 1 \\
 \end{bmatrix}
 \begin{bmatrix}
 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 \\
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 \end{bmatrix}   \\
 \begin{bmatrix}
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 1 \\
 \end{bmatrix}
 &\begin{bmatrix}
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 \\
 \end{bmatrix}
 \begin{bmatrix}
 0 & 0 & 0 & 1 \\
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 1 & 0 & 0 & 0 \\
 \end{bmatrix}   \\
 \begin{bmatrix}
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 \\
 0 & 0 & 1 & 0 \\
 \end{bmatrix}
 &\begin{bmatrix}
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 1 \\
 1 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 \\
 \end{bmatrix}
 \begin{bmatrix}
 0 & 0 & 0 & 1 \\
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 \\
 \end{bmatrix}   \\
\end{align*}

The first two rows contain permutations that swaps (two) elements of the vector ($a\leftrightarrow b$), these are the possible actions of $S_2$.
The last row contains permutations that swaps pairs ($(a, b) \leftrightarrow (c, b)$), this entire row is the only representation of $S_2 \times S_2$.

\subsubsection{Race grid world}\label{race-grid-world}

We want to construct a simple symmetric problem to test our learners.
The intuition behind this toy problem comes from a $100m$ sprint.
It doesn't matter which lane you are in, you should run forwards, not sideways...

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth,height=0.35\textheight]{../../pictures/figures/race-mdp-puzzle-n3.png}
  \caption{The purple nodes represent the starting states. The green-teal nodes represent states wth reward $1$. There are 5 actions, left, right, up, down, none.
	Starting from the leftmost purple node: Left moves clockwise to the next purple node. Up moves to the blue node. Down doesn't result in movement. None doesn't result in movement.}
\end{figure}

\newpage
\section{n-dimensional Cart pole}\label{action-space-experiments}

\begin{displayquote}
  \textit{How can we test a learners ability to detect symmetries and exploit them?}
\end{displayquote}

We propose a simple test, the n-dimensional cart pole: a generalisation of the
cart pole problem to $n$ dimensions. Rather than receiving observations in
$\mathbb{R}^4$ (the position, velocity, angle and angular velocity), observations are
in $\mathbb{R}^{4\times n}$. And the action space is generalised from $\{0,1\}$ (left and right),
to $\{0,1\}^{n}$.

% What makes this problem hard??
% This setting allows us to easily control the amount of symmetry.
% Existing envs dont test this because!?!?

\subsection{How is this problem symmetric?}

The $n$-dimensional cart pole problem can be reduced to $n$, one dimensional cart pole problems.
Where each of these one dimensional cart pole problems is easy to solve.

In a more formal sense. This problem is symmetric because the optimal policy and its ($Q$) values are invariant to the actions of the permutation group of order n, $S_n$.

\begin{align*}
g \circ s_j &= g \circ [x_0, \dots, x_i, \dots, x_{n-1}] \\
&= [x_1, x_0, \dots, x_i, \dots, x_{n-1}] \\
g_i \circ a_k &= g \circ [u_0, \dots, u_i, \dots, u_{n-1}] \\
&= [u_1, u_0 \dots, u_i, \dots, u_{n-1}] \\
g\circ \tau(s'|s, a) &= \tau(g\circ s'|g\circ s, g\circ a) \\
g\circ R(s, a) &= R(g\circ s, g\circ a) \\
g\circ \pi^{* }(a|s) &= \pi^{* }(g\circ a| g\circ s) \\
&= \pi^{* }(a|s) \tag{invariance of the optimal policy}\\
g\circ Q^{\pi^{* }}(s, a) &= Q^{\pi^{* }}(g\circ s, g\circ a) \\
&= Q^{\pi^{* }}(s, a) \tag{invariance of the optimal values}
\end{align*}

We describe a state as $s_j = [x_0, \dots, x_i, \dots, x_{n-1}]$, where $x_i = (p_c^i, v_c^i, p_p^i, v_p^i)$. Where, $p_c$ is the position of the cart, $v_c$ is the velocity of the cart, $p_p$ is the position of the pole, $v_p$ is the angular velocity of the pole. We describe actions as $a_k \in \{0, 1\}^n$. Let $g\in S_n$ be the pairwise permutation, swapping the first two elements $(0\to 1)$.

% However, the learner needs to infer these symmetries, so they can be exploited.

% Well, the original cart pole problem has a few symmetries in it (as explored in \ref{}).
% However, by

\subsection{An advantage}

\begin{displayquote}
\textit{What advantage is provided by exploiting symmetries?}
\end{displayquote}

If a learner has inferred that the $n$-dimensional cart pole problem can be decomposed into $n$ identical sub problems,
then that means it is gathering $n$ times the data for the one-dimensional cart pole problem.
So, we should see a factor of $n$ speed up in learning.
This is the same argument made here [quotient groups appendix...].

For a learner that doesn't know of the symmetries. How is this problem hard?
The more dimensions there are, the more ways there are to fail.
Consider how exploration is done. In a single dimension, actions are taken with probability  is
taken with some chance of exploring instead.
% How does PPO / PG do exploration?!?!?
Maybe you correctly balanced the pole in all dimensions except one. To bad, you don't get any reward.

\subsection{Experiments}\label{ndcart-experiments}

We use OpenAI's Gym \cite{Brockman2016} and Baselines \cite{baselines} to test this environment.

% What about if we rotate the observations. So the observations are not aligned with the actions?
% Or generalising to n+1? Could start the agent off with n+1 dims. But set them to observe nothing / actions do nothing. Until t > T?

Note: 'Average mean reward' refers to the fact that we have averaged (n=5)
the mean reward (per episode). Also note: This reward is the training performance.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/figures/multibinary-nd-cart.png}
  \caption{Default PPO2 solving the nd cartpole problem with access to a \textit{MultiBinary} action space. Each color corresponds to a the average mean return of different, $n$, the number of repeated cart pole problems.}
\end{figure}

As mentioned in the previous section, we expected learning to become much
harder for a learner that doesn't exploit symmetries. These results suggest either of two possibilities:
that PPO2 can discover and exploit symmetries, our setting does not test what we think it does.

While investigating this further, we realised that the given action space, \textit{MultiBinary}, provides a large amount of information. We ran another test with a \textit{Discrete} action space. Where the learner gets to choose $a\in \mathbb Z_n$.
This action then gets (binary) decoded to the \textit{MultiBinary} format.

A learner that exploits the permutation symmetries in the $n$-dimensional cart pole problem should learn $n$ times quicker.
However, the cost of discovering this permutation symmetry is unknown.

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth,height=0.5\textheight]{../../pictures/figures/discrete-nd-cart.png}
\caption{Default PPO2 solving the nd cartpole problem with access to a \textit{Discrete} action space. Each color corresponds to a the average mean return of different, $n$, the number of repeated cart pole problems.}
\end{figure}



% It seems surprising that access to the \textit{MultiBinary} action space provides such an advantage.
% Also, it seems surprising that the an increase of 6 dimensions only results in approximately a ~2 million increase in the data required.
% Is the learner doing some sort of intelligent sharing?
% Why is it so hard for the Discrete learner? What operation does it find hard to learn. The ability to decode? $n$ bits to $2^n$ onehots?

% Also, interesting to note that the 1D learner equipped with a \textit{Discrete}
% action space achieves max performance at ~1.75 million samples, while the learner
% equipped with a \textit{MultiBinary} action space achieves max performance at ~2.25 million samples. (significant??)
