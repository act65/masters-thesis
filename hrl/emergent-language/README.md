Action space gets abstracted into more 'meaningful' concepts.
But is influenced by the interaction with the model and the reward.

https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00328

As an agent learns a more abstract action space it invents a language!?

Imagine we have a small set of primitive actions.
A) E.g the ability to place tranistors in circuits.
We can then explore different circuits and their behaviour.
We could invent a general language for describing these circuits.

B) Also we could include the influence of the reward function and tune the exploration and capacity to what is rewarding.


Does it make sense to talk about the "grammar" of different RL envs?
Nautrally they would have some? In a maze with options. It wouldnt make sense to search-the-room if you are in a hallway...!?
And some options are not possible in different states. (but we could still try!?)
